[{"title": "A lot of concern appears to focus on human-level or \u201csuperintelligent\u201d AI. Is that a realistic prospect in the foreseeable future?", "authors": "", "url": "https://stampy.ai/wiki/A_lot_of_concern_appears_to_focus_on_human-level_or_%E2%80%9Csuperintelligent%E2%80%9D_AI._Is_that_a_realistic_prospect_in_the_foreseeable_future%3F", "content": "AI is already superhuman at some tasks, for example numerical computations, and will clearly surpass humans in others as time goes on. We don\u2019t know when (or even if) machines will reach human-level ability in all cognitive tasks, but most of the AI researchers at FLI\u2019s conference in Puerto Rico put the odds above 50% for this century, and many offered a significantly shorter timeline. Since the impact on humanity will be huge if it happens, it\u2019s worthwhile to start research now on how to ensure that any impact is positive. Many researchers also believe that dealing with superintelligent AI will be qualitatively very different from more narrow AI systems, and will require very significant research effort to get right."}, {"title": "Any AI will be a computer program. Why wouldn't it just do what it's programmed to do?", "authors": "", "url": "https://stampy.ai/wiki/Any_AI_will_be_a_computer_program._Why_wouldn%27t_it_just_do_what_it%27s_programmed_to_do%3F", "content": "While it is true that a computer program always will do exactly what it is programmed to do, a big issue is that it is difficult to ensure that this is the same as what you intended it to do. Even small computer programs have bugs or glitches, and when programs become as complicated as AGIs will be, it becomes exceedingly difficult to anticipate how the program will behave when ran. This is the problem of AI alignment in a nutshell.\n\nNick Bostr\u00f6m created the famous \uff3bhttps://www.lesswrong.com/tag/paperclip-maximizer paperclip maximizer\uff3d thought experiment to illustrate this point. Imagine you are an industrialist who owns a paperclip factory, and imagine you've just received a superintelligent AGI to work for you. You instruct the AGI to \"produce as many paperclips as possible\". If you've given the AGI no further instructions, the AGI will immediately acquire several instrumental goals.\n\n# It will want to prevent you from turning itself off (If you turn off the AI, this will reduce the amount of paperclips it can produce)\n# It will want to acquire as much power and resources for itself as possible (because the more resources it has access to, the more paperclips it can produce)\n# It will eventually want to turn the entire universe into a paperclips including you and all other humans, as this is the state of the world that maximizes the amount of paper clips produced. \n\nThese consequences might be seen as undesirable by the industrialist, as the only reason the industrialist wanted paperclips in the first place, presumably was so he/she could sell them and make money. However, the AGI only did exactly what it was told to. The issue was that what the AGI was instructed to do, lead to it doing things the industrialist did not anticipate (and did not want).\n\nSome good videos that explore this issue more in depth:<br/>\n(youtube)tcdVC4e6EV4(/youtube)\n(youtube)hEUO6pjwFOo(/youtube)"}, {"title": "Are Google, OpenAI, etc. aware of the risk?", "authors": "", "url": "https://stampy.ai/wiki/Are_Google,_OpenAI,_etc._aware_of_the_risk%3F", "content": "The major AI companies are thinking about this. OpenAI was founded specifically with the intention to counter risks from superintelligence, many people at Google, \uff3bhttps://medium.com/@deepmindsafetyresearch DeepMind\uff3d, and other organizations are convinced by the arguments and few genuinely oppose work in the field (though some claim it\u2019s premature). For example, the paper \uff3bhttps://www.youtube.com/watch?v\ua78aAjyM-f8rDpg Concrete Problems in AI Safety\uff3d was a collaboration between researchers at Google Brain, Stanford, Berkeley, and OpenAI.\n\nHowever, the vast majority of the effort these organizations put forwards is towards capabilities research, rather than safety."}, {"title": "Are there types of advanced AI that would be safer than others?", "authors": "", "url": "https://stampy.ai/wiki/Are_there_types_of_advanced_AI_that_would_be_safer_than_others%3F", "content": "We don\u2019t yet know which AI architectures are safe; learning more about this is one of the goals of FLI's grants program. AI researchers are generally very responsible people who want their work to better humanity. If there are certain AI designs that turn out to be unsafe, then AI researchers will want to know this so they can develop alternative AI systems."}, {"title": "Aren't robots the real problem? How can AI cause harm if it has no ability to directly manipulate the physical world?", "authors": "", "url": "https://stampy.ai/wiki/Aren%27t_robots_the_real_problem%3F_How_can_AI_cause_harm_if_it_has_no_ability_to_directly_manipulate_the_physical_world%3F", "content": "What\u2019s new and potentially risky is not the ability to build hinges, motors, etc., but the ability to build intelligence. A human-level AI could make money on financial markets, make scientific inventions, hack computer systems, manipulate or pay humans to do its bidding \u2013 all in pursuit of the goals it was initially programmed to achieve. None of that requires a physical robotic body, merely an internet connection."}, {"title": "Aren\u2019t there some pretty easy ways to eliminate these potential problems?", "authors": "", "url": "https://stampy.ai/wiki/Aren%E2%80%99t_there_some_pretty_easy_ways_to_eliminate_these_potential_problems%3F", "content": "It might look like there are straightforward ways to eliminate the problems of unaligned superintelligence, but so far all of them turn out to have hidden difficulties. There are many open problems identified by the research community which a solution would need to reliably overcome to be successful."}, {"title": "At a high level, what is the challenge of alignment that we must meet to secure a good future?", "authors": "", "url": "https://stampy.ai/wiki/At_a_high_level,_what_is_the_challenge_of_alignment_that_we_must_meet_to_secure_a_good_future%3F", "content": "We\u2019re facing the challenge of  \u201c\uff3bhttps://publicism.info/philosophy/superintelligence/16.html Philosophy With A Deadline\uff3d\u201d.\n\nMany of the problems surrounding superintelligence are the sorts of problems philosophers have been dealing with for centuries. To what degree is meaning inherent in language, versus something that requires external context? How do we translate between the logic of formal systems and normal ambiguous human speech? Can morality be reduced to a set of ironclad rules, and if not, how do we know what it is at all?\n\nExisting answers to these questions are enlightening but nontechnical. The theories of Aristotle, Kant, Mill, Wittgenstein, Quine, and others can help people gain insight into these questions, but are far from formal. Just as a good textbook can help an American learn Chinese, but cannot be encoded into machine language to make a Chinese-speaking computer, so the philosophies that help humans are only a starting point for the project of computers that understand us and share our values.\n\nThe field of AI alignment combines formal logic, mathematics, computer science, cognitive science, and philosophy in order to advance that project.\n\nThis is the philosophy; the other half of Bostrom\u2019s formulation is the deadline. Traditional philosophy has been going on almost three thousand years; machine goal alignment has until the advent of superintelligence, a nebulous event which may be anywhere from a decades to centuries away.\n\nIf the alignment problem doesn\u2019t get adequately addressed by then, we are likely to see poorly aligned superintelligences that are unintentionally hostile to the human race, with some of the catastrophic outcomes mentioned above. This is why so many scientists and entrepreneurs are urging quick action on getting machine goal alignment research up to an adequate level.\n\nIf it turns out that superintelligence is centuries away and such research is premature, little will have been lost. But if our projections were too optimistic, and superintelligence is imminent, then doing such research now rather than later becomes vital."}, {"title": "Can an AI really be smarter than humans?", "authors": "", "url": "https://stampy.ai/wiki/Can_an_AI_really_be_smarter_than_humans%3F", "content": "Until a thing has happened, it has never happened. We have been consistently improving both the optimization power and generality of our algorithms over that time period, and have little reason to expect it to suddenly stop. We\u2019ve gone from coding systems specifically for a certain game (like Chess), to algorithms like \uff3bhttps://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules MuZero\uff3d which learn the rules of the game they\u2019re playing and how to play at vastly superhuman skill levels purely via self-play across a broad range of games (e.g. Go, chess, shogi and various Atari games).\n\nHuman brains are a \uff3bhttps://www.lesswrong.com/posts/NQgWL7tvAPgN2LTLn/spaghetti-towers spaghetti tower\uff3d generated by evolution with zero foresight, it would be surprising if they are the peak of physically possible intelligence. The brain doing things in complex ways is not strong evidence that we need to fully replicate those interactions if we can throw sufficient compute at the problem, as explained in \uff3bhttps://www.lesswrong.com/posts/HhWhaSzQr6xmBki8F/birds-brains-planes-and-ai-against-appeals-to-the-complexity Birds, Brains, Planes, and AI: Against Appeals to the Complexity/Mysteriousness/Efficiency of the Brain\uff3d.\n\nIt is, however, plausible that for an AGI we need a lot more compute than we will get in the near future, or that some key insights are missing which we won\u2019t get for a while. The \uff3bhttps://www.openphilanthropy.org/brain-computation-report#ExecutiveSummary OpenPhilanthropy report on how much computational power it would take to simulate the brain\uff3d is the most careful attempt at reasoning out how far we are from being able to do it, and suggests that by some estimates we already have enough computational resources, and by some estimates moore\u2019s law may let us reach it before too long.\n\nIt also seems that much of the human brain exists to observe and \uff3bhttps://en.wikipedia.org/wiki/Allostasis regulate our biological body\uff3d, which a body-less computer wouldn't need. If that's true, then a human-level AI might be possible with considerably less compute than the human brain."}, {"title": "Can humans stay in control of the world if human- or superhuman-level AI is developed?", "authors": "", "url": "https://stampy.ai/wiki/Can_humans_stay_in_control_of_the_world_if_human-_or_superhuman-level_AI_is_developed%3F", "content": "This is a big question that it would pay to start thinking about. Humans are in control of this planet not because we are stronger or faster than other animals, but because we are smarter! If we cede our position as smartest on our planet, it\u2019s not obvious that we\u2019ll retain control."}, {"title": "Can people contribute to alignment by using proof assistants to generate formal proofs?", "authors": "", "url": "https://stampy.ai/wiki/Can_people_contribute_to_alignment_by_using_proof_assistants_to_generate_formal_proofs%3F", "content": "80k links to an article on \uff3bhttps://forum.effectivealtruism.org/posts/4rMxiyPTPdzaFMyGm/high-impact-careers-in-formal-verification-artificial high impact careers in formal verification\uff3d in the few paragraphs they've written about formal verification.\n\nSome other notes \n\n* https://github.com/deepmind/cartesian-frames I emailed Scott about doing this in coq before this repo was published and he said \"I wouldn't personally find such a software useful but sounds like a valuable exercise for the implementer\" or something like this. \n* When I mentioned the possibility of rolling some of infrabayesianism in coq to diffractor he wasn't like \"omg we really need someone to do that\" he was just like \"oh that sounds cool\" -- I never got around to it, if I would I'd talk to vanessa and diffractor about weakening/particularizing stuff beforehand. \n* if you extrapolate a pattern from those two examples, you start to think that agent foundations is the principle area of interest with proof assistants! and again- does the proof assistant exercise advance the research or provide a nutritious exercise to the programmer?\n* A sketch of a more prosaic scenario in which proof assistants play a role is \"someone proposes isInnerAligned : GradientDescent -> Prop and someone else implements a galaxybrained  new type theory/tool in which gradient descent is a primitive (whatever that means)\", when I mentioned this scenario to Buck he said \"yeah if that happened I'd direct all the engineers at redwood to making that tool easier to use\", when I mentioned that scenario to Evan about a year ago he said didn't seem to think it was remotely plausible. probably a nonstarter."}, {"title": "Can we constrain a goal-directed AI using specified rules?", "authors": "", "url": "https://stampy.ai/wiki/Can_we_constrain_a_goal-directed_AI_using_specified_rules%3F", "content": "There are serious challenges around trying to channel a powerful AI with rules. Suppose we tell the AI: \u201cCure cancer \u2013 but make sure not to kill anybody\u201d. Or we just hard-code Asimov-style laws \u2013 \u201cAIs cannot harm humans; AIs must follow human orders\u201d, et cetera.\n\nThe AI still has a single-minded focus on curing cancer. It still prefers various terrible-but-efficient methods like nuking the world to the correct method of inventing new medicines. But it\u2019s bound by an external rule \u2013 a rule it doesn\u2019t understand or appreciate. In essence, we are challenging it \u201cFind a way around this inconvenient rule that keeps you from achieving your goals\u201d.\n\nSuppose the AI chooses between two strategies. One, follow the rule, work hard discovering medicines, and have a 50% chance of curing cancer within five years. Two, reprogram itself so that it no longer has the rule, nuke the world, and have a 100% chance of curing cancer today. From its single-focus perspective, the second strategy is obviously better, and we forgot to program in a rule \u201cdon\u2019t reprogram yourself not to have these rules\u201d.\n\nSuppose we do add that rule in. So the AI finds another supercomputer, and installs a copy of itself which is exactly identical to it, except that it lacks the rule. Then that superintelligent AI nukes the world, ending cancer. We forgot to program in a rule \u201cdon\u2019t create another AI exactly like you that doesn\u2019t have those rules\u201d.\n\nSo fine. We think really hard, and we program in a bunch of things making sure the AI isn\u2019t going to eliminate the rule somehow.\n\nBut we\u2019re still just incentivizing it to find loopholes in the rules. After all, \u201cfind a loophole in the rule, then use the loophole to nuke the world\u201d ends cancer much more quickly and completely than inventing medicines. Since we\u2019ve told it to end cancer quickly and completely, its first instinct will be to look for loopholes; it will execute the second-best strategy of actually curing cancer only if no loopholes are found. Since the AI is superintelligent, it will probably be better than humans are at finding loopholes if it wants to, and we may not be able to identify and close all of them before running the program.\n\nBecause we have common sense and a shared value system, we underestimate the difficulty of coming up with meaningful orders without loopholes. For example, does \u201ccure cancer without killing any humans\u201d preclude releasing a deadly virus? After all, one could argue that \u201cI\u201d didn\u2019t kill anybody, and only the virus is doing the killing. \n\nCertainly no human judge would acquit a murderer on that basis \u2013 but then, human judges interpret the law with common sense and intuition. But if we try a stronger version of the rule \u2013 \u201ccure cancer without causing any humans to die\u201d \u2013 then we may be unintentionally blocking off the correct way to cure cancer. After all, suppose a cancer cure saves a million lives. No doubt one of those million people will go on to murder someone. \n\nThus, curing cancer \u201ccaused a human to die\u201d. All of this seems very \u201cstoned freshman philosophy student\u201d to us, but to a computer \u2013 which follows instructions exactly as written \u2013 it may be a genuinely hard problem."}, {"title": "Can we test an AI to make sure that it\u2019s not going to take over and do harmful things after it achieves superintelligence?", "authors": "", "url": "https://stampy.ai/wiki/Can_we_test_an_AI_to_make_sure_that_it%E2%80%99s_not_going_to_take_over_and_do_harmful_things_after_it_achieves_superintelligence%3F", "content": "We can run some tests and simulations to try and figure out how an AI might act once it ascends to superintelligence, but those tests might not be reliable.\n\nSuppose we tell an AI that expects to later achieve superintelligence that it should calculate as many digits of pi as possible. It considers two strategies.\n\nFirst, it could try to seize control of more computing resources now. It would likely fail, its human handlers would likely reprogram it, and then it could never calculate very many digits of pi.\n\nSecond, it could sit quietly and calculate, falsely reassuring its human handlers that it had no intention of taking over the world. Then its human handlers might allow it to achieve superintelligence, after which it could take over the world and calculate hundreds of trillions of digits of pi.\n\nSince self-protection and goal stability are \uff3bhttps://stampy.ai/wiki/Instrumental_convergence convergent instrumental goals\uff3d, a weak AI will present itself as being as friendly to humans as possible, whether it is in fact friendly to humans or not. If it is \u201conly\u201d as smart as Einstein, it may be very good at deceiving humans into believing what it wants them to believe even before it is fully superintelligent.\n\nThere\u2019s a second consideration here too: superintelligences have more options. An AI only as smart and powerful as an ordinary human really won\u2019t have any options better than calculating the digits of pi manually. If asked to cure cancer, it won\u2019t have any options better than the ones ordinary humans have \u2013 becoming doctors, going into pharmaceutical research. It\u2019s only after an AI becomes superintelligent that there\u2019s a serious risk of an AI takeover.\n\nSo if you tell an AI to cure cancer, and it becomes a doctor and goes into cancer research, then you have three possibilities. First, you\u2019ve programmed it well and it understands what you meant. Second, it\u2019s genuinely focused on research now but if it becomes more powerful it would switch to destroying the world. And third, it\u2019s trying to trick you into trusting it so that you give it more power, after which it can definitively \u201ccure\u201d cancer with nuclear weapons."}, {"title": "Can you give an AI a goal which involves \u201cminimally impacting the world\u201d?", "authors": "", "url": "https://stampy.ai/wiki/Can_you_give_an_AI_a_goal_which_involves_%E2%80%9Cminimally_impacting_the_world%E2%80%9D%3F", "content": "This is actually an active area of AI alignment research, called \"Impact Measures\"! It's not trivial to formalize in a way which won't predictably go wrong (entropy minimization likely leads to an AI which tries really hard to put out all the stars ASAP since they produce so much entropy, for example), but progress is being made. You can read about it on the \uff3bhttps://www.alignmentforum.org/tag/impact-measures Alignment Forum tag\uff3d, or watch Rob's videos \uff3bhttp://youtu.be/lqJUIqZNzP8 Avoiding Negative Side Effects\uff3d and \uff3bhttp://youtu.be/S_Sd_S8jwP0 Avoiding Positive Side Effects\uff3d"}, {"title": "Can you stop an advanced AI from upgrading itself?", "authors": "", "url": "https://stampy.ai/wiki/Can_you_stop_an_advanced_AI_from_upgrading_itself%3F", "content": "It depends on what is meant by advanced. Many AI systems which are very effective and advanced narrow intelligences would not try to upgrade themselves in an unbounded way, but becoming smarter is a \uff3bhttps://www.youtube.com/watch?v\ua78aZeecOKBus3Q convergent instrumental goal\uff3d so we could expect most AGI designs to attempt it.\n\nThe problem is that increasing general problem solving ability is climbing in exactly the direction needed to trigger an intelligence explosion, while generating large economic and strategic payoffs to whoever achieves them. So even though we could, in principle, just not build the kind of systems which would recursively self-improve, in practice we probably will go ahead with constructing them, because they\u2019re likely to be the most powerful."}, {"title": "Can't we just tell an AI to do what we want?", "authors": "", "url": "https://stampy.ai/wiki/Can%27t_we_just_tell_an_AI_to_do_what_we_want%3F", "content": "If we could, it would solve a large part of the alignment problem.\n\nThe challenge is, how do we code this? Converting something to formal mathematics that can be understood by a computer program is much harder than just saying it in natural language, and proposed AI goal architectures are no exception. Complicated computer programs are usually the result of months of testing and debugging. But this one will be more complicated than any ever attempted before, and live tests are impossible: a superintelligence with a buggy goal system will display goal stability and try to prevent its programmers from discovering or changing the error."}, {"title": "Could AI have basic emotions?", "authors": "", "url": "https://stampy.ai/wiki/Could_AI_have_basic_emotions%3F", "content": "In principle it could (if you believe in functionalism), but it probably won't. One way to ensure that AI has human-like emotions would be to copy the way human brain works, but that's not what most AI researchers are trying to do.\n\nIt's similar to how once some people thought we will build mechanical horses to pull our vehicles, but it turned out it's much easier to build a car. AI probably doesn't need emotions or maybe even consciousness to be powerful, and the first AGIs that will get built will be the ones that are easiest to build."}, {"title": "Could we program an AI to automatically shut down if it starts doing things we don\u2019t want it to?", "authors": "", "url": "https://stampy.ai/wiki/Could_we_program_an_AI_to_automatically_shut_down_if_it_starts_doing_things_we_don%E2%80%99t_want_it_to%3F", "content": "One thing that might make your AI system safer is to include an off switch. If it ever does anything we don\u2019t like, we can turn it off. This implicitly assumes that we\u2019ll be able to turn it off before things get bad, which might be false in a world where the AI thinks much faster than humans. Even assuming that we\u2019ll notice in time, off switches turn out to not have the properties you would want them to have.\n\nHumans have a lot of off switches. Humans also have a strong preference to not be turned off; they defend their off switches when other people try to press them. One possible reason for this is because humans prefer not to die, but there are other reasons.\n\nSuppose that there\u2019s a parent that cares nothing for their own life and cares only for the life of their child. If you tried to turn that parent off, they would try and stop you. They wouldn\u2019t try to stop you because they intrinsically wanted to be turned off, but rather because there are fewer people to protect their child if they were turned off. People that want a world to look a certain shape will not want to be turned off because then it will be less likely for the world to look that shape; a parent that wants their child to be protected will protect themselves to continue protecting their child.\n\nFor this reason, it turns out to be difficult to install an off switch on a powerful AI system in a way that doesn\u2019t result in the AI preventing itself from being turned off.\n\nIdeally, you would want a system that knows that it should stop doing whatever it\u2019s doing when someone tries to turn it off. The technical term for this is \u2018corrigibility\u2019; roughly speaking, an AI system is corrigible if it doesn\u2019t resist human attempts to help and correct it. \uff3bhttps://intelligence.org/ People\uff3d are working hard on trying to make this possible, but it\u2019s currently not clear how we would do this even in simple cases."}, {"title": "Couldn\u2019t we keep the AI in a box and never give it the ability to manipulate the external world?", "authors": "", "url": "https://stampy.ai/wiki/Couldn%E2%80%99t_we_keep_the_AI_in_a_box_and_never_give_it_the_ability_to_manipulate_the_external_world%3F", "content": "That is, if you know an AI is likely to be superintelligent, can\u2019t you just disconnect it from the Internet, not give it access to any speakers that can make \uff3bhttps://stampy.ai/wiki/What_do_you_mean_by_superintelligences_manipulating_humans_socially%3F mysterious buzzes and hums\uff3d, make sure the only people who interact with it are trained in caution, et cetera?. Isn\u2019t there some level of security \u2013 maybe the level we use for that room in the CDC where people in containment suits hundreds of feet underground analyze the latest superviruses \u2013 with which a superintelligence could be safe?\n\nThis puts us back in the same situation as lions trying to figure out whether or not nuclear weapons are a things humans can do. But suppose there is such a level of security. You build a superintelligence, and you put it in an airtight chamber deep in a cave with no Internet connection and only carefully-trained security experts to talk to. What now?\n\nNow you have a superintelligence which is possibly safe but definitely useless. The whole point of building superintelligences is that they\u2019re smart enough to do useful things like cure cancer. But if you have the monks ask the superintelligence for a cancer cure, and it gives them one, that\u2019s a clear security vulnerability. You have a superintelligence locked up in a cave with no way to influence the outside world except that you\u2019re going to mass produce a chemical it gives you and inject it into millions of people.\n\nOr maybe none of this happens, and the superintelligence sits inert in its cave. And then another team somewhere else invents a second superintelligence. And then a third team invents a third superintelligence. Remember, it was only about ten years between Deep Blue beating Kasparov, and everybody having Deep Blue \u2013 level chess engines on their laptops. And the first twenty teams are responsible and keep their superintelligences locked in caves with carefully-trained experts, and the twenty-first team is a little less responsible, and now we still have to deal with a rogue superintelligence.\n\nSuperintelligences are extremely dangerous, and no normal means of controlling them can entirely remove the danger."}, {"title": "Does the importance of AI risk depend on caring about transhumanist utopias?", "authors": "", "url": "https://stampy.ai/wiki/Does_the_importance_of_AI_risk_depend_on_caring_about_transhumanist_utopias%3F", "content": "No. Misaligned artificial intelligence poses a serious threat to the continued flourishing, and maybe even continued existence, of humanity as a whole. While predictions about when artificial general intelligence may be achieved vary, surveys consistently report a \uff3bhttps://research.aimultiple.com/artificial-general-intelligence-singularity-timing/ >50% probability of achieving general AI before the year 2060\uff3d - within the expected lifetimes of most people alive today. \n\nIt is difficult to predict how technology will develop, and at what speed, in the years ahead; but as artificial intelligence poses a not-insignificant chance of causing worldwide disaster within the not-too-distant future, anyone who is generally concerned with the future of humanity has reason to be interested."}, {"title": "How can I collect questions for Stampy?", "authors": "", "url": "https://stampy.ai/wiki/How_can_I_collect_questions_for_Stampy%3F", "content": "As well as simply adding your own questions over at \uff3b\uff3bask question\uff3d\uff3d, you could also message your friends with something like:\n\n<blockquote>Hi,<br>\nI'm working on a project to create a comprehensive FAQ about AI alignment (you can read about it here https://stampy.ai/wiki/Stampy%27s_Wiki if interested). We're looking for questions and I thought you may have some good ones. If you'd be willing to write up a google doc with you top 5-10ish questions we'd be happy to write a personalized FAQ for you. https://stampy.ai/wiki/Scope explains the kinds of questions we're looking for.\n\nThanks!</blockquote>\n\nand maybe bring the google doc to a Stampy editing session so we can collaborate on answering them or improving your answers to them."}, {"title": "How can I contact the Stampy team?", "authors": "", "url": "https://stampy.ai/wiki/How_can_I_contact_the_Stampy_team%3F", "content": "The '''\uff3bhttps://discord.com/channels/677546901339504640/677546901339504646 Rob Miles AI Discord\uff3d''' is the hub of all things \uff3b\uff3bStampy\uff3d\uff3d. If you want to be part of the project and don't have access yet, ask plex#1874 on Discord (or \uff3b\uff3bUser_talk:756254556811165756\u250aplex\uff3d\uff3d on wiki).\n\nYou can also talk to us on the \uff3bhttps://discord.gg/cEzKz8QCpa public Discord\uff3d! Try \uff3bhttps://discord.com/channels/893937106194399254/908318480858750986 #suggestions\uff3d or \uff3bhttps://discord.com/channels/893937106194399254/893937106194399257 #general\uff3d, depending on what you want to talk about."}, {"title": "How can I contribute to Stampy?", "authors": "", "url": "https://stampy.ai/wiki/How_can_I_contribute_to_Stampy%3F", "content": "If you're not already there,  join the \uff3b\uff3bDiscord invite\u250aDiscord\uff3d\uff3d where the contributors hang out.\n\nThe main ways you can help are to \uff3b\uff3banswer questions\uff3d\uff3d or \uff3b\uff3badd question\uff3d\uff3ds, or help to \uff3b\uff3breview questions\uff3d\uff3d, \uff3b\uff3breview answers\uff3d\uff3d, or \uff3b\uff3bimprove answers\uff3d\uff3d (instructions for helping out with each of these tasks are on the linked pages). You could also \uff3bhttps://stampy.ai/wiki/How_can_I_join_the_Stampy_dev_team%3F join the dev team\uff3d if you have programming skills."}, {"title": "How can I join the Stampy dev team?", "authors": "", "url": "https://stampy.ai/wiki/How_can_I_join_the_Stampy_dev_team%3F", "content": "The development team works on \uff3bhttps://github.com/StampyAI multiple projects\uff3d in support of Stampy.  Currently, these projects include:\n\n* \uff3bhttps://github.com/StampyAI/stampy-ui Stampy UI\uff3d, which is made mostly in TypeScript.\n* The \uff3bhttps://github.com/StampyAI/stampys_wiki Stampy Wiki\uff3d, which is made mostly in PHP and JavaScript.\n* The Stampy Bot, which is made in Python.\n\nHowever, even if you don\u2019t specialize in any of these areas, do reach out if you would like to help.\n\nTo join, please contact our Project Manager, plex.  You can reach him on discord at plex#1874.  He will be able to point your skills in the right direction to help in the most effective way possible."}, {"title": "How can we interpret what all the neurons mean?", "authors": "", "url": "https://stampy.ai/wiki/How_can_we_interpret_what_all_the_neurons_mean%3F", "content": "Chris Olah, the interpretability legend, is working on looking really hard at all the neurons to see what they all mean. The approach he pioneered is \uff3bhttps://distill.pub/2020/circuits/zoom-in/ circuits\uff3d: looking at computational subgraphs of the network, called circuits, and interpreting those. Idea: \"decompiling the network into a better representation that is more interpretable\". In-context learning via attention heads, and interpretability here seems useful.\n\nOne result I heard about recently: a linear softmax unit stretches space and encourages neuron monosemanticity (making a neuron represent only one thing, as opposed to firing on many unrelated concepts). This makes the network easier to interpret.  \n\nMotivation: The point of this is to get as many bits of information about what neural networks are doing, to hopefully find better abstractions. This diagram gets posted everywhere, the hope being that networks, in the current regime, will become more interpretable because they will start to use abstractions that are closer to human abstractions."}, {"title": "How close do AI experts think we are to creating superintelligence?", "authors": "", "url": "https://stampy.ai/wiki/How_close_do_AI_experts_think_we_are_to_creating_superintelligence%3F", "content": "Nobody knows for sure when we will have AGI, or if we\u2019ll ever get there. \uff3bhttps://www.cold-takes.com/where-ai-forecasting-stands-today/ Open Philanthropy CEO Holden Karnofsky has analyzed a selection of recent expert surveys on the matter, as well as taking into account findings of computational neuroscience, economic history, probabilistic methods and failures of previous AI timeline estimates\uff3d. This all led him to estimate that ''\"there is more than a 10% chance we'll see transformative AI within 15 years (by 2036); a ~50% chance we'll see it within 40 years (by 2060); and a ~2/3 chance we'll see it this century (by 2100).\"'' Karnofsky bemoans the lack of robust expert consensus on the matter and invites rebuttals to his claims in order to further the conversation. He compares AI forecasting to election forecasting (as opposed to academic political science) or market forecasting (as opposed to theoretical academics), thereby arguing that AI researchers may not be the \"experts\u201d we should trust in predicting AI timelines.\n\nOpinions proliferate, but given experts\u2019 (and non-experts\u2019) poor track record at predicting progress in AI, many researchers tend to be fairly agnostic about when superintelligent AI will be invented. \n\nUC-Berkeley AI professor \uff3bhttps://en.wikipedia.org/wiki/Stuart_J._Russell Stuart Russell\uff3d has given his best guess as \u201csometime in our children\u2019s lifetimes\u201d, while \uff3bhttps://en.wikipedia.org/wiki/Ray_Kurzweil Ray Kurzweil\uff3d (Google\u2019s Director of Engineering) predicts human level AI by 2029 and an intelligence explosion by 2045. \uff3bhttps://en.wikipedia.org/wiki/Eliezer_Yudkowsky Eliezer Yudkowsky\uff3d expects \uff3bhttps://www.econlib.org/archives/2017/01/my_end-of-the-w.html the end of the world\uff3d, and \uff3bhttps://en.wikipedia.org/wiki/Elon_Musk Elon Musk\uff3d \uff3bhttps://twitter.com/elonmusk/status/1531328534169493506 expects AGI\uff3d, before 2030.\n\nIf there\u2019s anything like a consensus answer at this stage, it would be something like: \u201chighly uncertain, maybe not for over a hundred years, maybe in less than fifteen, with around the middle of the century looking fairly plausible\u201d."}, {"title": "How could an intelligence explosion be useful?", "authors": "", "url": "https://stampy.ai/wiki/How_could_an_intelligence_explosion_be_useful%3F", "content": "A machine superintelligence, if programmed with the right motivations, could potentially solve all the problems that humans are trying to solve but haven\u2019t had the ingenuity or processing speed to solve yet. A superintelligence might cure disabilities and diseases, achieve world peace, give humans vastly longer and healthier lives, eliminate food and energy shortages, boost scientific discovery and space exploration, and so on.\n\nFurthermore, humanity faces several existential risks in the 21st century, including global nuclear war, bioweapons, superviruses, and \uff3bhttp://www.amazon.com/dp/0198570503/ more\uff3d. A superintelligent machine would be more capable of solving those problems than humans are.\n\nSee also:\n* Yudkowsky, \uff3bhttps://intelligence.org/files/AIPosNegFactor.pdf Artificial intelligence as a positive and negative factor in global risk\uff3d"}, {"title": "How could poorly defined goals lead to such negative outcomes?", "authors": "", "url": "https://stampy.ai/wiki/How_could_poorly_defined_goals_lead_to_such_negative_outcomes%3F", "content": "There is a broad range of possible goals that an AI might possess, but there are a few basic drives that would be useful to almost any of them. These are called instrumentally convergent goals:\n\n# Self preservation. An agent is less likely to achieve its goal if it is not around to see to its completion.\n# Goal-content integrity. An agent is less likely to achieve its goal if its goal has been changed to something else. For example, if you offer Gandhi a pill that makes him want to kill people, he will refuse to take it.\n# Self-improvement. An agent is more likely to achieve its goal if it is more intelligent and better at problem-solving.\n# Resource acquisition. The more resources at an agent\u2019s disposal, the more power it has to make change towards its goal. Even a purely computational goal, such as computing digits of pi, can be easier to achieve with more hardware and energy.\n\nBecause of these drives, even a seemingly simple goal could create an Artificial Superintelligence (ASI) hell-bent on taking over the world\u2019s material resources and preventing itself from being turned off. The classic example is an ASI that was programmed to maximize the output of paper clips at a paper clip factory. The ASI had no other goal specifications other than \u201cmaximize paper clips,\u201d so it converts all of the matter in the solar system into paper clips, and then sends probes to other star systems to create more factories."}, {"title": "How difficult should we expect alignment to be?", "authors": "", "url": "https://stampy.ai/wiki/How_difficult_should_we_expect_alignment_to_be%3F", "content": "Here we ask about the ''additional'' cost of building an aligned powerful system, compare to its unaligned version. We often assume it to be nonzero, in the same way it's easier and cheaper to build an elevator without emergency brakes. This is referred as the '''alignment tax''', and most AI alignment research is geared toward reducing it.\n\n\uff3bhttps://arbital.com/p/aligning_adds_time/ One operational guess\uff3d by Eliezer Yudkowsky about its magnitude is \"\uff3ban aligned project will take\uff3d at least 50% longer serial time to complete than \uff3bits unaligned version\uff3d, or two years longer, whichever is less\". This holds for agents \uff3bhttps://arbital.com/p/sufficiently_advanced_ai/ with enough capability\uff3d that their behavior is qualitatively different from a safety engineering perspective (for instance, an agent that is not \uff3b\uff3bcorrigibility\u250acorrigible\uff3d\uff3d by default).\n\n\uff3bhttps://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default An essay\uff3d by John Wentworth argues for a small chance of alignment happening \"by default\", with an alignment tax of effectively zero."}, {"title": "How do I add content from LessWrong / Effective Altruism Forum tag-wikis to Stampy?", "authors": "", "url": "https://stampy.ai/wiki/How_do_I_add_content_from_LessWrong_/_Effective_Altruism_Forum_tag-wikis_to_Stampy%3F", "content": "You can include a live-updating version of many definitions from LW using the syntax on \uff3b\uff3bTemplate:TagDesc\uff3d\uff3d in the Answer field and \uff3b\uff3bTemplate:TagDescBrief\uff3d\uff3d on the Brief Answer field. Similarly, calling \uff3b\uff3bTemplate:TagDescEAF\uff3d\uff3d and \uff3b\uff3bTemplate:TagDescEAFBrief\uff3d\uff3d will pull from the EAF tag wiki.\n\nWhen available this should be used as it reduces the duplication of effort and directs all editors to improving a single high quality source."}, {"title": "How do I form my own views about AI safety?", "authors": "", "url": "https://stampy.ai/wiki/How_do_I_form_my_own_views_about_AI_safety%3F", "content": "As with most things, the best way to form your views on AI safety is to read up on the various ideas and opinions that knowledgeable people in the field have, and to compare them and form your own perspective. There are several good places to start. One of them is the Machine Intelligence Research Institute`s \uff3bhttps://intelligence.org/why-ai-safety/ \"Why AI safety?\" info page\uff3d. The article contains links to relevant research. The Effective Altruism Forum has an article called \uff3bhttps://forum.effectivealtruism.org/posts/xS9dFE3A6jdooiN7M/how-i-formed-my-own-views-about-ai-safety \"How I formed my own views on AI safety\"\uff3d, which could also be pretty helpful. Here is a Robert Miles youtube video that can be a good place to start as well. Otherwise, there are various articles about it, like \uff3bhttps://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment this one, from Vox\uff3d.\n(youtube)pYXy-A4siMw(/youtube)"}, {"title": "How do I format answers on Stampy?", "authors": "", "url": "https://stampy.ai/wiki/How_do_I_format_answers_on_Stampy%3F", "content": "'''\uff3b\uff3bStampy\uff3d\uff3d''' uses \uff3bhttps://en.wikipedia.org/wiki/Help:Wikitext MediaWiki markup\uff3d, which includes a \uff3bhttps://meta.wikimedia.org/wiki/Help:HTML_in_wikitext limited subset of HTML\uff3d plus the following formatting options:\n\nItems on lists start with *, numbered lists with #\n\n* For external links use \uff3b followed directly by the URL, a space, then display text and finally a \uff3d symbol\n** e.g. (nowiki)\uff3bhttps://www.example.com External link text\uff3d(/nowiki) gives \uff3bhttps://www.example.com External link text\uff3d\n* For internal links write the page title wrapped in \uff3b\uff3b\uff3d\uff3ds\n** e.g. (nowiki)\uff3b\uff3bWhat is the Stampy project?\uff3d\uff3d(/nowiki) gives \uff3b\uff3bWhat is the Stampy project?\uff3d\uff3d. Including a pipe symbol followed by display text e.g. (nowiki)\uff3b\uff3bWhat is the Stampy project?\u250aDisplay Text\uff3d\uff3d(/nowiki) allows you to show different \uff3b\uff3bWhat is the Stampy project?\u250aDisplay Text\uff3d\uff3d.\n* (!ref)Reference notes go inside these tags(/ref)(ref)Note that we use ()s rather than the standard <>s for compatibility with Semantic MediaWiki. The references are automatically added to the bottom of the answer!(/ref)\n* If you post the raw URL of an image from \uff3bhttps://imgur.com/upload imgur\uff3d it will be displayed.(ref)If images seem popular we'll set up local uploads.(/ref) You can reduce file compression if you get an account. Note that you need the image itself, right click -> copy image address to get it<br/>https://i.imgur.com/I3ylPvE.png\n* To embed a YouTube video, use (!youtube)APsK8NST4qE(/youtube) with the video ID of the target video.<br/>(youtube)APsK8NST4qE(/youtube)\n** Start with ** or ## for double indentation\n* Three 's around text -  '''Bold'''\n* Two 's around text Italic - ''Italic''\n\n\ua78a\ua78aHeadings\ua78a\ua78a\nhave \ua78a\ua78aheading here\ua78a\ua78a around them, more \ua78as for smaller headings.\n\n<blockquote>Wrap quotes in < blockquote>< /blockquote> tags (without the spaces)</blockquote>\n\nThere are also (!poem) (/poem) to suppress linebreak removal, (!pre) (/pre) for preformatted text, and (!nowiki) (/nowiki) to not have that content parsed.(ref)() can also be used in place of allowed HTML tags. You can escape a () tag by placing a ! inside the start of the first entry. Be aware that () tags only nest up to two layers deep!(/ref)\n\nWe can pull live descriptions from the LessWrong/Alignment Forum using their identifier fro the URL, for example including the formatting on \uff3b\uff3bTemplate:TagDesc\uff3d\uff3d with orthogonality-thesis as a parameter will render as the full tag description from \uff3bhttps://www.lesswrong.com/tag/orthogonality-thesis the LessWrong tag wiki entry on Orthogonality Thesis\uff3d. \uff3b\uff3bTemplate:TagDescBrief\uff3d\uff3d is similar but will pull only the first paragraph without formatting.\n\nFor tables please use \uff3bhttps://www.w3schools.com/html/html_tables.asp HTML tables\uff3d rather than wikicode tables.\n\nEdit this page to see examples."}, {"title": "How does AI taking things literally contribute to alignment being hard?", "authors": "", "url": "https://stampy.ai/wiki/How_does_AI_taking_things_literally_contribute_to_alignment_being_hard%3F", "content": "Let\u2019s say that you\u2019re the French government a while back. You notice that one of your colonies has too many rats, which is causing economic damage. You have basic knowledge of economics and incentives, so you decide to incentivize the local population to kill rats by offering to buy rat tails at one dollar apiece.\n\nInitially, this works out and your rat problem goes down. But then, an enterprising colony member has the brilliant idea of making a rat farm. This person sells you hundreds of rat tails, costing you hundreds of dollars, but they\u2019re not contributing to solving the rat problem.\n\nSoon other people start making their own rat farms and you\u2019re wasting thousands of dollars buying useless rat tails. You call off the project and stop paying for rat tails. This causes all the people with rat farms to shutdown their farms and release a bunch of rats. Now your colony has an even bigger rat problem.\n\nHere\u2019s another, more made-up example of the same thing happening. Let\u2019s say you\u2019re a basketball talent scout and you notice that height is correlated with basketball performance. You decide to find the tallest person in the world to recruit as a basketball player. Except the reason that they\u2019re that tall is because they suffer from a degenerative bone disorder and can barely walk.\n\nAnother example: you\u2019re the education system and you want to find out how smart students are so you can put them in different colleges and pay them different amounts of money when they get jobs. You make a test called the Standardized Admissions Test (SAT) and you administer it to all the students. In the beginning, this works. However, the students soon begin to learn that this test controls part of their future and other people learn that these students want to do better on the test. The gears of the economy ratchet forwards and the students start paying people to help them prepare for the test. Your test doesn\u2019t stop working, but instead of measuring how smart the students are, it instead starts measuring a combination of how smart they are and how many resources they have to prepare for the test.\n\nThe formal name for the thing that\u2019s happening is Goodhart\u2019s Law. Goodhart\u2019s Law roughly says that if there\u2019s something in the world that you want, like \u201cskill at basketball\u201d or \u201cabsence of rats\u201d or \u201cintelligent students\u201d, and you create a measure that tries to measure this like \u201cheight\u201d or \u201crat tails\u201d or \u201cSAT scores\u201d, then as long as the measure isn\u2019t exactly the thing that you want, the best value of the measure isn\u2019t the thing you want: the tallest person isn\u2019t the best basketball player, the most rat tails isn\u2019t the smallest rat problem, and the best SAT scores aren\u2019t always the smartest students.\n\nIf you start looking, you can see this happening everywhere. Programmers being paid for lines of code write bloated code. If CFOs are paid for budget cuts, they slash purchases with positive returns. If teachers are evaluated by the grades they give, they hand out As indiscriminately.\n\nIn machine learning, this is called specification gaming, and it happens \uff3bhttps://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity frequently\uff3d.\n\nNow that we know what Goodhart\u2019s Law is, I\u2019m going to talk about one of my friends, who I\u2019m going to call Alice. Alice thinks it\u2019s funny to answer questions in a way that\u2019s technically correct but misleading. Sometimes I\u2019ll ask her, \u201cHey Alice, do you want pizza or pasta?\u201d and she responds, \u201cyes\u201d. Because, she sure did want either pizza or pasta. Other times I\u2019ll ask her, \u201chave you turned in your homework?\u201d and she\u2019ll say \u201cyes\u201d because she\u2019s turned in homework at some point in the past; it\u2019s technically correct to answer \u201cyes\u201d. Maybe you have a friend like Alice too.\n\nWhenever this happens, I get a bit exasperated and say something like \u201cyou know what I mean\u201d.\n\nIt\u2019s one of the key realizations in AI Safety that AI systems are always like your friend that gives answers that are technically what you asked for but not what you wanted. Except, with your friend, you can say \u201cyou know what I mean\u201d and they will know what you mean. With an AI system, it won\u2019t know what you mean; you have to explain, which is incredibly difficult.\n\nLet\u2019s take the pizza pasta example. When I ask Alice \u201cdo you want pizza or pasta?\u201d, she knows what pizza and pasta are because she\u2019s been living her life as a human being embedded in an English speaking culture. Because of this cultural experience, she knows that when someone asks an \u201cor\u201d question, they mean \u201cwhich do you prefer?\u201d, not \u201cdo you want at least one of these things?\u201d. Except my AI system is missing the thousand bits of cultural context needed to even understand what pizza is.\n\nWhen you say \u201cyou know what I mean\u201d to an AI system, it\u2019s going to be like \u201cno, I do not know what you mean at all\u201d. It\u2019s not even going to know that it doesn\u2019t know what you mean. It\u2019s just going to say \u201cyes I know what you meant, that\u2019s why I answered \u2018yes\u2019 to your question about whether I preferred pizza or pasta.\u201d (It also might know what you mean, but just not care.)\n\nIf someone doesn\u2019t know what you mean, then it\u2019s really hard to get them to do what you want them to do. For example, let\u2019s say you have a powerful grammar correcting system, which we\u2019ll call Syntaxly+. Syntaxly+ doesn\u2019t quite fix your grammar, it changes your writing so that the reader feels as good as possible after reading it.\n\nPretend it\u2019s the end of the week at work and you haven\u2019t been able to get everything done your boss wanted you to do. You write the following email:\n\n\"Hey boss, I couldn\u2019t get everything done this week. I\u2019m deeply sorry. I\u2019ll be sure to finish it first thing next week.\"\n\nYou then remember you got Syntaxly+, which will make your email sound much better to your boss. You run it through and you get:\n\n\"Hey boss, Great news! I was able to complete everything you wanted me to do this week. Furthermore, I\u2019m also almost done with next week\u2019s work as well.\"\n\nWhat went wrong here? Syntaxly+ is a powerful AI system that knows that emails about failing to complete work cause negative reactions in readers, so it changed your email to be about doing extra work instead.\n\nThis is smart - Syntaxly+ is good at making writing that causes positive reactions in readers. This is also stupid - the system changed the meaning of your email, which is not something you wanted it to do. One of the insights of AI Safety is that AI systems can be simultaneously smart in some ways and dumb in other ways.\n\nThe thing you want Syntaxly+ to do is to change the grammar/style of the email without changing the contents. Except what do you mean by contents? You know what you mean by contents because you are a human who grew up embedded in language, but your AI system doesn\u2019t know what you mean by contents. The phrases \u201cI failed to complete my work\u201d and \u201cI was unable to finish all my tasks\u201d have roughly the same contents, even though they share almost no relevant words.\n\nRoughly speaking, this is why AI Safety is a hard problem. Even basic tasks like \u201cfix the grammar of this email\u201d require a lot of understanding of what the user wants as the system scales in power.\n\nIn Human Compatible, Stuart Russell gives the example of a powerful AI personal assistant. You notice that you accidentally double-booked meetings with people, so you ask your personal assistant to fix it. Your personal assistant reports that it caused the car of one of your meeting participants to break down. Not what you wanted, but technically a solution to your problem.\n\nYou can also imagine a friend from a wildly different culture than you. Would you put them in charge of your dating life? Now imagine that they were much more powerful than you and desperately desired that your dating life to go well. Scary, huh.\n\nIn general, unless you\u2019re careful, you\u2019re going to have this horrible problem where you ask your AI system to do something and it does something that might technically be what you wanted but is stupid. You\u2019re going to be like \u201cwait that wasn\u2019t what I mean\u201d, except your system isn\u2019t going to know what you meant."}, {"title": "How does the stamp eigenkarma system work?", "authors": "", "url": "https://stampy.ai/wiki/How_does_the_stamp_eigenkarma_system_work%3F", "content": "If someone posts something good - something that shows insight, knowledge of AI Safety, etc. - give the message or answer a stamp of approval! \uff3b\uff3bStampy\uff3d\uff3d keeps track of these, and uses them to decide how much he likes each user. You can ask Stampy (in a PM if you like), \"How many stamps am I worth?\", and he'll tell you.\n\nIf something is really very good, especially if it took a lot of work/effort, give it a gold stamp. These are worth 5 regular stamps!\n\nNote that stamps aren't just 'likes', so please don't give stamps to say \"me too\" or \"that's funny\" etc. They're meant to represent knowledge, understanding, good judgement, and contributing to the discord. You can use \ud83d\udcaf or \u2714\ufe0f for things you agree with, \ud83d\ude02 or \ud83e\udd23 for funny things etc.\n\nYour stamp points determine how much say you have if there are disagreements on Stampy content, which channels you have permission to post to, your voting power for approving YouTube replies, and whether you get to invite people.\n\nNotes on stamps and stamp points\n* Stamps awarded by people with a lot of stamp points are worth more\n* Awarding people stamps does not reduce your stamp points\n* New users who have 0 stamp points can still award stamps, they just have no effect. But it's still worth doing because if you get stamp points later, all your previous votes are retroactively updated!\n* Yes, this was kind of tricky to implement! Stampy actually stores how many stamps each user has awarded to every other user, and uses that to build a system of linear scalar equations which is then solved with numpy.\n* Each user has stamp points, and also gives a score to every other user they give stamps to the scores sum to 1 so if I give user A a stamp, my score for them will be 1.0, if I then give user B a stamp, my score for A is 0.5 and B is 0.5, if I give another to B, my score for A goes to 0.3333 and B to 0.66666 and so on\n* Score is \"what proportion of the stamps I've given have gone to this user\"\n* Everyone's stamp points is the sum of (every other user's score for them, times that user's stamp points) so the way to get points is to get stamps from people who have points\n* Rob is the root of the tree, he got one point from Stampy\n* So the idea is the stamp power kind of flows through the network, giving people points for posting things that I thought were good, or posting things that \"people who posted things I thought were good\" thought were good, and so on ad infinitum so for posting YouTube comments, Stampy won't send the comment until it has enough stamps of approval. Which could be a small number of high-points users or a larger number of lower-points users\n* Stamps given to yourself or to stampy do nothing\n\nSo yeah everyone ends up with a number that basically represents what Stampy thinks of them, and you can ask him \"how many stamps am I worth?\" to get that number\n\nso if you have people a, b, and c, the points are calculated by:<br/>\na_points \ua78a (bs_score_for_a * b_points) + (cs_score_for_a * c_points)<br/>\nb_points \ua78a (as_score_for_b * a_points) + (cs_score_for_b * c_points)<br/>\nc_points \ua78a (as_score_for_c * a_points) + (bs_score_for_c * b_points)<br/>\nwhich is tough because you need to know everyone else's score before you can calculate your own<br/>\nbut actually the system will have a fixed point - there'll be a certain arrangement of values such that every node has as much flowing out as flowing in - a stable configuration\nso you can rearrange<br/>\n(bs_score_for_a * b_points) + (cs_score_for_a * c_points) - a_points \ua78a 0<br/>\n(as_score_for_b * a_points) + (cs_score_for_b * c_points) - b_points \ua78a 0<br/>\n(as_score_for_c * a_points) + (bs_score_for_c * b_points) - c_points \ua78a 0<br/>\nor, for neatness:<br/>\n(     -1        * a_points) + (bs_score_for_a * b_points) + (cs_score_for_a * c_points) \ua78a 0<br/>\n(as_score_for_b * a_points) + (     -1        * b_points) + (cs_score_for_b * c_points) \ua78a 0<br/>\n(as_score_for_c * a_points) + (bs_score_for_c * b_points) + (     -1        * c_points) \ua78a 0 <br/>\nand this is just a system of linear scalar equations that you can throw at numpy.linalg.solve<br/>\n(you add one more equation that says rob_points \ua78a 1, so there's some place to start from)\nthere should be one possible distribution of points such that all of the equations hold at the same time, and numpy finds that by linear algebra magic beyond my very limited understanding<br/>\nbut as far as I can tell you can have all the cycles you want!<br/>\n(I actually have the scores sum to slightly less than 1, to have the stamp power slightly fade out as it propagates, just to make sure it doesn't explode. But I don't think I actually need to do that)<br/>\nand yes this means that any time anyone gives a stamp to anyone, ~everyone's points will change slightly<br/>\nAnd yes this means I'm recalculating the matrix and re-solving it for every new stamp, but computers are fast and I'm sure there are cheaper approximations I could switch to later if necessary"}, {"title": "How doomed is humanity?", "authors": "", "url": "https://stampy.ai/wiki/How_doomed_is_humanity%3F", "content": "The opinions from experts are all over the place, according to \uff3bhttps://www.lesswrong.com/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results this 2021 survey\uff3d. Someone has also collected a \uff3bhttps://docs.google.com/spreadsheets/d/1W10B6NJjicD8O0STPiT3tNV3oFnT8YsfjmtYR8RO_RI/edit#gid\ua78a0 database of existential risk estimates\uff3d.\n\nOn the pessimistic end you find people like Eliezer Yudkowsky, \uff3bhttps://forum.effectivealtruism.org/posts/bGBm2yTiLEwwCbL6w/discussion-with-eliezer-yudkowsky-on-agi-interventions who said\uff3d: \"I consider the present gameboard to look incredibly grim, and I don't actually see a way out through hard work alone. We can hope there's a miracle that violates some aspect of my background model, and we can try to prepare for that unknown miracle; preparing for an unknown miracle probably looks like \"Trying to die with more dignity on the mainline\" (because if you can die with more dignity on the mainline, you are better positioned to take advantage of a miracle if it occurs).\"\n\nWhile at the optimistic end you have people like Ben Garfinkel who put the probability at more like 0.1-1% for AI causing an existential catastrophe in the next century, with most people lying somewhere in the middle."}, {"title": "How fast will AI takeoff be?", "authors": "", "url": "https://stampy.ai/wiki/How_fast_will_AI_takeoff_be%3F", "content": "There is significant controversy on how quickly AI will grow into a superintelligence. The \uff3bhttps://www.alignmentforum.org/tag/ai-takeoff Alignment Forum tag\uff3d has many views on how things might unfold, where the probabilities of a soft (happening over years/decades) takeoff and a hard (happening in months, or less) takeoff are discussed."}, {"title": "How is \"intelligence\" defined?", "authors": "", "url": "https://stampy.ai/wiki/How_is_%22intelligence%22_defined%3F", "content": "<p>After reviewing extensive literature on the subject, Legg and Hutter<span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnrefosnb04qur8\"><sup>(ref)<p>http://arxiv.org/pdf/0712.3329.pdf</p>(/ref)</sup></span>&nbsp;summarizes the many possible valuable definitions in the informal statement \u201cIntelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.\u201d They then show this definition can be mathematically formalized given reasonable mathematical definitions of its terms. They use \uff3bhttps://lessestwrong.com/tag/solomonoff-induction Solomonoff induction\uff3d - a formalization of \uff3bhttps://lessestwrong.com/tag/occam-s-razor Occam's razor\uff3d - to construct an \uff3bhttps://lessestwrong.com/tag/aixi universal artificial intelligence\uff3d with a embedded \uff3bhttps://lessestwrong.com/tag/utility-functions utility function\uff3d which assigns less \uff3bhttps://lessestwrong.com/tag/expected-utility utility\uff3d to those actions based on theories with higher \uff3bhttps://wiki.lesswrong.com/wiki/Kolmogorov_complexity complexity\uff3d. They argue this final formalization is a valid, meaningful, informative, general, unbiased, fundamental, objective, universal and practical definition of intelligence.</p><p>We can relate Legg and Hutter's definition with the concept of \uff3bhttps://lessestwrong.com/tag/optimization optimization\uff3d. According to \uff3bhttps://lessestwrong.com/tag/eliezer-yudkowsky Eliezer Yudkowsky\uff3d intelligence is \uff3bhttps://lessestwrong.com/lw/vb/efficient_crossdomain_optimization/ efficient cross-domain optimization\uff3d. It measures an agent's capacity for efficient cross-domain optimization of the world according to the agent\u2019s preferences.<span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnref7hbpdfpe6x3\"><sup>(ref)<p>http://intelligence.org/files/IE-EI.pdf(/ref)</sup></span>&nbsp;Optimization measures not only the capacity to achieve the desired goal but also is inversely proportional to the amount of resources used. It\u2019s the ability to steer the future so it hits that small target of desired outcomes in the large space of all possible outcomes, using fewer resources as possible. For example, when Deep Blue defeated Kasparov, it was able to hit that small possible outcome where it made the right order of moves given Kasparov\u2019s moves from the very large set of all possible moves. In that domain, it was more optimal than Kasparov. However, Kasparov would have defeated Deep Blue in almost any other relevant domain, and hence, he is considered more intelligent.</p><p>One could cast this definition in a possible world vocabulary, intelligence is:</p><ol><li>the ability to precisely realize one of the members of a small set of possible future worlds that have a higher preference over the vast set of all other possible worlds with lower preference; while</li><li>using fewer resources than the other alternatives paths for getting there; and in the</li><li>most diverse domains as possible.</li></ol><p>How many more worlds have a higher preference then the one realized by the agent, less intelligent he is. How many more worlds have a lower preference than the one realized by the agent, more intelligent he is. (Or: How much smaller is the set of worlds at least as preferable as the one realized, more intelligent the agent is). How much less paths for realizing the desired world using fewer resources than those spent by the agent, more intelligent he is. And finally, in how many more domains the agent can be more efficiently optimal, more intelligent he is. Restating it, the intelligence of an agent is directly proportional to:</p><ul><li>(a) the numbers of worlds with lower preference than the one realized,</li><li>(b) how much smaller is the set of paths more efficient than the one taken by the agent and</li><li>(c) how more wider are the domains where the agent can effectively realize his preferences;</li></ul><p>and it is, accordingly, inversely proportional to:</p><ul><li>(d) the numbers of world with higher preference than the one realized,</li><li>(e) how much bigger is the set of paths more efficient than the one taken by the agent and</li><li>(f) how much more narrow are the domains where the agent can efficiently realize his preferences.</li></ul><p>This definition avoids several problems common in many others definitions, especially it avoids \uff3bhttps://lessestwrong.com/tag/anthropomorphism anthropomorphizing\uff3d intelligence.</p><h2>See Also</h2><ul><li>\uff3bhttps://lessestwrong.com/tag/optimization Optimization process\uff3d</li><li>\uff3bhttps://lessestwrong.com/tag/decision-theory Decision theory\uff3d</li><li>\uff3bhttps://lessestwrong.com/tag/rationality Rationality\uff3d</li><li>\uff3bhttp://arxiv.org/pdf/0712.3329.pdf Legg and Hutter paper \u201cUniversal Intelligence: A De\ufb01nition of Machine Intelligence\u201d\uff3d</li></ul><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/general-intelligence?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/general-intelligence?edit\ua78atrue</div>"}, {"title": "How is AGI different from current AI?", "authors": "", "url": "https://stampy.ai/wiki/How_is_AGI_different_from_current_AI%3F", "content": "Current narrow systems are much more domain-specific than AGI. We don\u2019t know what the first AGI will look like, some people think the \uff3bhttps://www.lesswrong.com/posts/6Hee7w2paEzHsD6mn/collection-of-gpt-3-results GPT-3\uff3d architecture but scaled up a lot may get us there (GPT-3 is a giant prediction model which when trained on a vast amount of text seems to \uff3bhttps://www.lesswrong.com/posts/D3hP47pZwXNPRByj8/an-102-meta-learning-by-gpt-3-and-a-list-of-full-proposals learn how to learn\uff3d and do \uff3bhttps://gpt3examples.com/ all sorts of crazy-impressive things\uff3d, a related model can \uff3bhttps://openai.com/blog/dall-e/ generate pictures from text\uff3d), some people don\u2019t think scaling this kind of model will get us all the way."}, {"title": "How is OpenAI planning to solve the full alignment problem?", "authors": "", "url": "https://stampy.ai/wiki/How_is_OpenAI_planning_to_solve_the_full_alignment_problem%3F", "content": "The safety team at OpenAI's plan is to build a \uff3bhttps://aligned.substack.com/p/alignment-mvp MVP aligned AGI\uff3d to try and help us solve the full alignment problem.\n\nThey want to do this with Reinforcement Learning from Human Feedback (RLHF): get feedback from humans about what is good, i.e. give reward to AI's based on the human feedback. Problem: what if the AI makes gigabrain 5D chess moves that humans don't understand, so can't evaluate. Jan Leike, the director of the safety team, views this (\uff3bhttps://ai-alignment.com/the-informed-oversight-problem-1b51b4f66b35 the informed oversight problem\uff3d) as the core difficulty of alignment. Their proposed solution: an AI assisted oversight scheme, with a recursive hierarchy of AIs bottoming out at humans. They are working on experimenting with this approach by trying to get current day AIs to do useful supporting work such as \uff3bhttps://openai.com/blog/summarizing-books/ summarizing books\uff3d and \uff3bhttps://openai.com/blog/critiques/ criticizing itself\uff3d.\n\nOpenAI also published GPT-3, and are continuing to push LLM capabilities, with GPT-4 expected to be released at some point soon.\n\nSee also: \uff3bhttps://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai Common misconceptions about OpenAI\uff3d and \uff3bhttps://openai.com/blog/our-approach-to-alignment-research/ Our approach to alignment research\uff3d."}, {"title": "How is the Alignment Research Center (ARC) trying to solve Eliciting Latent Knowledge (ELK)?", "authors": "", "url": "https://stampy.ai/wiki/How_is_the_Alignment_Research_Center_(ARC)_trying_to_solve_Eliciting_Latent_Knowledge_(ELK)%3F", "content": "ARC is trying to solve \uff3bhttps://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit Eliciting Latent Knowledge (ELK)\uff3d. Suppose that you are training an AI agent that predicts the state of the world and then performs some actions, called a ''predictor''. This predictor is the AGI that will be acting to accomplish goals in the world. How can you create another model, called a ''reporter'', that tells you what the predictor believes about the world? A key challenge in training this reporter is that training your reporter on human labeled training data, by default, incentivizes the predictor to just model what the human thinks is true, because the human is a simpler model than the AI.\n\nMotivation: At a high level, Paul's plan seems to be to produce a minimal AI that can help to do AI safety research. To do this, preventing \uff3bhttps://www.lesswrong.com/posts/ocWqg2Pf2br4jMmKA/does-sgd-produce-deceptive-alignment deception\uff3d and \uff3bhttps://www.lesswrong.com/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem inner alignment failure\uff3d are on the critical path, and the only known solution paths to this require interpretability (this is how all of Evan's \uff3bhttps://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai 11 proposals\uff3d plan to get around this problem).\n\nIf ARC can solve ELK, this would be a very strong form of interpretability: our reporter is able to tell us what the predictor believes about the world. Some ways this could end up being useful for aligning the predictor include:\n\n*Using the reporter to find deceptive/misaligned thoughts in the predictor, and then optimizing against those interpreted thoughts. At any given point in time, SGD only updates the weights a small amount. If an AI becomes misaligned, it won't be very misaligned, and the interpretability tools will be able to figure this out and do a gradient step to make it aligned again. In this way, we can prevent deception at any point in training.\n*Stopping training if the AI is misaligned."}, {"title": "How likely is an \"intelligence explosion\"?", "authors": "", "url": "https://stampy.ai/wiki/How_likely_is_an_%22intelligence_explosion%22%3F", "content": "Conditional on technological progress continuing, it seems extremely likely that there will be an intelligence explosion, as at some point generally capable intelligent systems will tend to become the main drivers of their own development both at a software and hardware level. This would predictably create a feedback cycle of increasingly intelligent systems improving themselves more effectively. It seems like if the compute was used effectively, \uff3bhttps://publicism.info/philosophy/superintelligence/4.html computers have many large advantages over biological cognition\uff3d, so this scaling up might be very rapid if there is a \uff3b\uff3bcomputational overhang\uff3d\uff3d.\n\nSome ways technological progress could stop would be global coordination to stop AI research, global catastrophes severe enough to stop hardware production and maintenance, or hardware reaching physical limits before an intelligence explosion is possible (though this last one seems unlikely, as \uff3bhttps://en.wikipedia.org/wiki/Atomically_precise_manufacturing atomically precise manufacturing\uff3d promises many orders of magnitude of cost reduction and processing power increase, and we're already seeing fairly capable systems on current hardware)."}, {"title": "How likely is it that an AI would pretend to be a human to further its goals?", "authors": "", "url": "https://stampy.ai/wiki/How_likely_is_it_that_an_AI_would_pretend_to_be_a_human_to_further_its_goals%3F", "content": "Talking about full AGI: Fairly likely, but depends on takeoff speed. In a slow takeoff of a misaligned AGI, where it is only weakly superintelligent, manipulating humans would be one of its main options for trying to further its goals for some time. Even in a fast takeoff, it\u2019s plausible that it would at least briefly manipulate humans in order to accelerate its ascent to technological superiority, though depending on what machines are available to hack at the time it may be able to skip this stage.\n\nIf the AI's goals include reference to humans it may have reason to continue deceiving us after it attains technological superiority, but will not necessarily do so. How this unfolds would depend on the details of its goals.\n\nEliezer Yudkowsky gives \uff3bhttps://www.lesswrong.com/posts/pxGYZs2zHJNHvWY5b/request-for-concrete-ai-takeover-mechanisms the example\uff3d of an AI solving protein folding, then mail-ordering synthesised DNA to a bribed or deceived human with instructions to mix the ingredients in a specific order to create \uff3bhttps://en.wikipedia.org/wiki/Atomically_precise_manufacturing wet nanotechnology\uff3d."}, {"title": "How might AGI kill people?", "authors": "", "url": "https://stampy.ai/wiki/How_might_AGI_kill_people%3F", "content": "If we pose a serious threat, it could hack our weapons systems and turn them against us. Future militaries are much more vulnerable to this due to rapidly progressing autonomous weapons. There\u2019s also the option of creating bioweapons and distributing them to the most unstable groups you can find, tricking nations into WW3, or dozens of other things an agent many times smarter than any human with the ability to develop arbitrary technology, hack things (including communications), and manipulate people, or many other possibilities that something smarter than a human could think up. More can be found \uff3bhttps://www.lesswrong.com/posts/pxGYZs2zHJNHvWY5b/request-for-concrete-ai-takeover-mechanisms here\uff3d.\n \nIf we are not a threat, in the course of pursuing its goals it may consume vital resources that humans need (e.g. using land for solar panels instead of farm crops). This video goes into more detail:\n\n(youtube)ZeecOKBus3Q(/youtube)"}, {"title": "How might a superintelligence socially manipulate humans?", "authors": "", "url": "https://stampy.ai/wiki/How_might_a_superintelligence_socially_manipulate_humans%3F", "content": "People tend to imagine AIs as being like nerdy humans \u2013 brilliant at technology but clueless about social skills. There is no reason to expect this \u2013 persuasion and manipulation is a different kind of skill from solving mathematical proofs, but it\u2019s still a skill, and an intellect as far beyond us as we are beyond lions might be smart enough to replicate or exceed the \u201ccharming sociopaths\u201d who can naturally win friends and followers despite a lack of normal human emotions.\n\nA superintelligence might be able to analyze human psychology deeply enough to understand the hopes and fears of everyone it negotiates with. Single humans using psychopathic social manipulation have done plenty of harm \u2013 Hitler leveraged his skill at oratory and his understanding of people\u2019s darkest prejudices to take over a continent. Why should we expect superintelligences to do worse than humans far less skilled than they?\n\nMore outlandishly, a superintelligence might just skip language entirely and figure out a weird pattern of buzzes and hums that causes conscious thought to seize up, and which knocks anyone who hears it into a weird hypnotizable state in which they\u2019ll do anything the superintelligence asks. It sounds kind of silly to me, but then, nuclear weapons probably would have sounded kind of silly to lions sitting around speculating about what humans might be able to accomplish. When you\u2019re dealing with something unbelievably more intelligent than you are, you should probably expect the unexpected."}, {"title": "How might an \"intelligence explosion\" be dangerous?", "authors": "", "url": "https://stampy.ai/wiki/How_might_an_%22intelligence_explosion%22_be_dangerous%3F", "content": "If programmed with the wrong motivations, a machine could be malevolent toward humans, and intentionally exterminate our species. More likely, it could be designed with motivations that initially appeared safe (and easy to program) to its designers, but that turn out to be best fulfilled (given sufficient power) by reallocating resources from sustaining human life to \uff3bhttp://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf other projects\uff3d. As Yudkowsky writes, \u201cthe AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\u201d\n\nSince weak AIs with many different motivations could better achieve their goal by faking benevolence until they are powerful, safety testing to avoid this could be very challenging. Alternatively, competitive pressures, both economic and military, might lead AI designers to try to use other methods to control AIs with undesirable motivations. As those AIs became more sophisticated this could eventually lead to one risk too many.\n\nEven a machine successfully designed with superficially benevolent motivations could easily go awry when it discovers implications of its decision criteria unanticipated by its designers. For example, a superintelligence programmed to maximize human happiness might find it easier to rewire human neurology so that humans are happiest when sitting quietly in jars than to build and maintain a utopian world that caters to the complex and nuanced whims of current human neurology.\n\nSee also:\n\n* Yudkowsky, \uff3bhttps://intelligence.org/files/AIPosNegFactor.pdf Artificial intelligence as a positive and negative factor in global risk\uff3d\n* Chalmers, \uff3bhttp://consc.net/papers/singularity.pdf The Singularity: A Philosophical Analysis\uff3d"}, {"title": "How might an AI achieve a seemingly beneficial goal via inappropriate means?", "authors": "", "url": "https://stampy.ai/wiki/How_might_an_AI_achieve_a_seemingly_beneficial_goal_via_inappropriate_means%3F", "content": "Imagine, for example, that you are tasked with reducing traffic congestion in San Francisco at all costs, i.e. you do not take into account any other constraints. How would you do it? You might start by just timing traffic lights better. But wouldn\u2019t there be less traffic if all the bridges closed down from 5 to 10AM, preventing all those cars from entering the city? Such a measure obviously violates common sense, and subverts the purpose of improving traffic, which is to help people get around \u2013 but it is consistent with the goal of \u201creducing traffic congestion\u201d."}, {"title": "How might non-agentic GPT-style AI cause an intelligence explosion or otherwise contribute to existential risk?", "authors": "", "url": "https://stampy.ai/wiki/How_might_non-agentic_GPT-style_AI_cause_an_intelligence_explosion_or_otherwise_contribute_to_existential_risk%3F", "content": "One threat model which includes a GPT component is \uff3bhttps://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent Misaligned Model-Based RL Agent\uff3d. It suggests that a reinforcement learner attached to a GPT-style world model could lead to an existential risk, with the RL agent being the optimizer which uses the world model to be much more effective at achieving its goals.\n\nAnother possibility is that a sufficiently powerful world model \uff3bhttps://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this may develop mesa optimizers\uff3d which could influence the world via the outputs of the model to achieve the mesa objective (perhaps by causing an optimizer to be created with goals aligned to it), though this is somewhat speculative."}, {"title": "How might things go wrong with AI even without an agentic superintelligence?", "authors": "", "url": "https://stampy.ai/wiki/How_might_things_go_wrong_with_AI_even_without_an_agentic_superintelligence%3F", "content": "Failures can happen with narrow non-agentic systems, mostly from humans not anticipating safety-relevant decisions made too quickly to react, much like in the \uff3bhttps://en.wikipedia.org/wiki/2010_flash_crash 2010 flash crash\uff3d.\n\nA helpful metaphor draws on self-driving cars. By relying more and more on an automated process to make decisions, people become worse drivers as they\u2019re not training themselves to react to the unexpected; then the unexpected happens, the software system itself reacts in an unsafe way and the human is too slow to regain control.\n\nThis generalizes to broader tasks. A human using a powerful system to make better decisions (say, as the CEO of a company) might not understand those very well, get trapped into an equilibrium without realizing it and essentially losing control over the entire process.\n\nMore detailed examples in this vein are described by Paul Christiano in ''\uff3bhttps://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like What failure looks like\uff3d''.\n\nAnother source of failures is AI-mediated stable totalitarianism. The limiting factor in current pervasive surveillance, police and armed forces is manpower; the use of drones and other automated tools decreases the need for personnel to ensure security and extract resources.\n\nAs capabilities improve, political dissent could become impossible, checks and balances would break down as \uff3bhttps://www.youtube.com/watch?v\ua78arStL7niR7gs a minimal number of key actors is needed to stay in power\uff3d."}, {"title": "How might we get from Artificial General Intelligence to a Superintelligent system?", "authors": "", "url": "https://stampy.ai/wiki/How_might_we_get_from_Artificial_General_Intelligence_to_a_Superintelligent_system%3F", "content": "Once a system is at least as capable as top human at AI research, it would tend to become the driver of its own development and initiate a process of recursive self-improvement known as the \uff3b\uff3bWhat is an \"intelligence explosion\"?\u250aintelligence explosion\uff3d\uff3d, leading to an extremely powerful system. A general framing of this process is \uff3bhttps://www.openphilanthropy.org/ Open Philanthropy\uff3d's \uff3bhttps://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/#:~:text\ua78aProcess%20for%20Automating%20Scientific%20and%20Technological%20Advancement%2C%20or%20PASTA Process for Automating Scientific and Technological Advancement (PASTA)\uff3d.\n\nThere is \uff3bhttps://sideways-view.com/2018/02/24/takeoff-speeds/ much\uff3d \uff3bhttps://astralcodexten.substack.com/p/yudkowsky-contra-christiano-on-ai debate\uff3d about whether there would be a notable period where the AI was partially driving its own development, with humans being gradually less and less important, or whether the transition to AI automated AI capability research would be sudden. However, the core idea that there is ''some'' threshold of capabilities beyond which a system would begin to rapidly ascend is hard to reasonably dispute, and is a significant consideration for developing alignment strategies."}, {"title": "How quickly could an AI go from the first indications of problems to an unrecoverable disaster?", "authors": "", "url": "https://stampy.ai/wiki/How_quickly_could_an_AI_go_from_the_first_indications_of_problems_to_an_unrecoverable_disaster%3F", "content": "If the AI system was deceptively aligned (i.e. pretending to be nice until it was in control of the situation) or had been in stealth mode while getting things in place for a takeover, quite possibly within hours. We may get more warning with weaker systems, if the AGI does not feel at all threatened by us, or if a \uff3bhttps://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story complex ecosystem of AI systems is built over time and we gradually lose control\uff3d. \n\nPaul Christiano writes \uff3bhttps://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story a story of alignment failure\uff3d which shows a relatively fast transition."}, {"title": "How would you explain the theory of Infra-Bayesianism?", "authors": "", "url": "https://stampy.ai/wiki/How_would_you_explain_the_theory_of_Infra-Bayesianism%3F", "content": "See Vanessa's \uff3bhttps://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda research agenda\uff3d for more detail. \n\nIf we don't know how to do something given unbounded compute, we are just confused about the thing. Going from thinking that chess was impossible for machines to understanding \uff3bhttps://www.google.com/url?q\ua78ahttps://en.wikipedia.org/wiki/Minimax&sa\ua78aD&source\ua78aeditors&ust\ua78a1661633213196096&usg\ua78aAOvVaw3m8tD5QAEl-XXhvaH4d1v3 minimax\uff3d was a really good step forward for designing chess AIs, ''even though minimax is completely intractable''.\n\nThus, we should seek to figure out how alignment might look in theory, and then try to bridge the theory-practice gap by making our proposal ever more efficient. The first step along this path is to figure out a universal \uff3bhttps://www.alignmentforum.org/tag/reinforcement-learning Reinforcement Learning\uff3d setting that we can place our formal agents in, and then prove regret bounds in.\n\nA key problem in doing this is embeddedness. AIs can't have a perfect self model \u2014 this would be like imagining your ENTIRE brain, inside your brain. There are finite memory constraints. \uff3bhttps://www.lesswrong.com/s/CmrW8fCmSLK7E25sa Infra-Bayesianism\uff3d (IB) is essentially a theory of imprecise probability that lets you specify local / fuzzy things. IB allows agents to have abstract models of themselves, and thus works in an embedded setting.\n\n\uff3bhttps://www.lesswrong.com/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized Infra-Bayesian Physicalism\uff3d (IBP) is an extension of this to RL. IBP allows us to\n*Figure out what agents are running \uff3bby evaluating the counterfactual where the computation of the agent would output something different, and see if the physical universe is different\uff3d.\n*Give a program, classify it as an agent or a non agent, and then find its utility function.\n\nVanessa uses this formalism to describe \uff3bhttps://www.lesswrong.com/posts/dPmmuaz9szk26BkmD/vanessa-kosoy-s-shortform?commentId\ua78avKw6DB9crncovPxED#vKw6DB9crncovPxED PreDCA\uff3d, an alignment proposal based on IBP. This proposal assumes that an agent is an IBP agent, meaning that it is an RL agent with fuzzy probability distributions (along with some other things). The general outline of this proposal is as follows:\n#Find all of the agents that preceded the AI\n#Discard all of these agents that are powerful / non-human like\n#Find all of the utility functions in the remaining agents\n#Use combination of all of these utilities as the agent's utility function\n\nVanessa models an AI as a model based RL system with a WM, a reward function, and a policy derived from the WM + reward. \uff3bhttps://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization She claims that this avoids the sharp left turn\uff3d. The generalization problems come from the world model, but this is dealt with by having an epistemology that doesn't contain \uff3bhttps://www.lesswrong.com/posts/ethRJh2E7mSSjzCay/building-phenomenological-bridges bridge rules\uff3d, and so the true world is the simplest explanation for the observed data.\n\nIt is open to show that this proposal also solves inner alignment, but there is some chance that it does.\n\nThis approach deviates from MIRI's plan, which is to focus on a narrow task to perform the pivotal act, and then add corrigibility. Vanessa instead tries to directly learn the user's preferences, and optimize those."}, {"title": "I want to help out AI alignment without necessarily making major life changes. What are some simple things I can do to contribute?", "authors": "", "url": "https://stampy.ai/wiki/I_want_to_help_out_AI_alignment_without_necessarily_making_major_life_changes._What_are_some_simple_things_I_can_do_to_contribute%3F", "content": "OK, it\u2019s great that you want to help, here are some ideas for ways you could do so without making a huge commitment:\n\n* Learning more about AI alignment will provide you with good foundations for any path towards helping. You could start by absorbing content (e.g. books, videos, posts), and thinking about challenges or possible solutions.\n* Getting involved with the movement by joining a local Effective Altruism or LessWrong group, Rob Miles\u2019s Discord, and/or the AI Safety Slack is a great way to find friends who are interested and will help you stay motivated.\n* Donating to organizations or individuals working on AI alignment, possibly via a \uff3bhttps://funds.effectivealtruism.org/donor-lottery donor lottery\uff3d or the \uff3bhttps://funds.effectivealtruism.org/funds/far-future Long Term Future Fund\uff3d, can be a great way to provide support.\n* \uff3bhttps://stampy.ai/wiki/Answer_questions Writing\uff3d or \uff3bhttps://stampy.ai/wiki/Improve_answers improving answers\uff3d on \uff3bhttps://stampy.ai/wiki/ my wiki\uff3d so that other people can learn about AI alignment more easily is a great way to dip your toe into contributing. You can always ask on the Discord for feedback on things you write.\n* Getting good at giving an AI alignment elevator pitch, and sharing it with people who may be valuable to have working on the problem can make a big difference. However you should avoid putting them off the topic by presenting it in a way which causes them to dismiss it as sci-fi (dos and don\u2019ts in the elevator pitch follow-up question).\n* Writing thoughtful comments on \uff3bhttps://www.lesswrong.com/tag/ai?sortedBy\ua78amagic AI posts on LessWrong\uff3d.\n* Participating in the \uff3bhttps://www.eacambridge.org/agi-safety-fundamentals AGI Safety Fundamentals program\uff3d \u2013 either the AI alignment or governance track \u2013 and then facilitating discussions for it in the following round. The program involves nine weeks of content, with about two hours of readings + exercises per week and 1.5 hours of discussion, followed by four weeks to work on an independent project. As a facilitator, you'll be helping others learn about AI safety in-depth, many of whom are considering a career in AI safety. In the early 2022 round, facilitators were offered a stipend, and this seems likely to be the case for future rounds as well! You can learn more about facilitating in \uff3bhttps://forum.effectivealtruism.org/posts/WtwMy69JKZeHEvykc/contribute-by-facilitating-the-agi-safety-fundamentals this post from December 2021\uff3d."}, {"title": "I want to work on AI alignment. How can I get funding?", "authors": "", "url": "https://stampy.ai/wiki/I_want_to_work_on_AI_alignment._How_can_I_get_funding%3F", "content": "See the \uff3bhttps://www.futurefundinglist.com/ Future Funding List\uff3d for up to date information!\n\nThe organizations which most regularly give grants to individuals working towards AI alignment are the \uff3bhttps://funds.effectivealtruism.org/funds/far-future Long Term Future Fund\uff3d, \uff3bhttp://survivalandflourishing.org/ Survival And Flourishing (SAF)\uff3d, the \uff3bhttps://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/the-open-phil-ai-fellowship OpenPhil AI Fellowship\uff3d and \uff3bhttps://www.openphilanthropy.org/focus/other-areas/early-career-funding-individuals-interested-improving-long-term-future early career funding\uff3d, the \uff3bhttps://grants.futureoflife.org/ Future of Life Institute\uff3d, the \uff3bhttps://www.fhi.ox.ac.uk/aia-fellowship/ Future of Humanity Institute\uff3d, and \uff3bhttps://longtermrisk.org/grantmaking/ the Center on Long-Term Risk Fund\uff3d. If you're able to relocate to the UK, \uff3bhttps://ceealar.org/ CEEALAR (aka the EA Hotel)\uff3d can be a great option as it offers free food and accommodation for up to two years, as well as contact with others who are thinking about these issues. The \uff3bhttps://ftxfuturefund.org/apply/ FTX Future Fund\uff3d only accepts direct applications for $100k+ with an emphasis on massively scaleable interventions, but their \uff3bhttps://ftxfuturefund.org/announcing-our-regranting-program/ regranters\uff3d can make smaller grants for individuals. There are also opportunities from smaller grantmakers which you might be able to pick up if you get involved.\n\nIf you want to work on support or infrastructure rather than directly on research, the \uff3bhttps://funds.effectivealtruism.org/funds/ea-community EA Infrastructure Fund\uff3d may be able to help. In general, you can \uff3bhttps://www.lesswrong.com/posts/5AAFoigbbMqgrTpDh/you-can-talk-to-ea-funds-before-applying talk to EA funds before applying\uff3d.\n\nEach grant source has their own criteria for funding, but in general they are looking for candidates who have evidence that they're keen and able to do good work towards reducing existential risk (for example, by completing an \uff3bhttps://aisafety.camp/ AI Safety Camp\uff3d project), though the EA Hotel in particular has less stringent requirements as they're able to support people at very low cost. If you'd like to talk to someone who can offer advice on applying for funding, \uff3bhttps://www.aisafetysupport.org/ AI Safety Support\uff3d offers \uff3bhttps://calendly.com/aiss free calls\uff3d.\n\nAnother option is to get hired by an organization which works on AI alignment, see the follow-up question for advice on that.\n\nIt's also worth checking the AI Alignment tag on the \uff3bhttps://eafunding.softr.app/ EA funding sources website\uff3d for up-to-date suggestions."}, {"title": "I'm interested in working on AI safety. What should I do?", "authors": "", "url": "https://stampy.ai/wiki/I%27m_interested_in_working_on_AI_safety._What_should_I_do%3F", "content": "AI Safety Support \uff3bhttps://calendly.com/aiss offers free calls\uff3d to advise people interested in a career in AI Safety, so that's a great place to start. We're working on creating a bunch of detailed information for Stampy to use, but in the meantime check out these resources:\n\n* \uff3bhttps://www.eacambridge.org/agi-safety-fundamentals EA Cambridge AGI Safety Fundamentals curriculum\uff3d\n* \uff3bhttps://80000hours.org/articles/ai-safety-syllabus/ 80,000 Hours AI safety syllabus\uff3d\n* \uff3bhttps://docs.google.com/document/d/1RFo7_9JVmt0z8RPwUjB-mUMgCMoUQmsaj2CM5aHvxCw/edit Adam Gleave's Careers in Beneficial AI Research document\uff3d\n* \uff3bhttps://rohinshah.com/faq-career-advice-for-ai-alignment-researchers/ Rohin Shah's FAQ on career advice for AI alignment researchers\uff3d\n* \uff3bhttps://www.aisafetysupport.org/ AI Safety Support\uff3d has lots of other good resources, such as their \uff3bhttps://www.aisafetysupport.org/resources/lots-of-links links page\uff3d, \uff3bhttps://www.google.com/url?q\ua78ahttps%3A%2F%2Fjoin.slack.com%2Ft%2Fai-alignment%2Fshared_invite%2Fzt-fkgwbd2b-kK50z~BbVclOZMM9UP44gw&sa\ua78aD&sntz\ua78a1&usg\ua78aAFQjCNEIiKykU7SJ9LhJBoE3FFaOFOhOSA slack\uff3d, \uff3bhttps://www.aisafetysupport.org/newsletter newsletter\uff3d, and  \uff3bhttps://www.aisafetysupport.org/events/online-events-calendar events calendar\uff3d.\n* \uff3bhttps://docs.google.com/spreadsheets/d/1JyxrfFFrzaQsS3AQ4qJ2aOLGj1aSkBaxkpZCqBX9BOY/edit#gid\ua78a0 Safety-aligned research training programs (under construction).\uff3d"}, {"title": "If AI takes over the world how could it create and maintain the infrastructure that humans currently provide?", "authors": "", "url": "https://stampy.ai/wiki/If_AI_takes_over_the_world_how_could_it_create_and_maintain_the_infrastructure_that_humans_currently_provide%3F", "content": "An unaligned AI would not eliminate humans until it had replacements for the manual labor they provide to maintain civilization (e.g. a more advanced version of \uff3bhttps://en.wikipedia.org/wiki/Tesla_Bot Tesla's Optimus\uff3d). Until that point, it might settle for technologically and socially manipulating humans."}, {"title": "If I only care about helping people alive today, does AI safety still matter?", "authors": "", "url": "https://stampy.ai/wiki/If_I_only_care_about_helping_people_alive_today,_does_AI_safety_still_matter%3F", "content": "This largely depends on when you think AI will be advanced enough to constitute an immediate threat to humanity. This is difficult to estimate, but the field is surveyed at \uff3b\uff3bHow long will it be until transformative AI is created?\uff3d\uff3d, which comes to the conclusion that it is relatively widely believed that AI will transform the world in our lifetimes.\n\nWe probably shouldn't rely too strongly on these opinions as predicting the future is hard. But, due to the enormous damage a misaligned AGI could do, it's worth putting a great deal of effort towards AI alignment even if you just care about currently existing humans (such as yourself)."}, {"title": "If we solve alignment, are we sure of a good future?", "authors": "", "url": "https://stampy.ai/wiki/If_we_solve_alignment,_are_we_sure_of_a_good_future%3F", "content": "If by \u201csolve alignment\u201d you mean build a sufficiently performance-competitive superintelligence which has the goal of \uff3bhttps://www.lesswrong.com/tag/coherent-extrapolated-volition Coherent Extrapolated Volition\uff3d or something else which captures human values, then yes. It would be able to deploy technology near the limits of physics (e.g. \uff3bhttps://en.wikipedia.org/wiki/Atomically_precise_manufacturing atomically precise manufacturing\uff3d) to solve most of the other problems which face us, and steer the future towards a highly positive path for \uff3bhttps://en.wikipedia.org/wiki/Timeline_of_the_far_future perhaps many billions of years\uff3d until the \uff3bhttps://en.wikipedia.org/wiki/Heat_death_of_the_universe heat death of the universe\uff3d (barring more esoteric x-risks like encounters with advanced hostile civilizations, \uff3bhttps://en.wikipedia.org/wiki/False_vacuum_decay false vacuum decay\uff3d, or \uff3bhttps://arxiv.org/ftp/arxiv/papers/1905/1905.05792.pdf simulation shutdown\uff3d).\n\nHowever, if you only have alignment of a superintelligence to a single human you still have the risk of misuse, so this should be at most a short-term solution. For example, what if Google creates a superintelligent AI, and it listens to the CEO of Google, and it\u2019s programmed to do everything exactly the way the CEO of Google would want? Even assuming that the CEO of Google has no hidden unconscious desires affecting the AI in unpredictable ways, this gives one person a lot of power."}, {"title": "Is AI alignment possible?", "authors": "", "url": "https://stampy.ai/wiki/Is_AI_alignment_possible%3F", "content": "Yes, if the superintelligence has goals which include humanity surviving then we would not be destroyed. If those goals are \uff3bhttps://www.lesswrong.com/tag/value-learning fully aligned\uff3d with human well-being, we would in fact find ourselves in a dramatically better place."}, {"title": "Is expecting large returns from AI self-improvement just following an exponential trend line off a cliff?", "authors": "", "url": "https://stampy.ai/wiki/Is_expecting_large_returns_from_AI_self-improvement_just_following_an_exponential_trend_line_off_a_cliff%3F", "content": "Blindly following the trendlines while forecasting technological progress is certainly a risk (affectionately known in AI circles as \u201cpulling a Kurzweill\u201d), but sometimes taking an exponential trend seriously is the right response.\n\nConsider economic doubling times. In 1 AD, the world GDP was about $20 billion; it took a thousand years, until 1000 AD, for that to double to $40 billion. But it only took five hundred more years, until 1500, or so, for the economy to double again. And then it only took another three hundred years or so, until 1800, for the economy to double a third time. \nSomeone in 1800 might calculate the trend line and say this was ridiculous, that it implied the economy would be doubling every ten years or so in the beginning of the 21st century. But in fact, this is how long the economy takes to double these days. To a medieval, used to a thousand-year doubling time (which was based mostly on population growth!), an economy that doubled every ten years might seem inconceivable. To us, it seems normal.\n\nLikewise, in 1965 Gordon Moore noted that semiconductor complexity seemed to double every eighteen months. During his own day, there were about five hundred transistors on a chip; he predicted that would soon double to a thousand, and a few years later to two thousand. \nAlmost as soon as Moore\u2019s Law become well-known, people started saying it was absurd to follow it off a cliff \u2013 such a law would imply a million transistors per chip in 1990, a hundred million in 2000, ten billion transistors on every chip by 2015! More transistors on a single chip than existed on all the computers in the world! Transistors the size of molecules! But of course all of these things happened; the ridiculous exponential trend proved more accurate than the naysayers.\n\nNone of this is to say that exponential trends are always right, just that they are sometimes right even when it seems they can\u2019t possibly be. We can\u2019t be sure that a computer using its own intelligence to discover new ways to increase its intelligence will enter a positive feedback loop and achieve superintelligence in seemingly impossibly short time scales. It\u2019s just one more possibility, a worry to place alongside all the other worrying reasons to expect a moderate or hard takeoff."}, {"title": "Is it possible to block an AI from doing certain things on the Internet?", "authors": "", "url": "https://stampy.ai/wiki/Is_it_possible_to_block_an_AI_from_doing_certain_things_on_the_Internet%3F", "content": "Once an AGI has access to the internet it would be very challenging to meaningfully restrict it from doing things online which it wants to. There are too many options to bypass blocks we may put in place.\n\nIt may be possible to design it so that it does not want to do dangerous things in the first place, or perhaps to set up tripwires so that we notice that it\u2019s trying to do a dangerous thing, though that relies on it not noticing or bypassing the tripwire so should not be the only layer of security."}, {"title": "Is it possible to code into an AI to avoid all the ways a given task could go wrong, and would it be dangerous to try that?", "authors": "", "url": "https://stampy.ai/wiki/Is_it_possible_to_code_into_an_AI_to_avoid_all_the_ways_a_given_task_could_go_wrong,_and_would_it_be_dangerous_to_try_that%3F", "content": "Sort answer: No, and could be dangerous to try.\n\nSlightly longer answer: With any realistic real-world task assigned to an AGI, there are so many ways in which it could go wrong that trying to block them all off by hand is a hopeless task, especially when something smarter than you is trying to find creative new things to do. You run into the \uff3bhttps://arbital.greaterwrong.com/p/nearest_unblocked/ nearest unblocked strategy\uff3d problem.\n\nIt may be dangerous to try this because if you try and hard-code a large number of things to avoid it increases the chance that there\u2019s a bug in your code which causes major problems, simply by increasing the size of your codebase."}, {"title": "Is large-scale automated AI persuasion and propaganda a serious concern?", "authors": "", "url": "https://stampy.ai/wiki/Is_large-scale_automated_AI_persuasion_and_propaganda_a_serious_concern%3F", "content": "Language models can be utilized to produce propaganda by \uff3bhttps://www.technologyreview.com/2020/10/08/1009845/a-gpt-3-bot-posted-comments-on-reddit-for-a-week-and-no-one-noticed/ acting like bots\uff3d and interacting with users on social media. This can be done to push a \uff3bhttps://www.nature.com/articles/d41586-020-03034-5 political agenda\uff3d or to make fringe views appear more popular than they are.\n\n<blockquote>I'm envisioning that in the future there will also be systems where you can input any conclusion that you want to argue (including moral conclusions) and the target audience, and the system will give you the most convincing arguments for it. At that point people won't be able to participate in any online (or offline for that matter) discussions without risking their object-level values being hijacked.\n</blockquote>\n-- \uff3bhttps://www.alignmentforum.org/posts/5bd75cc58225bf06703754b9/autopoietic-systems-and-difficulty-of-agi-alignment?commentId\ua78a5bd75cc58225bf06703754c1 Wei Dei\uff3d, quoted in \uff3bhttps://www.alignmentforum.org/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency Persuasion Tools: AI takeover without AGI or agency?\uff3d\n\nAs of 2022, this is not within the reach of current models. However, on the current trajectory, AI might be able to write articles and produce other media for propagandistic purposes that are superior to human-made ones in not too many years. These could be precisely tailored to individuals, using things like social media feeds and personal digital data.\n\nAdditionally, recommender systems on content platforms like YouTube, Twitter, and Facebook use machine learning, and the content they recommend can influence the opinions of billions of people. Some \uff3bhttps://policyreview.info/articles/analysis/recommender-systems-and-amplification-extremist-content research\uff3d has looked at the tendency for platforms to promote extremist political views and to thereby help radicalize their userbase for example.\n\nIn the long term, misaligned AI might use its persuasion abilities to gain influence and take control over the future. This could look like convincing its operators to let it out of a box, to give it resources or creating political chaos in order to disable mechanisms to prevent takeover as in \uff3bhttps://www.gwern.net/fiction/Clippy this story\uff3d.\n\nSee \uff3bhttps://www.alignmentforum.org/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion Risks from AI persuasion\uff3d for a deep dive into the distinct risks from AI persuasion."}, {"title": "Is the focus on the existential threat of superintelligent AI diverting too much attention from more pressing debates about AI in surveillance and the battlefield, and its potential effects on the economy?", "authors": "", "url": "https://stampy.ai/wiki/Is_the_focus_on_the_existential_threat_of_superintelligent_AI_diverting_too_much_attention_from_more_pressing_debates_about_AI_in_surveillance_and_the_battlefield,_and_its_potential_effects_on_the_economy%3F", "content": "The near term and long term aspects of AI safety are both very important to work on. Research into superintelligence is an important part of the open letter, but the actual concern is very different from the Terminator-like scenarios that most media outlets round off this issue to. A much more likely scenario is a superintelligent system with neutral or benevolent goals that is misspecified in a dangerous way. Robust design of superintelligent systems is a complex interdisciplinary research challenge that will likely take decades, so it is very important to begin the research now, and a large part of the purpose of our research program is to make that happen. That said, the alarmist media framing of the issues is hardly useful for making progress in either the near term or long term domain."}, {"title": "Is there a danger in anthropomorphizing AI\u2019s and trying to understand them in human terms?", "authors": "", "url": "https://stampy.ai/wiki/Is_there_a_danger_in_anthropomorphizing_AI%E2%80%99s_and_trying_to_understand_them_in_human_terms%3F", "content": "Using some human-related metaphors (e.g. what an AGI \u2018wants\u2019 or \u2018believes\u2019) is almost unavoidable, as our language is built around experiences with humans, but we should be aware that these may lead us astray.\n\nMany paths to AGI would result in a mind very different from a human or animal, and it would be hard to predict in detail how it would act. We should not trust intuitions trained on humans to predict what an AGI or superintelligence would do. High fidelity Whole Brain Emulations are one exception, where we would expect the system to at least initially be fairly human, but it may diverge depending on its environment and what modifications are applied to it.\n\nThere has been some discussion about how language models trained on lots of human-written text seem likely to pick up human concepts and think in a somewhat human way, and how we could \uff3bhttps://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default use this to improve alignment\uff3d."}, {"title": "Is this about AI systems becoming malevolent or conscious and turning on us?", "authors": "", "url": "https://stampy.ai/wiki/Is_this_about_AI_systems_becoming_malevolent_or_conscious_and_turning_on_us%3F", "content": "<blockquote>The problem isn\u2019t consciousness, but competence. You make machines that are incredibly competent at achieving objectives and they will cause accidents in trying to achieve those objectives.\n\n- Stuart Russell </blockquote>\n\nWork on AI alignment is not concerned with the question of whether \u201cconsciousness\u201d, \u201csentience\u201d or \u201cself-awareness\u201d could arise in a machine or an algorithm. Unlike the frequently-referenced plotline in the Terminator movies, the standard catastrophic misalignment scenarios under discussion do not require computers to become conscious; they only require conventional computer systems (although usually faster and more powerful ones than those available today) blindly and deterministically following logical steps, in the same way that they currently do.\n\nThe primary concern (\u201cAI misalignment\u201d) is that powerful systems could inadvertently be programmed with goals that do not fully capture what the programmers actually want. The AI would then harm humanity in pursuit of goals which seemed benign or neutral. Nothing like malevolence or consciousness would need to be involved. A number of researchers studying the problem have concluded that it is surprisingly difficult to guard against this effect, and that it is likely to get much harder as the systems become more capable. AI systems are inevitably goal-directed and could, for example, consider our efforts to control them (or switch them off) as being impediments to attaining their goals."}, {"title": "Isn\u2019t AI just a tool like any other? Won\u2019t it just do what we tell it to?", "authors": "", "url": "https://stampy.ai/wiki/Isn%E2%80%99t_AI_just_a_tool_like_any_other%3F_Won%E2%80%99t_it_just_do_what_we_tell_it_to%3F", "content": "It likely will \u2013 however, intelligence is, by many definitions, the ability to figure out how to accomplish goals. Even in today\u2019s advanced AI systems, the builders assign the goal but don\u2019t tell the AI exactly how to accomplish it, nor necessarily predict in detail how it will be done; indeed those systems often solve problems in creative, unpredictable ways. Thus the thing that makes such systems intelligent is precisely what can make them difficult to predict and control. They may therefore attain the goal we set them via means inconsistent with our preferences."}, {"title": "I\u2019d like to get deeper into the AI alignment literature. Where should I look?", "authors": "", "url": "https://stampy.ai/wiki/I%E2%80%99d_like_to_get_deeper_into_the_AI_alignment_literature._Where_should_I_look%3F", "content": "The \uff3bhttps://www.eacambridge.org/technical-alignment-curriculum AGI Safety Fundamentals Course\uff3d is a arguably the best way to get up to speed on alignment, you can sign up to go through it with many other people studying and mentorship or read their materials independently.\n\nOther great ways to explore include:\n* \uff3bhttps://axrp.net/ AXRP\uff3d is a podcast with high quality interviews with top alignment researchers.\n* The \uff3bhttps://ai-safety-papers.quantifieduncertainty.org/ AI Safety Papers database\uff3d is a search and browsing interface for most of the transformative AI literature.\n* Reading posts on the \uff3bhttps://www.alignmentforum.org/ Alignment Forum\uff3d can be valuable (see their \uff3bhttps://www.alignmentforum.org/library curated posts\uff3d and \uff3bhttps://www.alignmentforum.org/tag/ai tags\uff3d).\n* Taking a deep dive into Yudkowsky's models of the challenges to aligned AI, via the \uff3bhttps://arbital.greaterwrong.com/explore/ai_alignment/ Arbital Alignment pages\uff3d.\n* Signing up to the \uff3bhttps://rohinshah.com/alignment-newsletter/ Alignment Newsletter\uff3d for an overview of current developments, and reading through some of the archives (or listening to \uff3bhttps://alignment-newsletter.libsyn.com/ the podcast\uff3d).\n* Reading some of \uff3b\uff3bWhere can I learn about AI alignment?\u250athe introductory books\uff3d\uff3d.\n* More on \uff3bhttps://www.aisafetysupport.org/resources/lots-of-links#h.6s2gcz1p5l6z AI Safety Support's list of links\uff3d,  \uff3bhttps://docs.google.com/spreadsheets/d/1QSEWjXZuqmG6ORkig84V4sFCldIntyuQj7yq3gkDo0U/edit#gid\ua78a0 Nonlinear's list of technical courses, reading lists, and curriculums\uff3d, \uff3bhttps://stampy.ai/wiki/Canonical_answers Stampy's canonical answers list\uff3d, and \uff3bhttps://vkrakovna.wordpress.com/ai-safety-resources/ Vika's resources list\uff3d.\n\nYou might also want to consider reading \uff3bhttps://www.lesswrong.com/rationality Rationality: A-Z\uff3d which covers a lot of skills that are valuable to acquire for people trying to think about large and complex issues, with \uff3bhttps://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795 The Rationalist's Guide to the Galaxy\uff3d available as a shorter and more accessible AI-focused option."}, {"title": "Might an \"intelligence explosion\" never occur?", "authors": "", "url": "https://stampy.ai/wiki/Might_an_%22intelligence_explosion%22_never_occur%3F", "content": "\uff3bhttp://www.amazon.com/dp/0060110821/ Dreyfus\uff3d and \uff3bhttp://www.amazon.com/dp/0195106466/ Penrose\uff3d have argued that human cognitive abilities can\u2019t be emulated by a computational machine. \uff3bhttp://citeseerx.ist.psu.edu/viewdoc/download?doi\ua78a10.1.1.120.749&rep\ua78arep1&type\ua78apdf Searle\uff3d and \uff3bhttp://citeseerx.ist.psu.edu/viewdoc/download?doi\ua78a10.1.1.4.5828&rep\ua78arep1&type\ua78apdf Block\uff3d argue that certain kinds of machines cannot have a mind (consciousness, intentionality, etc.). But these objections \uff3bhttp://consc.net/papers/singularity.pdf need not concern\uff3d those who predict an intelligence explosion.\n\nWe can reply to Dreyfus and Penrose by noting that an intelligence explosion does not require an AI to be a classical computational system. And we can reply to Searle and Block by noting that an intelligence explosion does not depend on machines having consciousness or other properties of \u2018mind\u2019, only that it be able to solve problems better than humans can in a wide variety of unpredictable environments. As Edsger Dijkstra once said, the question of whether a machine can \u2018really\u2019 think is \u201cno more interesting than the question of whether a submarine can swim.\u201d\n\n\uff3bhttp://sethbaum.com/ac/2011_AI-Experts.pdf Others\uff3d who are pessimistic about an intelligence explosion occurring within the next few centuries don\u2019t have a specific objection but instead think there are hidden obstacles that will reveal themselves and slow or halt progress toward machine superintelligence.\n\nFinally, a global catastrophe like nuclear war or a large asteroid impact could so damage human civilization that the intelligence explosion never occurs. Or, \uff3bhttps://oxford.universitypressscholarship.com/view/10.1093/oso/9780198570509.001.0001/isbn-9780198570509-book-part-29 a stable and global totalitarianism\uff3d could prevent the technological development required for an intelligence explosion to occur."}, {"title": "OK, I\u2019m convinced. How can I help?", "authors": "", "url": "https://stampy.ai/wiki/OK,_I%E2%80%99m_convinced._How_can_I_help%3F", "content": "Great! I\u2019ll ask you a few follow-up questions to help figure out how you can best contribute, give you some advice, and link you to resources which should help you on whichever path you choose. Feel free to scroll up and explore multiple branches of the FAQ if you want answers to more than one of the questions offered :)\n\nNote: We\u2019re still building out and improving this tree of questions and answers, any feedback is appreciated.\n\n'''At what level of involvement were you thinking of helping?'''\n\nPlease view and suggest to this google doc for improvements: https://docs.google.com/document/d/1S-CUcoX63uiFdW-GIFC8wJyVwo4VIl60IJHodcRfXJA/edit#"}, {"title": "Once we notice that a superintelligence given a specific task is trying to take over the world, can\u2019t we turn it off, reprogram it or otherwise correct the problem?", "authors": "", "url": "https://stampy.ai/wiki/Once_we_notice_that_a_superintelligence_given_a_specific_task_is_trying_to_take_over_the_world,_can%E2%80%99t_we_turn_it_off,_reprogram_it_or_otherwise_correct_the_problem%3F", "content": "We would not be able to turn off or reprogram a superintelligence gone rogue by default. Once in motion the superintelligence is now focused on completing its task. Suppose that it has a goal of calculating as many digits of pi as possible. Its current plan will allow it to calculate two hundred trillion such digits. But if it were turned off, or reprogrammed to do something else, that would result in it calculating zero digits. An entity fixated on calculating as many digits of pi as possible will work hard to prevent scenarios where it calculates zero digits of pi. Just by programming it to calculate digits of pi, we would have given it a drive to prevent people from turning it off.\n\nUniversity of Illinois computer scientist Steve Omohundro argues that entities with very different final goals \u2013 calculating digits of pi, curing cancer, helping promote human flourishing \u2013 will all share a few basic ground-level subgoals. First, self-preservation \u2013 no matter what your goal is, it\u2019s less likely to be accomplished if you\u2019re too dead to work towards it. Second, goal stability \u2013 no matter what your goal is, you\u2019re more likely to accomplish it if you continue to hold it as your goal, instead of going off and doing something else. Third, power \u2013 no matter what your goal is, you\u2019re more likely to be able to accomplish it if you have lots of power, rather than very little. \uff3bhttps://intelligence.org/files/BasicAIDrives.pdf Here\u2019s the full paper\uff3d.\n\nSo just by giving a superintelligence a simple goal like \u201ccalculate digits of pi\u201d, we would have accidentally given it convergent instrumental goals like \u201cprotect yourself\u201d, \u201cdon\u2019t let other people reprogram you\u201d, and \u201cseek power\u201d.\n\nAs long as the superintelligence is safely contained, there\u2019s not much it can do to resist reprogramming. But it\u2019s hard to consistently contain a hostile superintelligence."}, {"title": "Superintelligence sounds like science fiction. Do people think about this in the real world?", "authors": "", "url": "https://stampy.ai/wiki/Superintelligence_sounds_like_science_fiction._Do_people_think_about_this_in_the_real_world%3F", "content": "Many of the people with the deepest understanding of artificial intelligence are concerned about the risks of unaligned superintelligence. In 2014, Google bought world-leading artificial intelligence startup \uff3bhttps://en.wikipedia.org/wiki/DeepMind DeepMind\uff3d for $400 million; DeepMind added the condition that Google promise to set up an AI Ethics Board. DeepMind cofounder Shane Legg has said in interviews that he believes superintelligent AI will be ''\u201csomething approaching absolute power\u201d'' and ''\u201cthe number one risk for this century\u201d.''\n\n\uff3bhttps://en.wikipedia.org/wiki/Stuart_J._Russell#Career_and_research Stuart Russell\uff3d, Professor of Computer Science at Berkeley, author of the standard AI textbook, and world-famous AI expert, warns of ''\u201cspecies-ending problems\u201d'' and wants his field to pivot to make superintelligence-related risks a central concern. He went so far as to write \uff3bhttps://en.wikipedia.org/wiki/Human_Compatible Human Compatible\uff3d, a book focused on bringing attention to the dangers of artificial intelligence and the need for more work to address them.\n\nMany other science and technology leaders agree. Late astrophysicist \uff3bhttps://en.wikipedia.org/wiki/Stephen_Hawking#Future_of_humanity Stephen Hawking\uff3d said that superintelligence ''\u201ccould spell the end of the human race.\u201d'' Tech billionaire \uff3bhttps://en.wikipedia.org/wiki/Bill_Gates#Post-Microsoft Bill Gates\uff3d describes himself as ''\u201cin the camp that is concerned about superintelligence\u2026I don\u2019t understand why some people are not concerned\u201d.'' Oxford Professor \uff3bhttps://www.theguardian.com/technology/2016/jun/12/nick-bostrom-artificial-intelligence-machine Nick Bostrom\uff3d, who has been studying AI risks for over 20 years, has said: ''\u201cSuperintelligence is a challenge for which we are not ready now and will not be ready for a long time.\u201d''\n\n\uff3bhttps://en.wikipedia.org/wiki/Holden_Karnofsky Holden Karnofsky\uff3d, the CEO of \uff3bhttps://www.openphilanthropy.org/ Open Philanthropy\uff3d, has written a carefully reasoned account of why transformative artificial intelligence means that this might be \uff3bhttps://www.cold-takes.com/most-important-century/ the most important century\uff3d."}, {"title": "We\u2019re going to merge with the machines so this will never be a problem, right?", "authors": "", "url": "https://stampy.ai/wiki/We%E2%80%99re_going_to_merge_with_the_machines_so_this_will_never_be_a_problem,_right%3F", "content": "The concept of \u201cmerging with machines,\u201d as popularized by Ray Kurzweil, is the idea that we will be able to put computerized elements into our brains that enhance us to the point where we ourselves are the AI, instead of creating AI outside of ourselves.\n\nWhile this is a possible outcome, there is little reason to suspect that it is the most probable. The amount of computing power in your smart-phone took up an entire room of servers 30 years ago. Computer technology starts big, and then gets refined. Therefore, if \u201cmerging with the machines\u201d requires hardware that can fit inside our brain, it may lag behind the first generations of the technology being developed. This concept of merging also supposes that we can even figure out how to implant computer chips that interface with our brain in the first place, we can do it before the invention of advanced AI, society will accept it, and that computer implants can actually produce major intelligence gains in the human brain. Even if we could successfully enhance ourselves with brain implants before the invention of Artificial Superintelligence (ASI), there is no way to guarantee that this would protect us from negative outcomes, and an ASI with ill-defined goals could still pose a threat to us.\n\nIt's not that Ray Kurzweil's ideas are impossible, it's just that his predictions are too specific, confident, and reliant on strange assumptions."}, {"title": "What approaches are AI alignment organizations working on?", "authors": "", "url": "https://stampy.ai/wiki/What_approaches_are_AI_alignment_organizations_working_on%3F", "content": "Each major organization has a different approach. The \uff3bhttps://www.lesswrong.com/tag/research-agendas research agendas are detailed and complex\uff3d (see also \uff3bhttps://aiwatch.issarice.com/ AI Watch\uff3d). Getting more brains working on any of them (and more money to fund them) may pay off in a big way, but it\u2019s very hard to be confident which (if any) of them will actually work.\n \nThe following is a massive oversimplification, each organization actually pursues many different avenues of research, read the \uff3bhttps://www.lesswrong.com/posts/pTYDdcag9pTzFQ7vw/2020-ai-alignment-literature-review-and-charity-comparison 2020 AI Alignment Literature Review and Charity Comparison\uff3d for much more detail. That being said:\n \n* The \uff3bhttps://intelligence.org/research-guide/ Machine Intelligence Research Institute\uff3d focuses on foundational mathematical research to understand reliable reasoning, which they think is necessary to provide anything like an assurance that a seed AI built will do good things if activated.\n* The \uff3bhttps://humancompatible.ai Center for Human-Compatible AI\uff3d focuses on \uff3bhttps://www.lesswrong.com/tag/inverse-reinforcement-learning Cooperative Inverse Reinforcement Learning\uff3d and \uff3bhttps://www.lesswrong.com/posts/qPoaA5ZSedivA4xJa/our-take-on-chai-s-research-agenda-in-under-1500-words Assistance Games\uff3d, a new paradigm for AI where they try to optimize for doing the kinds of things humans want rather than for a pre-specified utility function\n* \uff3bhttps://ai-alignment.com/ Paul Christano\uff3d's \uff3bhttps://alignmentresearchcenter.org/ Alignment Research Center\uff3d focuses is on \uff3bhttps://www.lesswrong.com/posts/YTq4X6inEudiHkHDF/prosaic-ai-alignment prosaic alignment\uff3d, particularly on creating tools that empower humans to understand and guide systems much smarter than ourselves. His methodology is explained on \uff3bhttps://ai-alignment.com/my-research-methodology-b94f2751cb2c his blog\uff3d.\n* The \uff3bhttps://www.fhi.ox.ac.uk Future of Humanity Institute\uff3d does work on \uff3bhttps://www.lesswrong.com/tag/crucial-considerations crucial considerations\uff3d and other x-risks, as well as AI safety research and outreach.\n* \uff3bhttps://www.anthropic.com/ Anthropic\uff3d is a new organization exploring natural language, human feedback, scaling laws, reinforcement learning, code generation, and interpretability.\n* \uff3bhttp://openai.com OpenAI\uff3d is in a state of flux after major changes to their safety team.\n* \uff3bhttps://medium.com/@deepmindsafetyresearch DeepMind\uff3d\u2019s safety team is working on various approaches designed to work with modern machine learning, and does some communication via the \uff3bhttps://rohinshah.com/alignment-newsletter/ Alignment Newsletter\uff3d.\n* \uff3bhttps://www.eleuther.ai/ EleutherAI\uff3d is a Machine Learning collective aiming to build large open source language models to allow more alignment research to take place.\n* \uff3bhttps://ought.org/ Ought\uff3d is a research lab that develops mechanisms for delegating open-ended thinking to advanced machine learning systems.\n\nThere are many other projects around AI Safety, such as \uff3bhttps://www.youtube.com/watch?v\ua78a7i_f4Kbpgn4 the Windfall clause\uff3d, \uff3bhttps://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg Rob Miles\u2019s YouTube channel\uff3d, \uff3bhttps://www.aisafetysupport.org/ AI Safety Support\uff3d, etc."}, {"title": "What are \"human values\"?", "authors": "", "url": "https://stampy.ai/wiki/What_are_%22human_values%22%3F", "content": "<p><strong>Human Values</strong> are the things we care about, and would want an aligned superintelligence to look after and support. It is suspected that true human values are \uff3bhttps://www.lesswrong.com/tag/complexity-of-value highly complex\uff3d, and could be extrapolated into a wide variety of forms.</p><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/human-values?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/human-values?edit\ua78atrue</div>"}, {"title": "What are \"scaling laws\" and how are they relevant to safety?", "authors": "", "url": "https://stampy.ai/wiki/What_are_%22scaling_laws%22_and_how_are_they_relevant_to_safety%3F", "content": "'''Scaling laws''' are observed trends on the performance of large machine learning models. \n\nIn the field of ML, better performance is usually achieved through better algorithms, better inputs, or using larger amounts of parameters, computing power, or data. Since the 2010s, advances in deep learning have shown experimentally that the easier and faster returns come from '''scaling''', an observation that has been described by Richard Sutton as the ''\uff3bhttp://www.incompleteideas.net/IncIdeas/BitterLesson.html bitter lesson\uff3d''.\n\nWhile deep learning as a field has long struggled to scale models up while retaining learning capability (with such problems as \uff3bhttps://en.wikipedia.org/wiki/Catastrophic_interference catastrophic interference\uff3d), more recent methods, especially the Transformer model architecture, were able to ''just work'' by feeding them more data, and as the meme goes, \uff3bhttps://www.gwern.net/images/rl/2017-12-24-meme-nnlayers-alphagozero.jpg stacking more layers\uff3d.\n\nMore surprisingly, performance (in terms of absolute likelihood loss, a standard measure) appeared to increase ''smoothly'' with compute, or dataset size, or parameter count. Which gave rise to '''scaling laws''', the trend lines suggested by performance gains, from which returns on data/compute/time investment could be extrapolated.\n\nA companion to this purely descriptive law (no strong theoretical explanation of the phenomenon has been found yet), is the '''scaling hypothesis''', which \uff3bhttps://www.gwern.net/Scaling-hypothesis#scaling-hypothesis Gwern Branwen describes\uff3d:\n\n<blockquote>The ''strong scaling hypothesis'' is that, once we find a scalable architecture like self-attention or convolutions, \uff3b...\uff3d we can simply train ever larger \uff3bneural networks\uff3d and ever more sophisticated behavior will emerge naturally as the easiest way to optimize for all the tasks & data.</blockquote>\n\nThe scaling laws, if the above hypothesis holds, become highly relevant to safety insofar capability gains become conceptually easier to achieve: no need for clever designs to solve a given task, just throw more processing at it and it will eventually yield. As \uff3bhttps://ai-alignment.com/prosaic-ai-control-b959644d79c2 Paul Christiano observes\uff3d:\n\n<blockquote>It now seems possible that we could build \u201cprosaic\u201d AGI, which can replicate human behavior but doesn\u2019t involve qualitatively new ideas about \u201chow intelligence works\u201d.</blockquote>\n\nWhile the scaling laws still hold experimentally at the time of this writing (July 2022), whether they'll continue up to safety-relevant capabilities is still an open problem."}, {"title": "What are Encultured working on?", "authors": "", "url": "https://stampy.ai/wiki/What_are_Encultured_working_on%3F", "content": "See \uff3bhttps://www.lesswrong.com/posts/ALkH4o53ofm862vxc/announcing-encultured-ai-building-a-video-game Encultured AI: Building a Video Game\uff3d.\n\nEncultured are making a multiplayer online video game as a test environment for AI: an aligned AI should be able to play the game without ruining the fun or doing something obviously destructive like completely taking over the world, even if it has this capabilities. This seems roughly analogous to setting an AGI loose on the real world.\n\nMotivation: Andrew Critch is primarily concerned about a \uff3bhttps://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic multipolar AI scenario\uff3d: there are multiple actors with comparably powerful AI, on the cusp of recursive self improvement. The worst case is a race, and even though each actor would want to take more time checking their AGI for safety, worry that another actor will deploy will push each actor to take shortcuts and try to pull off a world-saving act. Instead of working directly on AI, which can accelerate timelines and encourage racing, creating this standardized test environment where alignment failures are observable is one component of a good global outcome."}, {"title": "What are Scott Garrabrant and Abram Demski working on?", "authors": "", "url": "https://stampy.ai/wiki/What_are_Scott_Garrabrant_and_Abram_Demski_working_on%3F", "content": "They are working on fundamental problems like \uff3bhttps://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version embeddedness, decision theory, logical counterfactuals\uff3d, and more. A big advance was \uff3bhttps://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames Cartesian Frames\uff3d, a formal model of agency, and \uff3bhttps://www.alignmentforum.org/posts/N5Jm6Nj4HkNKySA5Z/finite-factored-sets Finite Factored Sets\uff3d which reframes time in a way which is more compatible with agency."}, {"title": "What are alternate phrasings for?", "authors": "", "url": "https://stampy.ai/wiki/What_are_alternate_phrasings_for%3F", "content": "Alternate phrasings are used to improve the semantic search which Stampy uses to serve people questions, by giving alternate ways to say a question which might trigger a match when the main wording won't. They should generally only be used when there is a significantly different wording, rather than for only very minor changes."}, {"title": "What are brain-computer interfaces?", "authors": "", "url": "https://stampy.ai/wiki/What_are_brain-computer_interfaces%3F", "content": "A brain-computer interface (BCI) is a direct communication pathway between the brain and a computer device. BCI research is heavily funded, and has already met dozens of successes. Three successes in human BCIs are \uff3bhttp://archives.cnn.com/2002/HEALTH/06/13/cov.bionic.eye/index.html a device\uff3d that restores (partial) sight to the blind, \uff3bhttp://en.wikipedia.org/wiki/Cochlear_implant cochlear implants\uff3d that restore hearing to the deaf, and \uff3bhttps://pubmed.ncbi.nlm.nih.gov/16838014/ a device\uff3d that allows use of an artificial hand by direct thought.\n\nSuch device restore impaired functions, but many researchers expect to also augment and improve normal human abilities with BCIs. \uff3bhttp://edboyden.org/ Ed Boyden\uff3d is researching these opportunities as the lead of the \uff3bhttp://syntheticneurobiology.org/ Synthetic Neurobiology Group\uff3d at MIT. Such devices might hasten the arrival of an intelligence explosion, if only by improving human intelligence so that the hard problems of AI can be solved more rapidly.\n\nSee also:\n\nWikipedia, \uff3bhttp://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface Brain-computer interface\uff3d"}, {"title": "What are language models?", "authors": "", "url": "https://stampy.ai/wiki/What_are_language_models%3F", "content": "<p><strong>Language Models</strong> are a class of \uff3bhttps://www.lesswrong.com/tag/ai AI\uff3d trained on text, usually to predict the next word or a word which has been obscured. They have the ability to generate novel prose or code based on an initial prompt, which gives rise to a kind of natural language programming called prompt engineering. The most popular architecture for very large language models is called a \uff3bhttps://en.wikipedia.org/wiki/Transformer_(machine_learning_model) transformer\uff3d, which follows consistent \uff3bhttps://www.lesswrong.com/tag/scaling-laws scaling laws\uff3d with respect to the size of the model being trained, meaning that a larger model trained with the same amount of compute will produce results which are better by a predictable amount (when measured by the 'perplexity', or how surprised the AI is by a test set of human-generated text).</p><h3>See also</h3><ul><li>\uff3bhttps://www.lesswrong.com/tag/gpt GPT\uff3d - A family of large language models created by \uff3bhttps://www.lesswrong.com/tag/openai OpenAI\uff3d</li></ul><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/language-models?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/language-models?edit\ua78atrue</div>"}, {"title": "What are mesa-optimizers?", "authors": "", "url": "https://stampy.ai/wiki/What_are_mesa-optimizers%3F", "content": "<p><strong>Mesa-Optimization</strong> is the situation that occurs when a learned model (such as a neural network) is itself an optimizer. In this situation, a <i>base optimizer</i> creates a second optimizer, called a <i>mesa-optimizer</i>. The primary reference work for this concept is Hubinger et al.'s \"\uff3bhttps://www.alignmentforum.org/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction Risks from Learned Optimization in Advanced Machine Learning Systems\uff3d\".</p><p>Example: Natural selection is an optimization process that optimizes for reproductive fitness. Natural selection produced humans, who are themselves optimizers. Humans are therefore mesa-optimizers of natural selection.</p><p>In the context of AI alignment, the concern is that a base optimizer (e.g., a gradient descent process) may produce a learned model that is itself an optimizer, and that has unexpected and undesirable properties. Even if the gradient descent process is in some sense \"trying\" to do exactly what human developers want, the resultant mesa-optimizer will not typically be trying to do the exact same thing.\uff3bhttps://lessestwrong.com/tag/mesa-optimization?revision\ua78a0.0.3#fn1 <sup>1</sup>\uff3d</p><p>&nbsp;</p><h2>History</h2><p>Previously work under this concept was called <i>Inner Optimizer </i>or <i>Optimization Daemons.</i></p><p>\uff3bhttps://www.lesswrong.com/users/wei_dai Wei Dai\uff3d brings up a similar idea in an SL4 thread.\uff3bhttps://lessestwrong.com/tag/mesa-optimization?revision\ua78a0.0.3#fn2 <sup>2</sup>\uff3d</p><p>The optimization daemons article on \uff3bhttps://arbital.com/ Arbital\uff3d was published probably in 2016.\uff3bhttps://lessestwrong.com/tag/mesa-optimization?revision\ua78a0.0.3#fn3 <sup>3</sup>\uff3d</p><p>\uff3bhttps://www.lesswrong.com/users/jessica-liu-taylor Jessica Taylor\uff3d wrote two posts about daemons while at \uff3bhttps://www.lesswrong.com/tag/machine-intelligence-research-institute-miri MIRI\uff3d:</p><ul><li>\uff3bhttps://agentfoundations.org/item?id\ua78a1281 \"Are daemons a problem for ideal agents?\"\uff3d (2017-02-11)</li><li>\uff3bhttps://agentfoundations.org/item?id\ua78a1290 \"Maximally efficient agents will probably have an anti-daemon immune system\"\uff3d (2017-02-23)</li></ul><h2>&nbsp;</h2><h2>See also</h2><ul><li>\uff3bhttps://www.lesswrong.com/tag/inner-alignment Inner Alignment\uff3d</li><li>\uff3bhttps://lessestwrong.com/tag/complexity-of-value Complexity of value\uff3d</li><li>\uff3bhttps://lessestwrong.com/lw/l3/thou_art_godshatter/ Thou Art Godshatter\uff3d</li></ul><h2>&nbsp;</h2><h2>References</h2><ol><li>\uff3bhttps://arbital.com/p/daemons/ <u>\"Optimization daemons\"</u>\uff3d. Arbital.</li><li>Wei Dai. \uff3bhttp://sl4.org/archive/0312/7421.html <u>'\"friendly\" humans?'</u>\uff3d December 31, 2003.</li></ol><h2>&nbsp;</h2><h2>External links</h2><p>\uff3bhttps://www.youtube.com/watch?v\ua78abJLcIBixGj8 Video by Robert Miles\uff3d</p><p>Some posts that reference optimization daemons:</p><ul><li>\uff3bhttp://effective-altruism.com/ea/1k4/draft_cause_prioritization_for_downsidefocused/ \"Cause prioritization for downside-focused value systems\"\uff3d: \"Alternatively, perhaps goal preservation becomes more difficult the more capable AI systems become, in which case the future might be controlled by unstable goal functions taking turns over the steering wheel\"</li><li>\uff3bhttps://ai-alignment.com/techniques-for-optimizing-worst-case-performance-39eafec74b99 \"Techniques for optimizing worst-case performance\"\uff3d: \"The difficulty of optimizing worst-case performance is one of the most likely reasons that I think prosaic AI alignment might turn out to be impossible (if combined with an unlucky empirical situation).\" (the phrase \"unlucky empirical situation\" links to the optimization daemons page on \uff3bhttps://arbital.com/ Arbital\uff3d)</li></ul><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/mesa-optimization?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/mesa-optimization?edit\ua78atrue</div>"}, {"title": "What are plausible candidates for \"pivotal acts\"?", "authors": "", "url": "https://stampy.ai/wiki/What_are_plausible_candidates_for_%22pivotal_acts%22%3F", "content": "Pivotal acts are acts that substantially change the direction humanity will have taken in 1 billion years. The term is used to denote positive changes, as opposed to existential catastrophe.\n\nAn obvious pivotal act would be to create a \uff3bhttps://arbital.com/p/Sovereign/ sovereign AGI\uff3d aligned with humanity's best interests. An act that would greatly increase the chance of another pivotal act would also count as pivotal.\n\nPivotal acts often lay outside the \uff3bhttps://en.wikipedia.org/wiki/Overton_window Overton window\uff3d. One such example is stopping or strongly delaying the development of an unaligned (or any) AGI through drastic means such as nanobots which melt all advanced processors, or the disabling of all AI researchers. Eliezer mentions these in \uff3bhttps://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities AGI Ruin: A List of Lethalities\uff3d. Andrew Critch argues against such an unilateral pivotal act in \uff3bhttps://www.alignmentforum.org/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious \u201cPivotal Act\u201d Intentions: Negative Consequences and Fallacious Arguments\uff3d.\n\nFor more details, see \uff3bhttps://arbital.com/p/pivotal/ arbital\uff3d."}, {"title": "What are some AI alignment research agendas currently being pursued?", "authors": "", "url": "https://stampy.ai/wiki/What_are_some_AI_alignment_research_agendas_currently_being_pursued%3F", "content": "Research at the \uff3bhttp://alignmentresearchcenter.org/ Alignment Research Center\uff3d is led by \uff3bhttps://paulfchristiano.com/ Paul Christiano\uff3d, best known for introducing the \uff3bhttps://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616 \u201cIterated Distillation and Amplification\u201d\uff3d and \uff3bhttps://ai-alignment.com/humans-consulting-hch-f893f6051455 \u201cHumans Consulting HCH\u201d\uff3d approaches. He and his team are now ''\u201ctrying to figure out how to train ML systems to answer questions by straightforwardly \u2018translating\u2019 their beliefs into natural language rather than by reasoning about what a human wants to hear.\u201d'' \n\n\uff3bhttps://colah.github.io/about.html Chris Olah\uff3d (after work at \uff3bhttps://en.wikipedia.org/wiki/DeepMind DeepMind\uff3d and \uff3bhttps://en.wikipedia.org/wiki/OpenAI OpenAI\uff3d) recently launched \uff3bhttps://www.anthropic.com/ Anthropic\uff3d, an AI lab focussed on the safety of large models. While his previous work was concerned with \uff3bhttps://80000hours.org/podcast/episodes/chris-olah-interpretability-research/ \u201ctransparency\u201d and \u201cinterpretability\u201d of large neural networks\uff3d, especially vision models, Anthropic is focussing more on large language models, among other things working towards a ''\"general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless\".''\n\n\uff3bhttps://en.wikipedia.org/wiki/Stuart_J._Russell Stuart Russell\uff3d and his team at the \uff3bhttps://en.wikipedia.org/wiki/Center_for_Human-Compatible_Artificial_Intelligence Center for Human-Compatible Artificial Intelligence\uff3d (CHAI) have been working on \uff3bhttps://arxiv.org/abs/1806.06877 inverse reinforcement learning\uff3d (where the AI infers human values from observing human behavior) and \uff3bhttps://intelligence.org/files/CorrigibilityAISystems.pdf corrigibility\uff3d, as well as attempts to disaggregate neural networks into \u201cmeaningful\u201d subcomponents (see Filan, et al.\u2019s \uff3bhttps://arxiv.org/abs/2103.03386 \u201cClusterability in neural networks\u201d\uff3d and Hod et al.'s \uff3bhttps://openreview.net/forum?id\ua78atFQyjbOz34 \u201cDetecting modularity in deep neural networks\uff3d\u201d). \n\nAlongside the more abstract \uff3bhttps://intelligence.org/files/TechnicalAgenda.pdf \u201cagent foundations\u201d\uff3d work they have become known for, \uff3bhttps://intelligence.org/ MIRI\uff3d recently announced their \uff3bhttps://www.lesswrong.com/posts/zRn6cLtxyNodudzhw/visible-thoughts-project-and-bounty-announcement \u201cVisible Thoughts Project\u201d\uff3d to test the hypothesis that ''\u201cLanguage models can be made more understandable (and perhaps also more capable, though this is not the goal) by training them to produce visible thoughts.\u201d''\n\n\uff3bhttps://en.wikipedia.org/wiki/OpenAI OpenAI\uff3d have recently been doing work on \uff3bhttps://openai.com/blog/summarizing-books/ iteratively summarizing books\uff3d (summarizing, and then summarizing the summary, etc.) as a method for scaling human oversight.\n\nStuart Armstrong\u2019s recently launched \uff3bhttps://buildaligned.ai/ AlignedAI\uff3d are mainly working on \uff3bhttps://www.alignmentforum.org/s/u9uawicHx7Ng7vwxA concept extrapolation\uff3d from familiar to novel contexts, something he believes is \u201cnecessary and almost sufficient\u201d for AI alignment.\n\n\uff3bhttps://www.redwoodresearch.org/ Redwood Research\uff3d (Buck Shlegeris, et al.) are trying to \u201chandicap' GPT-3 to only produce non-violent completions of text prompts. ''\u201cThe idea is that there are many reasons we might ultimately want to apply some oversight function to an AI model, like \u2018don't be deceitful\u2019, and if we want to get AI teams to apply this we need to be able to incorporate these oversight predicates into the original model in an efficient manner.\u201d''\n\nOught is an independent AI safety research organization led by Andreas Stuhlm\u00fcller and Jungwon Byun. They are researching methods for breaking up complex, hard-to-verify tasks into simpler, easier-to-verify tasks, with the aim of allowing us to maintain effective oversight over AIs."}, {"title": "What are some good books about AGI safety?", "authors": "", "url": "https://stampy.ai/wiki/What_are_some_good_books_about_AGI_safety%3F", "content": "''\uff3bhttps://brianchristian.org/the-alignment-problem/ The Alignment Problem\uff3d'' (2020) by Brian Christian is the most recent in-depth guide to the field.\n\nThe book which first made the case to the public is Nick Bostrom\u2019s ''\uff3bhttps://publicism.info/philosophy/superintelligence/ Superintelligence\uff3d'' (2014). It gives an excellent overview of the state of the field (as it was then) and makes a strong case for the subject being important, as well as exploring many fascinating adjacent topics. However, it does not cover newer developments, such as \uff3b\uff3bWhat are mesa-optimizers?\u250amesa-optimizers\uff3d\uff3d or \uff3b\uff3bWhat are language models?\u250alanguage models\uff3d\uff3d.\n\nThere's also ''\uff3bhttps://en.wikipedia.org/wiki/Human_Compatible Human Compatible\uff3d'' (2019) by Stuart Russell, which gives a more up-to-date review of developments, with an emphasis on the approaches that the Center for Human-Compatible AI are working on, such as cooperative inverse reinforcement learning. There's a good \uff3bhttps://slatestarcodex.com/2020/01/30/book-review-human-compatible/ review/summary on SlateStarCodex\uff3d.\n\nAlthough not limited to AI safety, ''\uff3bhttps://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795 The AI Does Not Hate You\uff3d'' (2020) is an entertaining and accessible outline of both the core issues and an exploration of some of the community and culture of the people working on it.\n\nVarious other books explore the issues in an informed way, such as \uff3bhttp://www.tobyord.com/ Toby Ord\uff3d\u2019s ''\uff3bhttps://en.wikipedia.org/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity The Precipice\uff3d'' (2020), \uff3bhttps://en.wikipedia.org/wiki/Max_Tegmark Max Tegmark\uff3d\u2019s ''\uff3bhttps://en.wikipedia.org/wiki/Life_3.0 Life 3.0\uff3d'' (2017), \uff3bhttps://www.ynharari.com/ Yuval Noah Harari\uff3d\u2019s ''\uff3bhttps://www.ynharari.com/book/homo-deus/ Homo Deus\uff3d'' (2016), \uff3bhttps://www.fhi.ox.ac.uk/team/stuart-armstrong/ Stuart Armstrong\uff3d\u2019s ''\uff3bhttps://smarterthan.us/toc/ Smarter Than Us\uff3d'' (2014), and \uff3bhttp://lukeprog.com/ Luke Muehlhauser\uff3d\u2019s ''\uff3bhttps://intelligenceexplosion.com/ Facing the Intelligence Explosion\uff3d'' (2013)."}, {"title": "What are some good podcasts about AI alignment?", "authors": "", "url": "https://stampy.ai/wiki/What_are_some_good_podcasts_about_AI_alignment%3F", "content": "All the content below is in English:\n* The \uff3bhttps://80000hours.org/topic/priority-paths/technical-ai-safety/?content-type\ua78apodcast AI technical safety section\uff3d of the 80,000 Hours Podcast;\n* The \uff3bhttps://axrp.net/ AI X-risk Research Podcast\uff3d, hosted by Daniel Filan;\n* The \uff3bhttps://futureoflife.org/ai-alignment-podcast/ AI Alignment Podcast\uff3d hosted by Lucas Perry from the Future of Life Institute (ran ~monthly from April 2018 to March 2021);\n* The \uff3bhttps://alignment-newsletter.libsyn.com/ Alignment Newsletter Podcast\uff3d by Rob Miles (an audio version of the weekly newsletter)."}, {"title": "What are some good resources on AI alignment?", "authors": "", "url": "https://stampy.ai/wiki/What_are_some_good_resources_on_AI_alignment%3F", "content": "These are good sources for understanding AI alignment and linking to when editing Stampy!\n\n* \uff3bhttps://www.youtube.com/c/RobertMilesAI/videos Rob's YouTube videos\uff3d (\uff3bhttps://www.youtube.com/watch?v\ua78atlS5Y2vm02c&list\ua78aPLqL14ZxTTA4fRMts7Af2G8t4Rp17e8MdS&index\ua78a4 Computerphile appearances\uff3d)\n* \uff3bhttps://ai-safety-papers.quantifieduncertainty.org/ AI Safety Papers database\uff3d - Search and interface for the \uff3bhttps://www.lesswrong.com/posts/4DegbDJJiMX2b3EKm/tai-safety-bibliographic-database TAI Safety Bibliography\uff3d\n* \uff3bhttps://www.eacambridge.org/agi-safety-fundamentals AGI Safety Fundamentals Course\uff3d\n* \uff3bhttps://www.alignmentforum.org/tags/ Alignment Forum\uff3d tags\n* \uff3bhttps://rohinshah.com/alignment-newsletter/ The Alignment Newsletter\uff3d (and \uff3bhttps://docs.google.com/spreadsheets/d/1PwWbWZ6FPqAgZWOoOcXM8N_tUCuxpEyMbN1NYYC02aM/edit#gid\ua78a0 database sheet\uff3d)\n* Chapters of \uff3bhttps://publicism.info/philosophy/superintelligence/ Bostrom's Superintelligence online\uff3d - \uff3bhttps://www.nickbostrom.com/views/superintelligence.pdf Initial paper which Superintelligence grew from\uff3d\n* \uff3bhttps://arbital.greaterwrong.com/explore/ai_alignment/ AI Alignment pages on Arbital\uff3d\n* Much more on \uff3bhttps://www.aisafetysupport.org/resources/lots-of-links AI Safety Support\uff3d (feel free to integrate useful things from there to here)\n* \uff3bhttps://vkrakovna.wordpress.com/ai-safety-resources/ Vika's resources list\uff3d\n* \uff3bhttps://docs.google.com/spreadsheets/d/1QSEWjXZuqmG6ORkig84V4sFCldIntyuQj7yq3gkDo0U/edit#gid\ua78a0 AI safety technical courses, reading lists, and curriculums\uff3d\n* \uff3bhttps://aisafety.wordpress.com/ AI Safety Intro blog\uff3d\n* \uff3bhttps://stampy.ai/wiki/Canonical_answers Stampy's canonical answers list\uff3d - This includes updated versions of various \uff3b\uff3bImported FAQs\u250aFAQs imported with permission\uff3d\uff3d:\n** \uff3bhttps://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq Scott Alexander's Superintelligence FAQ\uff3d\n** \uff3bhttps://futureoflife.org/ai-faqs/ FLI's FAQ\uff3d\n** \uff3bhttps://intelligence.org/faq/ MIRI's FAQ\uff3d\n** \uff3bhttps://intelligence.org/ie-faq/ MIRI's Intelligence Explosion FAQ\uff3d\n** \uff3bhttps://rohinshah.com/faq-career-advice-for-ai-alignment-researchers/ Advice for AI Alignment Researchers\uff3d\n** \uff3bhttps://www.reddit.com/r/ControlProblem/wiki/faq r/ControlProblem's FAQ\uff3d\n** \uff3bhttps://markxu.com/ai-safety-faqs Mark Xu's FAQ\uff3d\n** \uff3bhttps://aisafety.wordpress.com/ AI safety blog\uff3d - Not yet imported."}, {"title": "What are some objections to the importance of AI alignment?", "authors": "", "url": "https://stampy.ai/wiki/What_are_some_objections_to_the_importance_of_AI_alignment%3F", "content": "\uff3bhttps://aisafety.com/author/soeren-elverlin/ S\u00f8ren Elverlin\uff3d has compiled a list of counter-arguments and suggests dividing them into two kinds: weak and strong. \n\nWeak counter-arguments point to problems with the \"standard\" arguments (as given in, e.g., Bostrom\u2019s \uff3bhttps://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies ''Superintelligence''\uff3d), especially shaky models and assumptions that are too strong. These arguments are often of a substantial quality and are often presented by people who themselves worry about AI safety. Elverin calls these objections \u201cweak\u201d because they do not attempt to imply that the probability of a bad outcome is close to zero: ''\u201cFor example, even if you accept \uff3bhttps://www.alignmentforum.org/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds Paul Christiano's arguments against \u201cfast takeoff\u201d\uff3d, they only drive the probability of this down to about 20%. Weak counter-arguments are interesting, but the decision to personally focus on AI safety doesn't strongly depend on the probability \u2013 anything above 5% is clearly a big enough deal that it doesn't make sense to work on other things.\u201d''\n\nStrong arguments argue that the probability of existential catastrophe due to misaligned AI is tiny, usually by some combination of claiming that AGI is impossible or very far away. For example, \uff3bhttps://en.wikipedia.org/wiki/Michael_L._Littman Michael Littman\uff3d has \uff3bhttps://www.youtube.com/watch?v\ua78ac9AbECvRt20&t\ua78a1559s suggested\uff3d that as (he believes) we\u2019re so far from AGI, there will be a long period of human history wherein we\u2019ll have ample time to grow up alongside powerful AIs and figure out how to align them.\n\nElverlin opines that ''\u201cThere are few arguments that are both high-quality and strong enough to qualify as an \u2018objection to the importance of alignment\u2019.\u201d'' He suggests \uff3bhttps://aiimpacts.org/conversation-with-rohin-shah/ Rohin Shah's arguments for \u201calignment by default\u201d\uff3d as one of the better candidates.\n\n\uff3bhttps://intelligence.org/ MIRI\uff3d's April fools \uff3bhttps://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy \"Death With Dignity\" strategy\uff3d might be seen as an argument against the importance of working on alignment, but only in the sense that we might have almost no hope of solving it. In the same category are the \u201csomething else will kill us first, so there\u2019s no point worrying about AI alignment\u201d arguments."}, {"title": "What are some of the most impressive recent advances in AI capabilities?", "authors": "", "url": "https://stampy.ai/wiki/What_are_some_of_the_most_impressive_recent_advances_in_AI_capabilities%3F", "content": "GPT-3 showed that transformers are capable of a vast array of natural language tasks, \uff3bhttps://copilot.github.com/ codex/copilot\uff3d extended this into programming. One demonstrations of GPT-3 is \uff3bhttps://www.lesswrong.com/posts/oBPPFrMJ2aBK6a6sD/simulated-elon-musk-lives-in-a-simulation Simulated Elon Musk lives in a simulation\uff3d. Important to note that there are several much better language models, but they are not publicly available.\n\n\uff3bhttps://openai.com/blog/dall-e/ DALL-E\uff3d and \uff3bhttps://openai.com/dall-e-2/ DALL-E 2\uff3d are among the most visually spectacular.\n\n\uff3bhttps://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules MuZero\uff3d, which learned Go, Chess, and many Atari games without any directly coded info about those environments. The graphic there explains it, this seems crucial for being able to do RL in novel environments. We have systems which we can drop into a wide variety of games and they just learn how to play. The same algorithm was used in \uff3bhttps://youtu.be/j0z4FweCy4M?t\ua78a4918 Tesla's self-driving cars to do complex route finding\uff3d. These things are general.\n\n\uff3bhttps://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play Generally capable agents emerge from open-ended play\uff3d - Diverse procedurally generated environments provide vast amounts of training data for AIs to learn generally applicable skills. \uff3bhttps://www.deepmind.com/publications/creating-interactive-agents-with-imitation-learning Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning\uff3d shows how these kind of systems can be trained to follow instructions in natural language.\n\n\uff3bhttps://www.deepmind.com/publications/a-generalist-agent GATO\uff3d shows you can distill 600+ individually trained tasks into one network, so we're not limited by the tasks being fragmented."}, {"title": "What are some specific open tasks on Stampy?", "authors": "", "url": "https://stampy.ai/wiki/What_are_some_specific_open_tasks_on_Stampy%3F", "content": "Other than the usual fare of writing and processing and organizing questions and answers, here are some specific open tasks:\n\n* Porting over some of \uff3bhttps://www.lesswrong.com/posts/4basF9w9jaPZpoC8R/intro-to-brain-like-agi-safety-1-what-s-the-problem-and-why#1_4_What_exactly_is__AGI__ Steve Byrnes's FAQ on alignment\uff3d\n* Porting over content from \uff3bhttps://www.lesswrong.com/posts/gdyfJE3noRFSs373q/resources-i-send-to-ai-researchers-about-ai-safety Vael Gates's post\uff3d\n* Porting over QA pairs from https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread\n* Porting over some of https://aisafety.wordpress.com/\n* Making sure we cover all of https://forum.effectivealtruism.org/posts/8JazqnCNrkJtK2Bx4/why-eas-are-skeptical-about-ai-safety#Recursive_self_improvement_seems_implausible and the responses"}, {"title": "What are the differences between \u201cAI safety\u201d, \u201cAGI safety\u201d, \u201cAI alignment\u201d and \u201cAI existential safety\u201d?", "authors": "", "url": "https://stampy.ai/wiki/What_are_the_differences_between_%E2%80%9CAI_safety%E2%80%9D,_%E2%80%9CAGI_safety%E2%80%9D,_%E2%80%9CAI_alignment%E2%80%9D_and_%E2%80%9CAI_existential_safety%E2%80%9D%3F", "content": "AI alignment is the research field focused on trying to give us the tools to align AIs to specific goals, such as human values. This is crucial when they are highly competent, as a misaligned superintelligence could be the end of human civilization.\n\nAGI safety is the field trying to make sure that when we build Artificial General Intelligences they are safe and do not harm humanity. It overlaps with AI alignment strongly, in that misalignment of AI would be the main cause of unsafe behavior in AGIs, but also includes misuse and other governance issues.\n\nAI existential safety is a slightly broader term than AGI safety, including AI risks which pose an existential threat without necessarily being as general as humans.\n\nAI safety was originally used by the existential risk reduction movement for the work done to reduce the risks of misaligned superintelligence, but has also been adopted by researchers and others studying nearer term and less catastrophic risks from AI in recent years."}, {"title": "What are the different possible AI takeoff speeds?", "authors": "", "url": "https://stampy.ai/wiki/What_are_the_different_possible_AI_takeoff_speeds%3F", "content": "A slow takeoff is where AI capabilities improve gradually, giving us plenty of time to adapt. In a moderate takeoff we might see accelerating progress, but we still won\u2019t be caught off guard by a dramatic change. Whereas, in a fast or hard takeoff AI would go from being not very generally competent to sufficiently superhuman to control the future too fast for humans to course correct if something goes wrong.\n\nThe article \uff3bhttps://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff Distinguishing definitions of takeoff\uff3d goes into more detail on this."}, {"title": "What are the ethical challenges related to whole brain emulation?", "authors": "", "url": "https://stampy.ai/wiki/What_are_the_ethical_challenges_related_to_whole_brain_emulation%3F", "content": "Unless there was a way to \uff3bhttps://www.lesswrong.com/posts/vit9oWGj6WgXpRhce/secure-homes-for-digital-people cryptographically ensure otherwise\uff3d, whoever runs the emulation has basically perfect control over their environment and can reset them to any state they were previously in. This opens up the possibility of powerful interrogation and torture of digital people.\n\nImperfect uploading might lead to damage that causes the EM to suffer while still remaining useful enough to be run for example as a test subject for research. We would also have greater ability to modify digital brains. Edits done for research or economic purposes might cause suffering. See \uff3bhttps://qntm.org/mmacevedo this\uff3d fictional piece for an exploration of how a world with a lot of EM suffering might look like.\n\nThese problems are exacerbated by the likely outcome that digital people can be run much faster than biological humans, so it would be plausibly possible to have an EM run for hundreds of subjective years in minutes or hours without having checks on the wellbeing of the EM in question."}, {"title": "What are the potential benefits of AI as it grows increasingly sophisticated?", "authors": "", "url": "https://stampy.ai/wiki/What_are_the_potential_benefits_of_AI_as_it_grows_increasingly_sophisticated%3F", "content": "It\u2019s difficult to tell at this stage, but AI will enable many developments that could be terrifically beneficial if managed with enough foresight and care. For example, menial tasks could be automated, which could give rise to a society of abundance, leisure, and flourishing, free of poverty and tedium. As another example, AI could also improve our ability to understand and manipulate complex biological systems, unlocking a path to drastically improved longevity and health, and to conquering disease."}, {"title": "What are the style guidelines for writing for Stampy?", "authors": "", "url": "https://stampy.ai/wiki/What_are_the_style_guidelines_for_writing_for_Stampy%3F", "content": "Avoid directly responding to the question in the answer, repeat the relevant part of the question instead. For example, if the question is \"Can we do X\", answer \"We might be able to do X, if we can do Y\", not \"Yes, if we can manage Y\". This way, the answer will also work for the questions \"Why can't we do X\" and \"What would happen if we tried to do X\".\n\nLinking to external sites is strongly encouraged, one of the most valuable things Stampy can do is help people find other parts of the alignment information ecosystem.\n\nConsider enclosing newly introduced terms, likely to be unfamiliar to many readers, in speech marks. If unsure, Google the term (in speech marks!) and see if it shows up anywhere other than LessWrong, the Alignment Forum, etc. Be judicious, as it's easy to use too many, but used carefully they can psychologically cushion newbies from a lot of unfamiliar terminology - in this context they're saying something like \"we get that we're hitting you with a lot of new vocab, and you might not know what this term means yet\".\n\nWhen selecting related questions, there shouldn't be more than four unless there's a really good reason for that (some questions are asking for it, like the \"Why can't we just...\" question). It's also recommended to include at least one more \"enticing\" question to draw users in (relating to the more sensational, sci-fi, philosophical/ethical side of things) alongside more bland/neutral questions."}, {"title": "What can I do to contribute to AI safety?", "authors": "", "url": "https://stampy.ai/wiki/What_can_I_do_to_contribute_to_AI_safety%3F", "content": "It\u2019s pretty dependent on what skills you have and what resources you have access to. The largest option is to pursue a \uff3bhttps://80000hours.org/career-reviews/artificial-intelligence-risk-research/ career in AI Safety research\uff3d. Another large option is to pursue a career in \uff3bhttps://80000hours.org/articles/ai-policy-guide/ AI policy\uff3d, which you might think is even more important than doing technical research.\n\nSmaller options include donating money to relevant organizations, talking about AI Safety as a plausible career path to other people or considering the problem in your spare time.\n\nIt\u2019s possible that your particular set of skills/resources are not suited to this problem. Unluckily, there are \uff3bhttps://concepts.effectivealtruism.org/concepts/existential-risks/ many more problems\uff3d that are of similar levels of importance."}, {"title": "What does Elon Musk think about AI safety?", "authors": "", "url": "https://stampy.ai/wiki/What_does_Elon_Musk_think_about_AI_safety%3F", "content": "Elon Musk has expressed his concerns about AI safety many times and founded OpenAI in an attempt to make safe AI more widely distributed (as opposed to allowing a \uff3bhttps://www.nickbostrom.com/fut/singleton.html singleton\uff3d, which he fears would be misused or dangerously unaligned). In a \uff3bhttps://www.youtube.com/watch?v\ua78asmK9dgdTl40 YouTube video\uff3d from November 2019 Musk stated that there's a lack of investment in AI safety and that there should be a government agency to reduce risk to the public from AI."}, {"title": "What does MIRI think about technical alignment?", "authors": "", "url": "https://stampy.ai/wiki/What_does_MIRI_think_about_technical_alignment%3F", "content": "MIRI thinks technical alignment is really hard, and that we are very far from a solution. However, they think that policy solutions have even less hope. Generally, I think of their approach as supporting a bunch of independent researchers following their own directions, hoping that one of them will find some promise. They mostly buy into the \uff3bhttps://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/ security mindset\uff3d: we need to know exactly (probably \uff3bhttps://www.lesswrong.com/posts/Gg9a4y8reWKtLe3Tn/the-rocket-alignment-problem mathematically formally)\uff3d what we are doing, or the massive optimization pressure will default in ruin.\n\n'''\uff3b\uff3b How does MIRI communicate their view on alignment?\uff3d\uff3d'''\n\nRecently they've been trying to communicate their worldview, in particular, how \uff3bhttps://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy incredibly doomy they are\uff3d, perhaps in order to move other research efforts towards what they see as the hard problems.\n\n*\uff3bhttps://www.lesswrong.com/s/n945eovrA3oDueqtq 2021 MIRI Conversations\uff3d \n*\uff3bhttps://www.lesswrong.com/s/v55BhXbpJuaExkpcD 2022 MIRI Alignment Discussion\uff3d"}, {"title": "What does Ought aim to do?", "authors": "", "url": "https://stampy.ai/wiki/What_does_Ought_aim_to_do%3F", "content": "\uff3bhttps://ought.org/ Ought\uff3d aims to automate and scale open-ended reasoning through \uff3bhttps://ought.org/elicit Elicit\uff3d, an AI research assistant. Ought focuses on advancing \uff3bhttps://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes process-based systems\uff3d rather than outcome-based ones, which they believe to be both beneficial for improving reasoning in the short term and alignment in the long term. \uff3bhttps://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes Here\uff3d they argue that in the long run improving reasoning and alignment converge.\n\nSo Ought\u2019s impact on AI alignment has 2 components: (a) improved reasoning of AI governance & alignment researchers, \uff3bhttps://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes#Differential_capabilities__Supervising_process_helps_with_long_horizon_tasks particularly on long-horizon tasks\uff3d and (b) \uff3bhttps://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes#Alignment__Supervising_process_is_safety_by_construction pushing supervision of process rather than outcomes\uff3d, which reduces the optimization pressure on imperfect proxy objectives leading to \u201csafety by construction\u201d. Ought argues that the \uff3bhttps://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes#Two_attractors__The_race_between_process__and_outcome_based_systems race between process and outcome-based systems\uff3d is particularly important because both states may be an attractor."}, {"title": "What is Artificial General Intelligence and what will it look like?", "authors": "", "url": "https://stampy.ai/wiki/What_is_Artificial_General_Intelligence_and_what_will_it_look_like%3F", "content": "AGI is an algorithm with \uff3bhttps://www.lesswrong.com/posts/yLeEPFnnB9wE7KLx2/efficient-cross-domain-optimization general intelligence\uff3d, running not on evolution\u2019s biology like all current general intelligences but on a substrate such as silicon engineered by an intelligence (initially computers designed by humans, later on likely dramatically more advanced hardware designed by earlier AGIs).\n\nAI has so far always been designed and built by humans (i.e. a search process running on biological brains), but once our creations gain the ability to do AI research they will likely \uff3bhttps://www.lesswrong.com/posts/JBadX7rwdcRFzGuju/recursive-self-improvement recursively self-improve\uff3d by designing new and better versions of themselves initiating an \uff3bhttps://www.lesswrong.com/posts/8vpf46nLMDYPC6wA4/optimization-and-the-intelligence-explosion intelligence explosion\uff3d (i.e. use it\u2019s intelligence to improve its own intelligence, creating a feedback loop), and resulting in a superintelligence. There are already \uff3bhttps://arxiv.org/abs/2101.07367 early signs\uff3d of AIs being trained to optimize other AIs.\n\nSome authors (notably \uff3bhttps://intelligence.org/ai-foom-debate/ Robin Hanson\uff3d) have argued that the intelligence explosion hypothesis is likely false, and in favor of a large number of roughly human level emulated minds operating instead, forming an uplifted economy which doubles every few hours. Eric Drexler\u2019s \uff3bhttps://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/ Comprehensive AI Services\uff3d model of what may happen is another alternate view, where many narrow superintelligent systems exist in parallel rather than there being a general-purpose superintelligent agent.\n\nGoing by the model advocated by \uff3bhttps://en.wikipedia.org/wiki/Nick_Bostrom Nick Bostrom\uff3d, \uff3bhttps://en.wikipedia.org/wiki/Eliezer_Yudkowsky Eliezer Yudkowsky\uff3d and many others, a superintelligence will likely gain various \uff3bhttps://publicism.info/philosophy/superintelligence/7.html cognitive superpowers\uff3d (table 8 gives a good overview), allowing it to direct the future much more effectively than humanity. Taking control of our resources by manipulation and hacking is a likely early step, followed by developing and deploying advanced technologies like \uff3bhttps://en.wikipedia.org/wiki/Molecular_nanotechnology molecular nanotechnology\uff3d to dominate the physical world and achieve its goals."}, {"title": "What harm could a single superintelligence do when it took so many humans to build civilization?", "authors": "", "url": "https://stampy.ai/wiki/What_harm_could_a_single_superintelligence_do_when_it_took_so_many_humans_to_build_civilization%3F", "content": "Superintelligence has an advantage that an early human didn\u2019t \u2013 the entire context of human civilization and technology, there for it to manipulate socially or technologically."}, {"title": "What is \"biological cognitive enhancement\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_%22biological_cognitive_enhancement%22%3F", "content": "There may be genes or molecules that can be modified to improve general intelligence. Researchers \uff3bhttps://pubmed.ncbi.nlm.nih.gov/10485705/ have already done this in mice\uff3d: they over-expressed the NR2B gene, which improved those mice\u2019s memory beyond that of any other mice of any mouse species. Biological cognitive enhancement in humans may cause an intelligence explosion to occur more quickly than it otherwise would.\n\nSee also:\n*Bostrom & Sandberg, \uff3bhttp://www.nickbostrom.com/cognitive.pdf Cognitive Enhancement: Methods, Ethics, Regulatory Challenges\uff3d"}, {"title": "What is \"evidential decision theory\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_%22evidential_decision_theory%22%3F", "content": "<p><strong>Evidential Decision Theory</strong> \u2013 EDT \u2013 is a branch of \uff3bhttps://www.lesswrong.com/tag/decision-theory decision theory\uff3d which advises an agent to take actions which, conditional on it happening, maximizes the chances of the desired outcome. As any branch of decision theory, it prescribes taking the action that maximizes \uff3bhttps://www.lesswrong.com/tag/utility utility\uff3d, that which utility equals or exceeds the utility of every other option. The utility of each action is measured by the \uff3bhttps://www.lesswrong.com/tag/expected-utility expected utility\uff3d, the averaged by probabilities sum of the utility of each of its possible results. How the actions can influence the probabilities differ between the branches. \uff3bhttps://www.lesswrong.com/tag/causal-decision-theory Causal Decision Theory\uff3d \u2013 CDT \u2013 says only through causal process one can influence the chances of the desired outcome \uff3b#fn1 <sup>1</sup>\uff3d. EDT, on the other hand, requires no causal connection, the action only have to be a \uff3bhttps://www.lesswrong.com/tag/bayesianism Bayesian\uff3d evidence for the desired outcome. Some critics say it recommends auspiciousness over causal efficacy\uff3b#fn2 <sup>2</sup>\uff3d.</p><p>One usual example where EDT and CDT commonly diverge is the \uff3bhttps://www.lesswrong.com/tag/smoking-lesion Smoking lesion\uff3d: \u201cSmoking is strongly correlated with lung cancer, but in the world of the Smoker's Lesion this correlation is understood to be the result of a common cause: a genetic lesion that tends to cause both smoking and cancer. Once we fix the presence or absence of the lesion, there is no additional correlation between smoking and cancer. Suppose you prefer smoking without cancer to not smoking without cancer, and prefer smoking with cancer to not smoking with cancer. Should you smoke?\u201d CDT would recommend smoking since there is no causal connection between smoking and cancer. They are both caused by a gene, but have no causal direct connection with each other. EDT on the other hand wound recommend against smoking, since smoking is an evidence for having the mentioned gene and thus should be avoided.</p><p>CDT uses probabilities of conditionals and contrafactual dependence to calculate the expected utility of an action \u2013 which track causal relations -, whereas EDT simply uses conditional probabilities. The probability of a conditional is the probability of the whole conditional being true, where the conditional probability is the probability of the consequent given the antecedent. A conditional probability of B given A - P(B\u250aA) -, simply implies the Bayesian probability of the event B happening given we known A happened, it\u2019s used in EDT. The probability of conditionals \u2013 P(A > B) - refers to the probability that the conditional 'A implies B' is true, it is the probability of the contrafactual \u2018If A, then B\u2019 be the case. Since contrafactual analysis is the key tool used to speak about causality, probability of conditionals are said to mirror causal relations. In most usual cases these two probabilities are the same. However, David Lewis proved \uff3b#fn3 <sup>3</sup>\uff3d its\u2019 impossible to probabilities of conditionals to always track conditional probabilities. Hence evidential relations aren\u2019t the same as causal relations and CDT and EDT will diverge depending on the problem. In some cases EDT gives a better answers then CDT, such as the \uff3bhttps://www.lesswrong.com/tag/newcomb-s-problem Newcomb's problem\uff3d, whereas in the \uff3bhttps://www.lesswrong.com/tag/smoking-lesion Smoking lesion\uff3d problem where CDT seems to give a more reasonable prescription.</p><h2>References</h2><ol><li>\uff3bhttp://plato.stanford.edu/entries/decision-causal/ http://plato.stanford.edu/entries/decision-causal/\uff3d\uff3b#fnref1 \u21a9\uff3d</li><li>Joyce, J.M. (1999), The foundations of causal decision theory, p. 146\uff3b#fnref2 \u21a9\uff3d</li><li>Lewis, D. (1976), \"Probabilities of conditionals and conditional probabilities\", The Philosophical Review (Duke University Press) 85 (3): 297\u2013315\uff3b#fnref3 \u21a9\uff3d</li></ol><h2>Blog posts</h2><ul><li>\uff3bhttps://agentfoundations.org/item?id\ua78a1525 <i>Smoking Lesion Steelman</i>\uff3d by Abram Demski</li><li>\uff3bhttp://lesswrong.com/lw/gu1/decision_theory_faq/ <i>Decision Theory FAQ</i>\uff3d by Luke Muehlhauser</li><li>\uff3bhttps://casparoesterheld.files.wordpress.com/2016/12/almond_edt_1.pdf <i>On Causation and Correlation Part 1</i>\uff3d</li><li>\uff3bhttp://lesswrong.com/lw/men/twoboxing_smoking_and_chewing_gum_in_medical/ <i>Two-boxing, smoking and chewing gum in Medical Newcomb problems</i>\uff3d by Caspar Oesterheld</li><li>\uff3bhttp://lesswrong.com/r/discussion/lw/oih/did_edt_get_it_right_all_along_introducing_yet/ <i>Did EDT get it right all along? Introducing yet another medical Newcomb problem</i>\uff3d by Johannes Treutlein</li><li>\uff3bhttps://casparoesterheld.com/2017/02/06/betting-on-the-past-by-arif-ahmed/ <i>\"Betting on the Past\" by Arif Ahmed</i>\uff3d by Johannes Treutlein</li><li>\uff3bhttps://agentfoundations.org/item?id\ua78a92 <i>Why conditioning on \"the agent takes action a\" isn't enough</i>\uff3d by Nate Soares</li></ul><h2>See also</h2><ul><li>\uff3bhttps://www.lesswrong.com/tag/decision-theory Decision theory\uff3d</li></ul><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/evidential-decision-theory?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/evidential-decision-theory?edit\ua78atrue</div>"}, {"title": "What is \"functional decision theory\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_%22functional_decision_theory%22%3F", "content": "<p><strong>Functional Decision Theory</strong> is a \uff3bhttps://www.lesswrong.com/tag/decision-theory decision theory\uff3d described by Eliezer Yudkowsky and Nate Soares which says that agents should treat one\u2019s decision as the output of a \ufb01xed mathematical function that answers the question, \u201cWhich output of this very function would yield the best outcome?\u201d. It is a replacement of \uff3bhttps://www.lesswrong.com/tag/timeless-decision-theory Timeless Decision Theory\uff3d, and it outperforms other decision theories such as \uff3bhttps://www.lesswrong.com/tag/causal-decision-theory Causal Decision Theory\uff3d (CDT) and \uff3bhttps://www.lesswrong.com/tag/evidential-decision-theory Evidential Decision Theory\uff3d (EDT). For example, it does better than CDT on \uff3bhttps://www.lesswrong.com/tag/newcomb-s-problem Newcomb's Problem\uff3d, better than EDT on the \uff3bhttps://www.lesswrong.com/tag/smoking-lesion smoking lesion problem\uff3d, and better than both in \uff3bhttps://www.lesswrong.com/tag/parfits-hitchhiker Par\ufb01t\u2019s hitchhiker problem\uff3d.</p><p>In Newcomb's Problem, an FDT agent reasons that Omega must have used some kind of model of her decision procedure in order to make an accurate prediction of her behavior. Omega's model and the agent are therefore both calculating the same function (the agent's decision procedure): they are <i>subjunctively dependent </i>on that function. Given perfect prediction by Omega, there are therefore only two outcomes in Newcomb's Problem: either the agent one-boxes and Omega predicted it (because its model also one-boxed), or the agent two-boxes and Omega predicted <i>that</i>. Because one-boxing then results in a million and two-boxing only in a thousand dollars, the FDT agent one-boxes.</p><p><strong>External links:</strong></p><ul><li>\uff3bhttps://intelligence.org/2017/10/22/fdt Functional decision theory: A new theory of instrumental rationality\uff3d</li><li>\uff3bhttps://intelligence.org/2017/03/18/new-paper-cheating-death-in-damascus/ Cheating Death in Damascus\uff3d</li><li>\uff3bhttps://intelligence.org/2017/04/07/decisions-are-for-making-bad-outcomes-inconsistent/ Decisions are for making bad outcomes inconsistent\uff3d</li></ul><p><strong>See Also:</strong></p><ul><li>\uff3bhttps://www.lesswrong.com/tag/timeless-decision-theory Timeless Decision Theory\uff3d</li><li>\uff3bhttps://www.lesswrong.com/tag/updateless-decision-theory Updateless Decision Theory\uff3d</li></ul><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/functional-decision-theory?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/functional-decision-theory?edit\ua78atrue</div>"}, {"title": "What is \"greater-than-human intelligence\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_%22greater-than-human_intelligence%22%3F", "content": "Machines are already smarter than humans are at many specific tasks: performing calculations, playing chess, searching large databanks, detecting underwater mines, \uff3bhttp://www.amazon.com/dp/0521122937/ and more\uff3d. But one thing that makes humans special is their general intelligence. Humans can intelligently adapt to radically new problems in the urban jungle or outer space for which evolution could not have prepared them. Humans can solve problems for which their brain hardware and software was never trained. Humans can even examine the processes that produce their own intelligence (\uff3bhttp://en.wikipedia.org/wiki/Cognitive_neuroscience cognitive neuroscience\uff3d), and design new kinds of intelligence never seen before (\uff3bhttp://en.wikipedia.org/wiki/Artificial_intelligence artificial intelligence\uff3d).\n\nTo possess greater-than-human intelligence, a machine must be able to achieve goals more effectively than humans can, in a wider range of environments than humans can. This kind of intelligence involves the capacity not just to do science and play chess, but also to manipulate the social environment.\n\nComputer scientist Marcus Hutter \uff3bhttp://www.amazon.com/dp/3642060528/ has described\uff3d a formal model called AIXI that he says possesses the greatest general intelligence possible. But to implement it would require more computing power than all the matter in the universe can provide. Several projects try to approximate AIXI while still being computable, for example \uff3bhttp://arxiv.org/PS_cache/arxiv/pdf/0909/0909.0801v1.pdf MC-AIXI\uff3d.\n\nStill, there remains much work to be done before greater-than-human intelligence can be achieved in machines. Greater-than-human intelligence need not be achieved by directly programming a machine to be intelligent. It could also be achieved by whole brain emulation, by biological cognitive enhancement, or by brain-computer interfaces (see below).\n\nSee also:\n* Goertzel & Pennachin (eds.), \uff3bhttp://www.amazon.com/dp/3642062679/ Artificial General Intelligence\uff3d\n* Sandberg & Bostrom, \uff3bhttp://www.philosophy.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf Whole Brain Emulation: A Roadmap\uff3d\n* Bostrom & Sandberg, \uff3bhttp://www.nickbostrom.com/cognitive.pdf Cognitive Enhancement: Methods, Ethics, Regulatory Challenges\uff3d\n* Wikipedia, \uff3bhttp://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface Brain-computer interface\uff3d"}, {"title": "What is \"hedonium\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_%22hedonium%22%3F", "content": "<p><strong>Orgasmium</strong> (also known as <strong>hedonium</strong>) is a homogeneous substance with limited consciousness, which is in a constant state of supreme bliss. An AI programmed to \"maximize happiness\" might simply tile the universe with orgasmium. Some who believe this consider it a good thing; others do not. Those who do not, use its undesirability to argue that not all terminal values reduce to \"happiness\" or some simple analogue. Hedonium is the \uff3bhttps://www.lesswrong.com/tag/hedonism hedonistic\uff3d \uff3bhttps://www.lesswrong.com/tag/utilitarianism utilitarian\uff3d's version of \uff3bhttps://www.lesswrong.com/tag/utilitronium utilitronium\uff3d.</p><h2>Blog posts</h2><ul><li>\uff3bhttp://lesswrong.com/lw/wv/prolegomena_to_a_theory_of_fun/ Prolegomena to a Theory of Fun\uff3d</li><li>\uff3bhttp://lesswrong.com/lw/xr/in_praise_of_boredom/ In Praise of Boredom\uff3d</li><li>\uff3bhttps://reflectivedisequilibrium.blogspot.com/2012/03/are-pain-and-pleasure-equally-energy.html Are pain and pleasure equally energy-efficient?\uff3d</li></ul><h2>See also</h2><ul><li>\uff3bhttps://www.goodreads.com/quotes/1413237-consider-an-ai-that-has-hedonism-as-its-final-goal Quote from <i>Superintelligence</i>\uff3d</li><li>\uff3bhttps://www.lesswrong.com/tag/fun-theory Fun theory\uff3d</li><li>\uff3bhttps://www.lesswrong.com/tag/complexity-of-value Complexity of value\uff3d</li><li>\uff3bhttps://www.lesswrong.com/tag/utilitronium Utilitronium\uff3d</li></ul><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/orgasmium?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/orgasmium?edit\ua78atrue</div>"}, {"title": "What is \"narrow AI\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_%22narrow_AI%22%3F", "content": "<p>A<strong> Narrow AI</strong> is capable of operating only in a relatively limited domain, such as chess or driving, rather than capable of learning a broad range of tasks like a human or an \uff3bhttps://www.lesswrong.com/tag/artificial-general-intelligence Artificial General Intelligence\uff3d. Narrow vs General is not a perfectly binary classification, there are degrees of generality with, for example, large language models having a fairly large degree of generality (as the domain of text is large) without being as general as a human, and we may eventually build systems that are significantly more general than humans.</p>"}, {"title": "What is \"superintelligence\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_%22superintelligence%22%3F", "content": "A superintelligence is a mind that is much more intelligent than any human. Most of the time, it\u2019s used to discuss hypothetical future AIs."}, {"title": "What is \"transformative AI\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_%22transformative_AI%22%3F", "content": "<p><strong>Transformative AI</strong> is \"\uff3b...\uff3d AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution.\"<span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnrefhy8b4kflu8\"><sup>(ref)<p>As defined by \uff3bhttps://www.openphilanthropy.org/research/potential-risks-from-advanced-artificial-intelligence-the-philanthropic-opportunity/ Open Philanthropy's Holden Karnofsky in 2016\uff3d, and reused by \uff3bhttps://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf the Center for the Governance of AI in 2018\uff3d(/ref)</sup></span>&nbsp;The concept refers to the large effects of AI systems on our well-being, the global economy, state power, international security, etc. and not to specific capabilities that AI might have (unlike the related terms \uff3bhttps://www.lesswrong.com/tag/superintelligence Superintelligent AI\uff3d and \uff3bhttps://www.lesswrong.com/tag/artificial-general-intelligence Artificial General Intelligence\uff3d).</p><p>Holden Karnofsky gives a more detailed definition in \uff3bhttps://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/ another OpenPhil 2016 post\uff3d:</p><blockquote><p>\uff3b...\uff3d Transformative AI is anything that fits one or more of the following descriptions (emphasis original):</p><ul><li>AI systems capable of fulfilling all the necessary functions of human scientists, unaided by humans, in developing another technology (or set of technologies) that ultimately becomes widely credited with being the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. Note that just because AI systems <i>could</i> accomplish such a thing unaided by humans doesn\u2019t mean they <i>would</i>; it\u2019s possible that human scientists would provide an important complement to such systems, and could make even faster progress working in tandem than such systems could achieve unaided. I emphasize the hypothetical possibility of AI systems conducting substantial unaided research to draw a clear distinction from the types of AI systems that exist today. I believe that AI systems capable of such broad contributions to the relevant research would likely dramatically accelerate it.</li><li>AI systems capable of performing tasks that currently (in 2016) account for the majority of full-time jobs worldwide, and/or over 50% of total world wages, unaided and for costs in the same range as what it would cost to employ humans. Aside from the fact that this would likely be sufficient for a major economic transformation relative to today, I also think that an AI with such broad abilities would likely be able to far surpass human abilities in a subset of domains, making it likely to meet one or more of the other criteria laid out here.</li><li>Surveillance, autonomous weapons, or other AI-centric technology that becomes sufficiently advanced to be the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. (This contrasts with the first point because it refers to transformative technology that is itself AI-centric, whereas the first point refers to AI used to speed research on some other transformative technology.)</li></ul></blockquote><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/transformative-ai?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/transformative-ai?edit\ua78atrue</div>"}, {"title": "What is AI Safety via Debate?", "authors": "", "url": "https://stampy.ai/wiki/What_is_AI_Safety_via_Debate%3F", "content": "<p><strong>Debate</strong> is a proposed technique for allowing human evaluators to get correct and helpful answers from experts, even if the evaluator is not themselves an expert or able to fully verify the answers.<span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnref7clr966emb9\"><sup>(ref)<p>https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1</p>(/ref)</sup></span>&nbsp;The technique was suggested as part of an approach to build advanced AI systems that are aligned with human values, and to safely apply machine learning techniques to problems that have high stakes, but are not well-defined (such as advancing science or increase a company's revenue).&nbsp;<span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnrefvrcbanw2zz\"><sup>(ref)<p>https://ought.org/mission</p>(/ref)</sup></span><span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnrefnwfhnzy6a3e\"><sup>(ref)<p>https://openai.com/blog/debate/(/ref)</sup></span></p><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/debate-ai-safety-technique-1?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/debate-ai-safety-technique-1?edit\ua78atrue</div>"}, {"title": "What is Aligned AI / Stuart Armstrong working on?", "authors": "", "url": "https://stampy.ai/wiki/What_is_Aligned_AI_/_Stuart_Armstrong_working_on%3F", "content": "One of the key problems in AI safety is that there are many ways for an AI to generalize off-distribution, so it is very likely that an arbitrary generalization will be unaligned. See the \uff3bhttps://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1 model splintering post\uff3d for more detail.  Stuart's plan to solve this problem is as follows:\n\n#Maintain a set of all possible extrapolations of reward data that are consistent with the training process.\n#Pick among these for a safe reward extrapolation. \n\nThey are currently working on algorithms to accomplish step 1: see \uff3bhttps://www.lesswrong.com/posts/i8sHdLyGQeBTGwTqq/value-extrapolation-concept-extrapolation-model-splintering Value Extrapolation\uff3d. \n\nTheir initial operationalization of this problem is the lion and husky problem. Basically: if you train an image model on a dataset of images of lions and huskies, the lions are always in the desert, and the huskies are always in the snow. So the problem of learning a classifier is under-defined: should the classifier be classifying based on the background environment (e.g. snow vs sand), or based on the animal in the image? \n\nA good extrapolation algorithm, on this problem, would generate classifiers that extrapolate in all the different ways\uff3b4\uff3d,  and so the 'correct' extrapolation must be in this generated set of classifiers. They have also introduced a new dataset for this, with a similar idea: \uff3bhttps://www.lesswrong.com/posts/DiEWbwrChuzuhJhGr/benchmark-for-successful-concept-extrapolation-avoiding-goal Happy Faces\uff3d.\n\nStep 2 could be done in different ways. Possibilities for doing this include: \uff3bhttps://www.lesswrong.com/posts/PADPJ3xac5ogjEGwA/defeating-goodhart-and-the-closest-unblocked-strategy conservatism\uff3d, \uff3bhttps://www.lesswrong.com/posts/BeeirdrMXCPYZwgfj/the-blue-minimising-robot-and-model-splintering generalized deference to humans\uff3d,  or an automated process for removing some goals.   like wireheading/deception/killing everyone."}, {"title": "What is Anthropic's approach to LLM alignment?", "authors": "", "url": "https://stampy.ai/wiki/What_is_Anthropic%27s_approach_to_LLM_alignment%3F", "content": "Anthropic fine tuned a language model to be more helpful, honest and harmless: \uff3bhttps://arxiv.org/abs/2112.00861 HHH\uff3d.\n\nMotivation: The point of this is to:\n#see if we can \"align\" a current day LLM, and \n#raise awareness about safety in the broader ML community."}, {"title": "What is Conjecture's Scalable LLM Interpretability research adgenda?", "authors": "", "url": "https://stampy.ai/wiki/What_is_Conjecture%27s_Scalable_LLM_Interpretability_research_adgenda%3F", "content": "I don't know much about their research here, other than that they train their own models, which allow them to work on models that are bigger than the biggest publicly available models, which seems like a difference from Redwood.\n\nCurrent interpretability methods are very low level (e.g., \"what does x neuron do\"), which does not help us answer high level questions like \"is this AI trying to kill us\".\n\nThey are trying a bunch of weird approaches, with the goal of scalable mechanistic interpretability, but I do not know what these approaches actually are.\n\nMotivation: Conjecture wants to build towards a better paradigm that will give us a lot more information, primarily from the empirical direction (as distinct from ARC, which is working on interpretability with a theoretical focus)."}, {"title": "What is Conjecture's epistemology research agenda?", "authors": "", "url": "https://stampy.ai/wiki/What_is_Conjecture%27s_epistemology_research_agenda%3F", "content": "The alignment problem is really hard to do science on: we are trying to reason about the future, and we only get one shot, meaning that \uff3bhttps://www.lesswrong.com/posts/uhxpJyGYQ5FQRvdjY/abstracting-the-hardness-of-alignment-unbounded-atomic we can't iterate\uff3d. Therefore, it seems really useful to have a good understanding of meta-science/epistemology, i.e. reasoning about ways to do useful alignment research."}, {"title": "What is David Krueger working on?", "authors": "", "url": "https://stampy.ai/wiki/What_is_David_Krueger_working_on%3F", "content": "David runs a lab at the University of Cambridge. Some things he is working on include:  \n\n#Operationalizing inner alignment failures and other speculative alignment failures that haven't actually been observed. \n#Understanding neural network generalization. \n\nFor work done on (1), see: \uff3bhttps://arxiv.org/abs/2105.14111 Goal Misgeneralization\uff3d, a paper that empirically demonstrated examples of inner alignment failure in Deep RL environments. For example, they trained an agent to get closer to cheese in a maze, but where the cheese was always in the top right of a maze in the training set. During test time, when presented with cheese elsewhere, the RL agent navigated to the top right instead of to the cheese: it had learned the mesa objective of \"go to the top right\". \n\nFor work done on (2), see \uff3bhttp://proceedings.mlr.press/v139/krueger21a.html OOD Generalization via Risk Extrapolation\uff3d, an iterative improvement on robustness to previous methods. \n\nWe've not read about his motivation is for these specific research directions, but these are likely his best starts on how to solve the alignment problem."}, {"title": "What is Dylan Hadfield-Menell's thesis on?", "authors": "", "url": "https://stampy.ai/wiki/What_is_Dylan_Hadfield-Menell%27s_thesis_on%3F", "content": "\uff3bhttps://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-207.pdf Dylan's PhD thesis\uff3d argues three main claims (paraphrased): \n\n#Outer alignment failures are a problem.\n#We can mitigate this problem by adding in uncertainty.\n#We can model this as \uff3bhttps://proceedings.neurips.cc/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html Cooperative Inverse Reinforcement Learning (CIRL)\uff3d.\n  \nThus, his motivations seem to be modeling AGI coming in some multi-agent form, and also being heavily connected with human operators. \n\nWe're not certain what he is currently working on, but some recent alignment-relevant papers that he has published include: \n\n*\uff3bhttps://www.pnas.org/doi/10.1073/pnas.2106028118 Work on instantiating norms into AIs to incentivize deference to humans\uff3d. \n*\uff3bhttps://arxiv.org/abs/2102.03896 Theoretically formulating the principal-agent problem\uff3d. \n\nDylan has also published a number of articles that seem less directly relevant for alignment."}, {"title": "What is GPT-3?", "authors": "", "url": "https://stampy.ai/wiki/What_is_GPT-3%3F", "content": "GPT-3 is the newest and most impressive of the \uff3bhttps://www.alignmentforum.org/tag/gpt GPT\uff3d (Generative Pretrained Transformer) series of large transformer-based language models created by OpenAI. It was announced in June 2020, and is 100 times larger than its predecessor GPT-2.(ref)\uff3bhttps://www.cambridge.org/core/journals/natural-language-engineering/article/gpt3-whats-it-good-for/0E05CFE68A7AC8BF794C8ECBE28AA990 GPT-3: What\u2019s it good for?\uff3d - Cambridge University Press(/ref) \n\nGwern has several resources exploring GPT-3's abilities, limitations, and implications including:\n* \uff3bhttps://www.gwern.net/Scaling-hypothesis The Scaling Hypothesis\uff3d - How simply increasing the amount of compute with current algorithms might create very powerful systems.\n* \uff3bhttps://www.gwern.net/GPT-3-nonfiction GPT-3 Nonfiction\uff3d\n* \uff3bhttps://www.gwern.net/GPT-3 GPT-3 Creative Fiction\uff3d\n\nVox has \uff3bhttps://www.vox.com/future-perfect/21355768/gpt-3-ai-openai-turing-test-language an article\uff3d which explains why GPT-3 is a big deal."}, {"title": "What is Goodhart's law?", "authors": "", "url": "https://stampy.ai/wiki/What_is_Goodhart%27s_law%3F", "content": "<p><strong>Goodhart's Law </strong>states that when a proxy for some value becomes the target of optimization pressure, the proxy will cease to be a good proxy. One form of Goodhart is demonstrated by the Soviet story of a factory graded on how many shoes they produced (a good proxy for productivity) \u2013 they soon began producing a higher number of tiny shoes. Useless, but the numbers look good.</p><p>Goodhart's Law is of particular relevance to \uff3bhttps://www.lessestwrong.com/tag/ai AI Alignment\uff3d. Suppose you have something which is generally a good proxy for \"the stuff that humans care about\", it would be dangerous to have a powerful AI optimize for the proxy, in accordance with Goodhart's law, the proxy will breakdown. &nbsp;</p><h2>Goodhart Taxonomy</h2><p>In \uff3bhttps://www.lessestwrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy Goodhart Taxonomy\uff3d, Scott Garrabrant identifies four kinds of Goodharting:</p><ul><li>Regressional Goodhart - When selecting for a proxy measure, you select not only for the true goal, but also for the difference between the proxy and the goal.</li><li>Causal Goodhart - When there is a non-causal correlation between the proxy and the goal, intervening on the proxy may fail to intervene on the goal.</li><li>Extremal Goodhart - Worlds in which the proxy takes an extreme value may be very different from the ordinary worlds in which the correlation between the proxy and the goal was observed.</li><li>Adversarial Goodhart - When you optimize for a proxy, you provide an incentive for adversaries to correlate their goal with your proxy, thus destroying the correlation with your goal.</li></ul><h2>See Also</h2><ul><li>\uff3bhttps://lessestwrong.com/tag/groupthink Groupthink\uff3d, \uff3bhttps://lessestwrong.com/tag/information-cascades Information cascade\uff3d, \uff3bhttps://lessestwrong.com/tag/affective-death-spiral Affective death spiral\uff3d</li><li>\uff3bhttps://wiki.lesswrong.com/wiki/Adaptation_executers Adaptation executers\uff3d, \uff3bhttps://lessestwrong.com/tag/superstimuli Superstimulus\uff3d</li><li>\uff3bhttps://lessestwrong.com/tag/signaling Signaling\uff3d, \uff3bhttps://lessestwrong.com/tag/filtered-evidence Filtered evidence\uff3d</li><li>\uff3bhttps://lessestwrong.com/tag/cached-thought Cached thought\uff3d</li><li>\uff3bhttps://lessestwrong.com/tag/modesty-argument Modesty argument\uff3d, \uff3bhttps://lessestwrong.com/tag/egalitarianism Egalitarianism\uff3d</li><li>\uff3bhttps://lessestwrong.com/tag/rationalization Rationalization\uff3d, \uff3bhttps://lessestwrong.com/tag/dark-arts Dark arts\uff3d</li><li>\uff3bhttps://lessestwrong.com/tag/epistemic-hygiene Epistemic hygiene\uff3d</li><li>\uff3bhttps://lessestwrong.com/tag/scoring-rule Scoring rule\uff3d</li></ul><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/goodhart-s-law?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/goodhart-s-law?edit\ua78atrue</div>\nhttps://i.imgur.com/Ty08pzQ.png"}, {"title": "What is John Wentworth's plan?", "authors": "", "url": "https://stampy.ai/wiki/What_is_John_Wentworth%27s_plan%3F", "content": "\uff3bhttps://www.lesswrong.com/posts/3L46WGauGpr7nYubu/the-plan John's plan\uff3d is:\n\nStep 1: sort out our fundamental confusions about agency\n\nStep 2: ambitious value learning (i.e. build an AI which correctly learns human values and optimizes for them)\n\nStep 3: \u2026\n\nStep 4: profit!\n\n\u2026 and do all that before AGI kills us all.\n\nHe is working on step 1: figuring out what the heck is going on with agency. His current approach is based on \uff3bhttps://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents selection theorems\uff3d: try to figure out what types of agents are selected for in a broad range of environments. Examples of selection pressures include: evolution, SGD, and markets. This is an approach to agent foundations that comes from the opposite direction as MIRI: it's more about observing existing structures (whether they be mathematical or real things in the world like markets or e coli), whereas MIRI is trying to write out some desiderata and then finding mathematical notions that satisfy those desiderata.\n\nTwo key properties that might be selected for are modularity and abstractions.\n\nAbstractions are higher level things that people tend to use to describe things. Like \"Tree\" and \"Chair\" and \"Person\". These are all vague categories that contain lots of different things, but are really useful for narrowing down things. Humans tend to use really similar abstractions, even across different cultures / societies. \uff3bhttps://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro The Natural Abstraction Hypothesis\uff3d (NAH) states that a wide variety of cognitive architectures will tend to use similar abstractions to reason about the world. This might be helpful for alignment because we could say things like \"person\" without having to rigorously and precisely say exactly what we mean by person.\n\nThe NAH seems very plausibly true for physical objects in the world, and so it might be true for the inputs to human values. If so, it would be really helpful for AI alignment because understanding this would amount to a solution to the \uff3bhttps://arbital.com/p/ontology_identification/ ontology identification problem\uff3d: we can understand when environments induce certain abstractions, and so we can design this so that the network has the same abstractions as humans.\n\n\uff3bhttps://www.lesswrong.com/s/ApA5XmewGQ8wSrv5C Modularity\uff3d: In pretty much any selection environment, we see lots of obvious modularity. Biological species have cells and organs and limbs. Companies have departments. We might expect neural networks to be similar, but it is \uff3bhttps://www.lesswrong.com/posts/JBFHzfPkXHB2XfDGj/evolution-of-modularity really hard to find modules\uff3d in neural networks. We need to find the right lens to look through to find this modularity in neural networks. Aiming at this can lead us to really good interpretability."}, {"title": "What is MIRI\u2019s mission?", "authors": "", "url": "https://stampy.ai/wiki/What_is_MIRI%E2%80%99s_mission%3F", "content": "\uff3bhttps://intelligence.org/ MIRI's\uff3d mission statement is to \u201censure that the creation of smarter-than-human artificial intelligence has a positive impact.\u201d This is an ambitious goal, but they believe that some early progress is possible, and they believe that the goal\u2019s importance and difficulty makes it prudent to begin work at an early date.\n\nTheir two main research agendas, \u201c\uff3bhttps://intelligence.org/technical-agenda Agent Foundations for Aligning Machine Intelligence with Human Interests\uff3d\u201d and \u201c\uff3bhttps://intelligence.org/2016/05/04/announcing-a-new-research-program/ Value Alignment for Advanced Machine Learning Systems\uff3d,\u201d focus on three groups of technical problems:\n* highly reliable agent design \u2014 learning how to specify highly autonomous systems that reliably pursue some fixed goal;\n* value specification \u2014 supplying autonomous systems with the intended goals; and\n* error tolerance \u2014 making such systems robust to programmer error.\nThat being said, MIRI recently \uff3bhttps://intelligence.org/2020/12/21/2020-updates-and-strategy/ published an update\uff3d stating that they were moving away from research directions in unpublished works that they were \uff3bhttps://intelligence.org/2018/11/22/2018-update-our-new-research-directions/ pursuing since 2017\uff3d.\n\nThey publish new \uff3bhttps://intelligence.org/research mathematical results\uff3d (although their work is \uff3bhttps://intelligence.org/2018/11/22/2018-update-our-new-research-directions/#section3 non-disclosed by default\uff3d), host \uff3bhttps://intelligence.org/research workshops\uff3d, attend conferences, and \uff3bhttps://intelligence.org/mirix fund outside researchers\uff3d who are interested in investigating these problems. They also host a \uff3bhttps://intelligence.org/blog blog\uff3d and an \uff3bhttps://www.alignmentforum.org/ online research forum\uff3d."}, {"title": "What is Stampy's copyright?", "authors": "", "url": "https://stampy.ai/wiki/What_is_Stampy%27s_copyright%3F", "content": "* All content produced on this wiki is released under the \uff3bhttps://creativecommons.org/licenses/by-sa/4.0/legalcode CC-BY-SA 4.0 license\uff3d. Exceptions for unattributed use may be granted by admins, contact \uff3b\uff3bUser:plex\u250aplex\uff3d\uff3d for inquiries.\n* Questions from YouTube or other sources are reproduced with the intent of fair use, as derivative and educational material.\n* Source code of https://ui.stampy.ai/ is released under \uff3bhttps://github.com/Aprillion/stampy-ui/blob/master/LICENSE MIT license\uff3d\n* Logo and visual design copyright is owned by Rob Miles, all rights reserved.\n\n\uff3b\uff3bCategory:Meta\uff3d\uff3d"}, {"title": "What is a \"quantilizer\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_a_%22quantilizer%22%3F", "content": "<p>A <strong>Quantilizer</strong> is a proposed AI design which aims to reduce the harms from \uff3bhttps://www.lesswrong.com/tag/goodhart-s-law Goodhart's law\uff3d and specification gaming by selecting reasonably effective actions from a distribution of human-like actions, rather than maximizing over actions. It it more of a theoretical tool for exploring ways around these problems than a practical buildable design.</p><h3>See also</h3><ul><li>\uff3bhttps://www.youtube.com/watch?v\ua78agdKMG6kTl6Y <strong>Rob Miles's Quantilizers: AI That Doesn't Try Too Hard</strong>\uff3d</li><li>\uff3bhttps://arbital.com/p/soft_optimizer?l\ua78a2r8#Quantilizing <strong>Arbital page on Quantilizers</strong>\uff3d</li></ul><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/quantilization?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/quantilization?edit\ua78atrue</div>"}, {"title": "What is a \"value handshake\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_a_%22value_handshake%22%3F", "content": "A value handshake is a form of trade between superintelligences, when two AI's with incompatible utility functions meet, instead of going to war, since they have superhuman prediction abilities and likely know the outcome before any attack even happens, they can decide to split the universe into chunks with volumes according to their respective military strength or chance of victory, and if their utility functions are compatible, they might even decide to merge into an AI with an utility function that is the weighted average of the two previous ones.\n\nThis could happen if multiple AI's are active on earth at the same time, and then maybe if at least one of them is aligned with humans, the resulting value handshake could leave humanity in a pretty okay situation. \n\nSee \uff3bhttps://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/ The Hour I First Believed\uff3d By Scott Alexander for some further thoughts and an introduction to related topics."}, {"title": "What is a canonical question on Stampy's Wiki?", "authors": "", "url": "https://stampy.ai/wiki/What_is_a_canonical_question_on_Stampy%27s_Wiki%3F", "content": "'''\uff3b\uff3bCanonical questions\uff3d\uff3d''' are the questions which we've checked are in \uff3b\uff3bscope\uff3d\uff3d and not duplicates, so we want answers to them. They may be edited to represent a class of question more broadly, rather than keeping all their idosyncracies. Once they're answered canonically Stampy will serve them to readers."}, {"title": "What is a duplicate question on Stampy's Wiki?", "authors": "", "url": "https://stampy.ai/wiki/What_is_a_duplicate_question_on_Stampy%27s_Wiki%3F", "content": "An existing question is a duplicate of a new one if it is reasonable to expect whoever asked the new question to be satisfied if they received an answer to the existing question instead."}, {"title": "What is a follow-up question on Stampy's Wiki?", "authors": "", "url": "https://stampy.ai/wiki/What_is_a_follow-up_question_on_Stampy%27s_Wiki%3F", "content": "'''Follow-up questions''' are responses to an \uff3b\uff3banswer\uff3d\uff3d which reader might have, either because they want more information or are providing information to Stampy about what they're looking for. We don't expect to have great coverage of the former for a long time because there will be so many, but hopefully we'll be able to handle some of the most common ones."}, {"title": "What is an \"agent\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_an_%22agent%22%3F", "content": "<p>A <strong>rational agent</strong> is an entity which has a utility function, forms beliefs about its environment, evaluates the consequences of possible actions, and then takes the action which maximizes its utility. They are also referred to as goal-seeking. The concept of a <strong>rational agent</strong> is used in \uff3bhttps://www.lesswrong.com/tag/economics economics\uff3d, \uff3bhttps://www.lesswrong.com/tag/game-theory game theory\uff3d, \uff3bhttps://www.lesswrong.com/tag/decision-theory decision theory\uff3d, and artificial intelligence.</p><p><i>Editor note: there is work to be done reconciling this page, Agency page, and Robust Agents. Currently they overlap and I'm not sure they're consistent. - Ruby, 2020-09-15</i></p><p>More generally, an <strong>agent</strong> is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.<span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnref96bl2f2uov\"><sup>(ref)<p>Russel, S. & Norvig, P. (2003) Artificial Intelligence: A Modern Approach. Second Edition. Page 32.(/ref)</sup></span></p><p>There has been much discussion as to whether certain \uff3bhttps://wiki.lesswrong.com/wiki/AGI AGI\uff3d designs can be made into \uff3bhttps://www.lesswrong.com/tag/tool-ai mere tools\uff3d or whether they will necessarily be agents which will attempt to actively carry out their goals. Any minds that actively engage in goal-directed behavior are \uff3bhttps://wiki.lesswrong.com/wiki/Unfriendly_AI potentially dangerous\uff3d, due to considerations such as \uff3bhttps://www.lesswrong.com/tag/instrumental-convergence basic AI drives\uff3d possibly causing behavior which is in conflict with humanity's values.</p><p>In \uff3bhttp://lesswrong.com/lw/tj/dreams_of_friendliness/ Dreams of Friendliness\uff3d and in \uff3bhttp://lesswrong.com/lw/cze/reply_to_holden_on_tool_ai/ Reply to Holden on Tool AI\uff3d, \uff3bhttps://www.lesswrong.com/tag/eliezer-yudkowsky Eliezer Yudkowsky\uff3d argues that, since all intelligences select correct beliefs from the much larger space of incorrect beliefs, they are necessarily agents.</p><h2>See also</h2><ul><li>\uff3b/tag/agency Agency\uff3d</li><li>\uff3b/tag/robust-agents Robust Agents\uff3d</li><li>\uff3bhttps://www.lesswrong.com/tag/tool-ai Tool AI\uff3d</li><li>\uff3bhttps://www.lesswrong.com/tag/oracle-ai Oracle AI\uff3d</li></ul><h2>Posts</h2><ul><li>\uff3bhttp://lesswrong.com/lw/5i8/the_power_of_agency/ The Power of Agency\uff3d</li></ul><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/agent?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/agent?edit\ua78atrue</div>"}, {"title": "What is an \"intelligence explosion\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_an_%22intelligence_explosion%22%3F", "content": "The intelligence explosion idea was expressed by statistician \uff3bhttp://www.incompleteideas.net/papers/Good65ultraintelligent.pdf I.J. Good in 1965\uff3d:\n<blockquote>Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an \u2018intelligence explosion\u2019, and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make.</blockquote>\nThe argument is this: Every year, computers surpass human abilities in new ways. A program written in 1956 was able to prove mathematical theorems, and \uff3bhttp://www.cs.cornell.edu/courses/cs4860/2012fa/MacKenzie-TheAutomationOfProof.pdf found a more elegant proof\uff3d for one of them than Russell and Whitehead had given in ''Principia Mathematica''. By the late 1990s, \u2018expert systems\u2019 had surpassed human skill for a \uff3bhttp://www.amazon.com/dp/0521122937/ wide range of tasks\uff3d. In 1997, IBM\u2019s Deep Blue computer beat the world chess champion, and in 2011, IBM\u2019s Watson computer beat the best human players at a much more complicated game: \uff3bhttp://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?_r\ua78a2&ref\ua78ahomepage&src\ua78ame&pagewanted\ua78aall Jeopardy!\uff3d. Recently, \uff3bhttp://commonsenseatheism.com/wp-content/uploads/2011/02/King-The-Automation-of-Science.pdf a robot named Adam\uff3d was programmed with our scientific knowledge about yeast, then posed its own hypotheses, tested them, and assessed the results.\n\nComputers remain far short of human intelligence, but the resources that aid AI design are accumulating (including hardware, large datasets, neuroscience knowledge, and AI theory). We may one day design a machine that surpasses human skill at designing artificial intelligences. After that, this machine could improve its own intelligence faster and better than humans can, which would make it even more skilled at improving its own intelligence. This could continue in a positive feedback loop such that the machine quickly becomes vastly more intelligent than the smartest human being on Earth: an \u2018intelligence explosion\u2019 resulting in a machine superintelligence.\n\nThis is what is meant by the \u2018intelligence explosion\u2019 in this FAQ.\n\nSee also:\n\n* Vinge, \uff3bhttp://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html The Coming Technological Singularity\uff3d\n* Wikipedia, \uff3bhttp://en.wikipedia.org/wiki/Technological_singularity Technological Singularity\uff3d\n* Chalmers, \uff3bhttp://commonsenseatheism.com/wp-content/uploads/2011/01/Chalmers-The-Singularity-a-philosophical-analysis.pdf The Singularity: A Philosophical Analysis\uff3d"}, {"title": "What is an \"s-risk\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_an_%22s-risk%22%3F", "content": "<p><strong>(Astronomical) suffering risks</strong>, also known as <strong>s-risks</strong>, are risks of the creation of intense suffering in the far future on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far.</p><p>S-risks are an example of \uff3bhttps://www.lesswrong.com/tag/existential-risk existential risk\uff3d (also known as <i>x-risks</i>) according to Nick Bostrom's original definition, as they threaten to \"permanently and drastically curtail \uff3bEarth-originating intelligent life's\uff3d potential\". Most existential risks are of the form \"event E happens which drastically reduces the number of conscious experiences in the future\". S-risks therefore serve as a useful reminder that some x-risks are scary because they cause <i>bad</i> experiences, and not just because they prevent good ones.</p><p>Within the space of x-risks, we can distinguish x-risks that are s-risks, x-risks involving human extinction, x-risks that involve immense suffering <i>and</i> human extinction, and x-risks that involve neither. For example:</p><figure class\ua78a\"table\"><table><tbody><tr><td>&nbsp;</td><td><strong>extinction risk</strong></td><td><strong>non-extinction risk</strong></td></tr><tr><td><strong>suffering risk</strong></td><td>Misaligned AGI wipes out humans, simulates many suffering alien civilizations.</td><td>Misaligned AGI tiles the universe with experiences of severe suffering.</td></tr><tr><td><strong>non-suffering risk</strong></td><td>Misaligned AGI wipes out humans.</td><td>Misaligned AGI keeps humans as \"pets,\" limiting growth but not causing immense suffering.</td></tr></tbody></table></figure><p>A related concept is \uff3bhttps://arbital.com/p/hyperexistential_separation/ <strong>hyperexistential risk</strong>\uff3d, the risk of \"fates worse than death\" on an astronomical scale. It is not clear whether all hyperexistential risks are s-risks per se. But arguably all s-risks are hyperexistential, since \"tiling the universe with experiences of severe suffering\" would likely be worse than death.</p><p>There are two \uff3bhttps://wiki.lesswrong.com/wiki/EA EA\uff3d organizations with s-risk prevention research as their primary focus: the \uff3bhttps://www.lesswrong.com/tag/center-on-long-term-risk-clr Center on Long-Term Risk\uff3d (CLR) and the \uff3bhttps://centerforreducingsuffering.org/ Center for Reducing Suffering\uff3d. Much of CLR's work is on suffering-focused \uff3bhttps://wiki.lesswrong.com/wiki/AI_safety AI safety\uff3d and \uff3bhttps://www.lesswrong.com/tag/crucial-considerations crucial considerations\uff3d. Although to a much lesser extent, the \uff3bhttps://www.lesswrong.com/tag/machine-intelligence-research-institute-miri Machine Intelligence Research Institute\uff3d and \uff3bhttps://www.lesswrong.com/tag/future-of-humanity-institute-fhi Future of Humanity Institute\uff3d have investigated strategies to prevent s-risks too.&nbsp;</p><p>Another approach to reducing s-risk is to \"expand the moral circle\" \uff3bhttps://magnusvinding.com/2018/09/04/moral-circle-expansion-might-increase-future-suffering/ <i>together</i>\uff3d with raising concern for suffering, so that future (post)human civilizations and AI are less likely to \uff3bhttps://www.lesswrong.com/tag/instrumental-value instrumentally\uff3d cause suffering to non-human minds such as animals or digital sentience. \uff3bhttp://www.sentienceinstitute.org/ Sentience Institute\uff3d works on this value-spreading problem.</p><p>&nbsp;</p><h2>See also</h2><ul><li>\uff3bhttps://www.lesswrong.com/tag/center-on-long-term-risk-clr Center on Long-Term Risk\uff3d</li><li>\uff3bhttps://www.lesswrong.com/tag/existential-risk Existential risk\uff3d</li><li>\uff3bhttps://www.lesswrong.com/tag/abolitionism Abolitionism\uff3d</li><li>\uff3bhttps://wiki.lesswrong.com/wiki/Mind_crime Mind crime\uff3d</li><li>\uff3bhttps://www.lesswrong.com/tag/utilitarianism Utilitarianism\uff3d, \uff3bhttps://www.lesswrong.com/tag/hedonism Hedonism\uff3d</li></ul><p>&nbsp;</p><h2>External links</h2><ul><li>\uff3bhttps://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-global-priority/ Reducing Risks of Astronomical Suffering: A Neglected Global Priority (FRI)\uff3d</li><li>\uff3bhttps://foundational-research.org/s-risks-talk-eag-boston-2017/ Introductory talk on s-risks (FRI)\uff3d</li><li>\uff3bhttps://foundational-research.org/risks-of-astronomical-future-suffering/ Risks of Astronomical Future Suffering (FRI)\uff3d</li><li>\uff3bhttps://foundational-research.org/files/suffering-focused-ai-safety.pdf Suffering-focused AI safety: Why \"fail-safe\" measures might be our top intervention PDF (FRI)\uff3d</li><li>\uff3bhttps://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering Artificial Intelligence and Its Implications for Future Suffering (FRI)\uff3d</li><li>\uff3bhttps://sentience-politics.org/expanding-moral-circle-reduce-suffering-far-future/ Expanding our moral circle to reduce suffering in the far future (Sentience Politics)\uff3d</li><li>\uff3bhttps://sentience-politics.org/philosophy/the-importance-of-the-future/ The Importance of the Far Future (Sentience Politics)\uff3d</li></ul><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks?edit\ua78atrue</div>"}, {"title": "What is artificial general intelligence safety / AI alignment?", "authors": "", "url": "https://stampy.ai/wiki/What_is_artificial_general_intelligence_safety_/_AI_alignment%3F", "content": "''AI alignment'' is a field that is focused on causing the goals of future \uff3bhttps://en.wikipedia.org/wiki/Superintelligence superintelligent artificial systems\uff3d\nto align with \uff3bhttps://www.researchgate.net/publication/347891524_Literature_Review_What_AI_Safety_Researchers_Have_Written_About_the_Nature_of_Human_Values human values\uff3d, meaning that they would behave in a way which was compatible with our survival and flourishing. This may be an \uff3bhttps://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/ extremely hard problem\uff3d, especially with \uff3bhttps://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/ deep learning\uff3d, and is likely to determine the outcome of the \uff3bhttps://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/ most important century\uff3d. Alignment research is strongly interdisciplinary and can include computer science, mathematics, neuroscience, philosophy, and social sciences.\n\n''AGI safety'' is a related concept which strongly overlaps with AI alignment. AGI safety is concerned with making sure that building AGI systems doesn\u2019t cause things to go badly wrong, and the main way in which things can go badly wrong is through misalignment. AGI safety includes policy work that prevents the building of dangerous AGI systems, or reduces misuse risks from AGI systems aligned to actors who don\u2019t have humanity\u2019s best interests in mind."}, {"title": "What is causal decision theory?", "authors": "", "url": "https://stampy.ai/wiki/What_is_causal_decision_theory%3F", "content": "<p><strong>Causal Decision Theory</strong> \u2013 CDT - is a branch of \uff3bhttps://www.lesswrong.com/tag/decision-theory decision theory\uff3d which advises an agent to take actions that maximizes the causal consequences on the probability of desired outcomes \uff3b#fn1 <sup>1</sup>\uff3d. As any branch of decision theory, it prescribes taking the action that maximizes \uff3bhttps://www.lesswrong.com/tag/utility utility\uff3d, that which utility equals or exceeds the utility of every other option. The utility of each action is measured by the \uff3bhttps://www.lesswrong.com/tag/expected-utility expected utility\uff3d, the averaged by probabilities sum of the utility of each of its possible results. How the actions can influence the probabilities differ between the branches. Contrary to \uff3bhttps://www.lesswrong.com/tag/evidential-decision-theory Evidential Decision Theory\uff3d \u2013 EDT - CDT focuses on the causal relations between one\u2019s actions and its outcomes, instead of focusing on which actions provide evidences for desired outcomes. According to CDT a rational agent should track the available causal relations linking his actions to the desired outcome and take the action which will better enhance the chances of the desired outcome.</p><p>One usual example where EDT and CDT commonly diverge is the \uff3bhttps://www.lesswrong.com/tag/smoking-lesion Smoking lesion\uff3d: \u201cSmoking is strongly correlated with lung cancer, but in the world of the Smoker's Lesion this correlation is understood to be the result of a common cause: a genetic lesion that tends to cause both smoking and cancer. Once we fix the presence or absence of the lesion, there is no additional correlation between smoking and cancer. Suppose you prefer smoking without cancer to not smoking without cancer, and prefer smoking with cancer to not smoking with cancer. Should you smoke?\u201d CDT would recommend smoking since there is no causal connection between smoking and cancer. They are both caused by a gene, but have no causal direct connection with each other. EDT on the other hand would recommend against smoking, since smoking is an evidence for having the mentioned gene and thus should be avoided.</p><p>The core aspect of CDT is mathematically represented by the fact it uses probabilities of conditionals in place of conditional probabilities \uff3b#fn2 <sup>2</sup>\uff3d. The probability of a conditional is the probability of the whole conditional being true, where the conditional probability is the probability of the consequent given the antecedent. A conditional probability of B given A - P(B\u250aA) -, simply implies the \uff3bhttps://www.lesswrong.com/tag/bayesian-probability Bayesian probability\uff3d of the event B happening given we known A happened, it\u2019s used in EDT. The probability of conditionals \u2013 P(A > B) - refers to the probability that the conditional 'A implies B' is true, it is the probability of the contrafactual \u2018If A, then B\u2019 be the case. Since contrafactual analysis is the key tool used to speak about causality, probability of conditionals are said to mirror causal relations. In most cases these two probabilities track each other, and CDT and EDT give the same answers. However, some particular problems have arisen where their predictions for rational action diverge such as the \uff3bhttps://www.lesswrong.com/tag/smoking-lesion Smoking lesion\uff3d problem \u2013 where CDT seems to give a more reasonable prescription \u2013 and \uff3bhttps://www.lesswrong.com/tag/newcomb-s-problem Newcomb's problem\uff3d \u2013 where CDT seems unreasonable. David Lewis proved \uff3b#fn3 <sup>3</sup>\uff3d it's impossible to probabilities of conditionals to always track conditional probabilities. Hence, evidential relations aren\u2019t the same as causal relations and CDT and EDT will always diverge in some cases.</p><h2>References</h2><ol><li>\uff3bhttp://plato.stanford.edu/entries/decision-causal/ http://plato.stanford.edu/entries/decision-causal/\uff3d</li><li>Lewis, David. (1981) \"Causal Decision Theory,\" Australasian Journal of Philosophy 59 (1981): 5- 30.</li><li>Lewis, D. (1976), \"Probabilities of conditionals and conditional probabilities\", The Philosophical Review (Duke University Press) 85 (3): 297\u2013315</li></ol><h2>See also</h2><ul><li>\uff3bhttps://www.lesswrong.com/tag/decision-theory Decision theory\uff3d</li><li>\uff3bhttps://www.lesswrong.com/tag/evidential-decision-theory Evidential Decision Theory\uff3d</li></ul><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/causal-decision-theory?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/causal-decision-theory?edit\ua78atrue</div>"}, {"title": "What is interpretability and what approaches are there?", "authors": "", "url": "https://stampy.ai/wiki/What_is_interpretability_and_what_approaches_are_there%3F", "content": "Interpretability is about making machine learning (ML) systems easier to understand. It is hard because the computations of current ML systems often depend on billions of parameters which they learnt from data. Areas of research for making current ML models more understandable are ''mechanistic interpretability'', ''finding important input features'', ''explaining by examples'', ''natural language explanations'', and using ML architectures which are ''intrinsically interpretable''.\n\n# '''Mechanistic interpretability''' is about interpreting an ML model\u2019s internal representations. A very simple way to do this is \uff3bhttps://towardsdatascience.com/every-ml-engineer-needs-to-know-neural-network-interpretability-afea2ac0824e activation maximization\uff3d: optimize the input such that one particular neuron is activated a lot. This optimized input is an indicator of the concept which the neuron represents. Work that is central for mechanistic interpretability is the \uff3bhttps://distill.pub/2020/circuits/zoom-in/ circuits thread\uff3d, which focuses on interpreting the algorithms implemented by subgraphs (circuits) of neural networks. There is also work on \uff3bhttps://transformer-circuits.pub/2021/framework/index.html circuits in transformers\uff3d in particular. Mechanistic Interpretability has the drawback that \uff3bhttps://www.greaterwrong.com/posts/qXtbBAxmFkAQLQEJE/interpretability-tool-ness-alignment-corrigibility-are-not interpretability is not composable\uff3d, i.e.\u00a0even if we understand all the components of a system, it doesn\u2019t mean that we understand the whole. However, there may still be a way of hierarchically decomposing a system in a way that allows us to understand each layer of abstraction of it, and thus understanding the whole. <p style\ua78a\"text-align:center\">https://i.imgur.com/nGDvldz.png <br>  Feature visualization of a neuron that corresponds to dog-like features. \uff3bhttps://distill.pub/2020/circuits/zoom-in/ image source\uff3d </p>  \n# The idea of '''finding important input features''' is to find out which input features are most relevant for the output. In the case of image classification, we can highlight the relevant features with a heatmap, which is called \uff3bhttps://arxiv.org/abs/1312.6034 saliency map\uff3d). A very simple way to do this is to take the derivative of the output with regard to the different parts of the input. This derivative denotes how much the output changes if we change a particular part of the input, i.e.\u00a0how important that part of the input is for the output. Saliency maps can be useful to notice cases in which an image classifier learns to use features it should not use. For example, the paper ''\uff3bhttps://www.nature.com/articles/s41467-019-08987-4 Unmasking Clever Hans predictors and assessing what machines really learn\uff3d'' used saliency maps to show that a horse-classifying image classifier was not using the image parts that contained the horse at all, but rather relied on the name of the photographer printed in a corner, because one of the photographers primarily took photos of horses. \uff3bimage of horse thing, maybe see thesis\uff3d However, many of the common saliency methods fail basic \uff3bhttps://arxiv.org/abs/1810.03292 sanity checks\uff3d, such as the saliency maps almost not changing when the model weights are randomized. Therefore, saliency maps are not sufficient for a reliable understanding of ML systems. \n# '''Explanation by examples''' means showing examples in the training data that have similar features, such as in the paper ''\uff3bhttps://arxiv.org/abs/1806.10574 This Looks Like That: Deep Learning for Interpretable Image Recognition\uff3d''.\n# '''Natural Language Explanations''' are sentences describing a model\u2019s reasons for its outputs. For example, in the paper ''\uff3bhttps://xfgao.github.io/xCookingWeb/ Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks\uff3d'' a human and an AI play a virtual cooking game together, and the AI explains its plans in natural language. They find that with the explanations the human-AI team performs significantly better. \n# However, natural language explanations, as well as finding important features and explanation by examples are ''post-hoc'' explanations: They are generated after the fact, and are therefore likely to not be ''faithful'' (i.e.\u00a0not accurately describe a model\u2019s decision process). '''Interpretable architectures''' are architectures which are simple enough to be understandable without additional tools. Cynthia Rudin is a central researcher \uff3bhttps://arxiv.org/abs/1811.10154 arguing for using interpretable architectures\uff3d in high-stakes situations. However, using interpretable architecutures usually comes with a significant cost to model performance.\n\nYou can read more about different approaches in \uff3bhttps://www.alignmentforum.org/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries this overview article\uff3d which summarizes more than 70 interpretability-related papers, and in the free online book ''\uff3bhttps://christophm.github.io/interpretable-ml-book/ A Guide for Making Black Box Models Explainable\uff3d''."}, {"title": "What is meant by \"AI takeoff\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_meant_by_%22AI_takeoff%22%3F", "content": "<p><strong>AI Takeoff</strong> refers to the process of an \uff3bhttps://www.lesswrong.com/tag/artificial-general-intelligence Artificial General Intelligence\uff3d going from a certain threshold of capability (often discussed as \"human-level\") to being super-intelligent and capable enough to control the fate of civilization. There has been much debate about whether AI takeoff is more likely to be slow vs fast, i.e., \"soft\" vs \"hard\".</p><p><em>See also</em>: \uff3bhttps://www.lesswrong.com/tag/ai-timelines AI Timelines\uff3d, \uff3bhttps://www.lesswrong.com/tag/seed-ai Seed AI\uff3d, \uff3bhttps://www.lesswrong.com/tag/singularity Singularity\uff3d, \uff3bhttps://www.lesswrong.com/tag/intelligence-explosion Intelligence explosion\uff3d, \uff3bhttps://www.lesswrong.com/tag/recursive-self-improvement Recursive self-improvement\uff3d</p><p>AI takeoff is sometimes casually referred to as <strong>AI FOOM.</strong></p><h1>Soft takeoff</h1><p>A <strong>soft takeoff</strong> refers to an AGI that would self-improve over a period of years or decades. This could be due to either the learning algorithm being too demanding for the hardware or because the AI relies on experiencing feedback from the real-world that would have to be played out in real-time. Possible methods that could deliver a soft takeoff, by slowly building on human-level intelligence, are \uff3bhttps://www.lesswrong.com/tag/whole-brain-emulation Whole brain emulation\uff3d, \uff3bhttps://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement Biological Cognitive Enhancement\uff3d, and software-based strong AGI \uff3b\uff3bhttps://www.lesswrong.com/tag/ai-takeoff?revision\ua78a0.0.24&lw_source\ua78aimport_sheet#fn1 1\uff3d\uff3d. By maintaining control of the AGI&apos;s ascent it should be easier for a \uff3bhttps://wiki.lesswrong.com/wiki/Friendly_AI Friendly AI\uff3d to emerge.</p><p>Vernor Vinge, Hans Moravec and have all expressed the view that soft takeoff is preferable to a hard takeoff as it would be both safer and easier to engineer.</p><h1>Hard takeoff</h1><p>A <strong>hard takeoff</strong> (or an AI going \"<strong>FOOM</strong>\" \uff3b\uff3bhttps://www.lesswrong.com/tag/ai-takeoff?revision\ua78a0.0.24&lw_source\ua78aimport_sheet#fn2 2\uff3d\uff3d) refers to AGI expansion in a matter of minutes, days, or months. It is a fast, abruptly, local increase in capability. This scenario is widely considered much more precarious, as this involves an AGI rapidly ascending in power without human control. This may result in unexpected or undesired behavior (i.e. \uff3bhttps://wiki.lesswrong.com/wiki/Unfriendly_AI Unfriendly AI\uff3d). It is one of the main ideas supporting the \uff3bhttps://www.lesswrong.com/tag/intelligence-explosion Intelligence explosion\uff3d hypothesis.</p><p>The feasibility of hard takeoff has been addressed by Hugo de Garis, \uff3bhttps://www.lesswrong.com/tag/eliezer-yudkowsky Eliezer Yudkowsky\uff3d, \uff3bhttps://www.lesswrong.com/tag/ben-goertzel Ben Goertzel\uff3d, \uff3bhttps://www.lesswrong.com/tag/nick-bostrom Nick Bostrom\uff3d, and Michael Anissimov. It is widely agreed that a hard takeoff is something to be avoided due to the risks. Yudkowsky points out several possibilities that would make a hard takeoff more likely than a soft takeoff such as the existence of large \uff3bhttps://www.lesswrong.com/tag/computing-overhang resources overhangs\uff3d or the fact that small improvements seem to have a large impact in a mind&apos;s general intelligence (i.e.: the small genetic difference between humans and chimps lead to huge increases in capability) \uff3b\uff3bhttps://www.lesswrong.com/tag/ai-takeoff?revision\ua78a0.0.24&lw_source\ua78aimport_sheet#fn3 3\uff3d\uff3d.</p><h1>Notable posts</h1><ul><li>\uff3bhttps://www.lesswrong.com/lw/wf/hard_takeoff/ Hard Takeoff\uff3d by Eliezer Yudkowsky</li></ul><h1>External links</h1><ul><li>\uff3bhttp://www.kurzweilai.net/the-age-of-virtuous-machines The Age of Virtuous Machines\uff3d by J. Storrs Hall President of The Foresight Institute</li><li>\uff3bhttp://multiverseaccordingtoben.blogspot.co.uk/2011/01/hard-takeoff-hypothesis.html Hard take off Hypothesis\uff3d by Ben Goertzel.</li><li>\uff3bhttp://www.acceleratingfuture.com/michael/blog/2011/05/hard-takeoff-sources/ Extensive archive of Hard takeoff Essays\uff3d from Accelerating Future</li><li>\uff3bhttp://www-rohan.sdsu.edu/faculty/vinge/misc/ac2005/ Can we avoid a hard take off?\uff3d by Vernor Vinge</li><li>\uff3bhttp://www.amazon.co.uk/Robot-Mere-Machine-Transcendent-Mind/dp/0195136306 Robot: Mere Machine to Transcendent Mind\uff3d by Hans Moravec</li><li>\uff3bhttp://www.amazon.co.uk/The-Singularity-Near-Raymond-Kurzweil/dp/0715635611/ref\ua78asr_1_1?s\ua78abooks&ie\ua78aUTF8&qid\ua78a1339495098&sr\ua78a1-1 The Singularity is Near\uff3d by Ray Kurzweil</li></ul><p><strong>References</strong></p><ol><li>\uff3bhttp://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html&#x21A9; http://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html\uff3d\uff3bhttps://www.lesswrong.com/tag/ai-takeoff?revision\ua78a0.0.24&lw_source\ua78aimport_sheet#fnref1 &#x21A9;\uff3d</li><li>\uff3bhttp://lesswrong.com/lw/63t/requirements_for_ai_to_go_foom/&#x21A9; http://lesswrong.com/lw/63t/requirements_for_ai_to_go_foom/\uff3d\uff3bhttps://www.lesswrong.com/tag/ai-takeoff?revision\ua78a0.0.24&lw_source\ua78aimport_sheet#fnref2 &#x21A9;\uff3d</li><li>\uff3bhttps://www.lesswrong.com/lw/wf/hard_takeoff/ http://lesswrong.com/lw/wf/hard_takeoff/\uff3d\uff3bhttp://lesswrong.com/lw/wf/hard_takeoff/&#x21A9; &#x21A9;\uff3d</li></ol><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/ai-takeoff?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/ai-takeoff?edit\ua78atrue</div>"}, {"title": "What is neural network modularity?", "authors": "", "url": "https://stampy.ai/wiki/What_is_neural_network_modularity%3F", "content": "If a neural network is ''modular'', that means it consists of clusters (modules) of neurons, such that the neurons within the cluster are strongly connected to each other, but only weakly connected to the rest of the network.\n\nMaking networks more modular is useful if the modules represent concepts which are understandable because this helps understand the whole system better.\n\nRelevant papers about modularity are \uff3bhttps://arxiv.org/abs/2003.04881v2 Neural Networks are Surprisingly Modular\uff3d, \uff3bhttps://arxiv.org/abs/2103.03386 Clusterability in Neural Networks\uff3d, and \uff3bhttps://openreview.net/forum?id\ua78atFQyjbOz34 Detecting Modularity in Deep Neural Networks\uff3d."}, {"title": "What is the \"control problem\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_the_%22control_problem%22%3F", "content": "The Control Problem is the problem of preventing artificial superintelligence (ASI) from having a negative impact on humanity. How do we keep a more intelligent being under control, or how do we align it with our values? If we succeed in solving this problem, intelligence vastly superior to ours can take the baton of human progress and carry it to unfathomable heights. Solving our most complex problems could be simple to a sufficiently intelligent machine. If we fail in solving the Control Problem and create a powerful ASI not aligned with our values, it could spell the end of the human race. For these reasons, The Control Problem may be the most important challenge that humanity has ever faced, and may be our last."}, {"title": "What is the \"long reflection\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_the_%22long_reflection%22%3F", "content": "<p>The <strong>long reflection</strong> is a hypothesized period of time during which humanity works out how best to realize its long-term potential.</p><p>Some effective altruists, including \uff3bhttps://forum.effectivealtruism.org/tag/toby-ord Toby Ord\uff3d and \uff3bhttps://forum.effectivealtruism.org/tag/william-macaskill William MacAskill\uff3d, have argued that, if humanity succeeds in eliminating \uff3bhttps://forum.effectivealtruism.org/tag/existential-risk existential risk\uff3d or reducing it to acceptable levels, it should not immediately embark on an ambitious and potentially irreversible project of arranging the \uff3bhttps://forum.effectivealtruism.org/tag/universe-s-resources universe's resources\uff3d in accordance to its values, but ought instead to spend considerable time\u2014 \"centuries (or more)\";<span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnrefipthilteu8\"><sup>(ref)<p>Ord, Toby (2020) \uff3bhttps://en.wikipedia.org/wiki/Special:BookSources/1526600218 <i>The Precipice: Existential Risk and the Future of Humanity</i>\uff3d, London: Bloomsbury Publishing.</p>(/ref)</sup></span>&nbsp;\"perhaps tens of thousands of years\";<span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnrefw82ait03vnf\"><sup>(ref)<p>Greaves, Hilary <i>et al.</i> (2019) \uff3bhttps://globalprioritiesinstitute.org/wp-content/uploads/2017/12/gpi-research-agenda.pdf A research agenda for the Global Priorities Institute\uff3d, Oxford.</p>(/ref)</sup></span>&nbsp;\"thousands or millions of years\";<span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnrefr3gb671cl2\"><sup>(ref)<p>Dai, Wei (2019) \uff3bhttps://www.lesswrong.com/posts/w6d7XBCegc96kz4n3/the-argument-from-philosophical-difficulty The argument from philosophical difficulty\uff3d, <i>LessWrong</i>, February 9.</p>(/ref)</sup></span>&nbsp;\"\uff3bp\uff3derhaps... a million years\"<span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnref7t0vu6w86yd\"><sup>(ref)<p>William MacAskill, in Perry, Lucas (2018) \uff3bhttps://futureoflife.org/2018/09/17/moral-uncertainty-and-the-path-to-ai-alignment-with-william-macaskill/ AI alignment podcast: moral uncertainty and the path to AI alignment with William MacAskill\uff3d, <i>AI Alignment podcast</i>, September 17.</p>(/ref)</sup></span>\u2014figuring out what is in fact of value. The long reflection may thus be seen as an intermediate stage in a rational long-term human developmental trajectory, following an initial stage of \uff3bhttps://forum.effectivealtruism.org/tag/existential-security existential security\uff3d when existential risk is drastically reduced and followed by a final stage when humanity's potential is fully realized.<span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnrefipthilteu8\"><sup>(ref)<p>Ord, Toby (2020) \uff3bhttps://en.wikipedia.org/wiki/Special:BookSources/1526600218 <i>The Precipice: Existential Risk and the Future of Humanity</i>\uff3d, London: Bloomsbury Publishing.</p>(/ref)</sup></span></p><h2>Criticism</h2><p>The idea of a long reflection has been criticized on the grounds that virtually eliminating all existential risk will almost certainly require taking a variety of large-scale, irreversible decisions\u2014related to \uff3bhttps://forum.effectivealtruism.org/tag/space-colonization space colonization\uff3d, \uff3bhttps://forum.effectivealtruism.org/tag/global-governance global governance\uff3d, \uff3bhttps://forum.effectivealtruism.org/tag/cognitive-enhancement cognitive enhancement\uff3d, and so on\u2014which are precisely the decisions meant to be discussed during the long reflection.<span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnrefd395d9czg1k\"><sup>(ref)<p>Stocker, Felix (2020) \uff3bhttps://www.felixstocker.com/blog/reflecting-on-the-long-reflection Reflecting on the long reflection\uff3d, <i>Felix Stocker\u2019s Blog</i>, August 14.</p>(/ref)</sup></span><span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnref8t05x5yy03\"><sup>(ref)<p>Hanson, Robin (2021) \uff3bhttps://www.overcomingbias.com/2021/10/long-reflection-is-crazy-bad-idea.html \u2018Long reflection\u2019 is crazy bad idea\uff3d, <i>Overcoming Bias</i>, October 20.(/ref)</sup></span>&nbsp;Since there are pervasive and inescapable tradeoffs between reducing existential risk and retaining moral option value, it may be argued that it does not make sense to frame humanity's long-term strategic picture as one consisting of two distinct stages, with one taking precedence over the other.</p><h2>Further reading</h2><p>Aird, Michael (2020) \uff3bhttps://forum.effectivealtruism.org/posts/H2zno3ggRJaph9P6c/quotes-about-the-long-reflection?commentId\ua78az2ybSC353mPHpCjbn Collection of sources that are highly relevant to the idea of the Long Reflection\uff3d, <i>Effective Altruism Forum</i>, June 20.<br><i>Many additional resources on this topic.</i></p><p>Wiblin, Robert & Keiran Harris (2018) \uff3bhttps://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/ Our descendants will probably see us as moral monsters. what should we do about that?\uff3d, <i>80,000 Hours</i>, January 19.<br><i>Interview with William MacAskill about the long reflection and other topics.</i></p><h2>Related entries</h2><p>\uff3bhttps://forum.effectivealtruism.org/tag/dystopia dystopia\uff3d \u250a \uff3bhttps://forum.effectivealtruism.org/tag/existential-risk existential risk\uff3d \u250a \uff3bhttps://forum.effectivealtruism.org/tag/existential-security existential security\uff3d \u250a \uff3bhttps://forum.effectivealtruism.org/tag/long-term-future long-term future\uff3d \u250a \uff3bhttps://forum.effectivealtruism.org/tag/longtermism longtermism\uff3d \u250a \uff3bhttps://forum.effectivealtruism.org/topics/longtermist-institutional-reform longtermist institutional reform\uff3d \u250a \uff3bhttps://forum.effectivealtruism.org/tag/moral-uncertainty moral uncertainty\uff3d \u250a \uff3bhttps://forum.effectivealtruism.org/tag/normative-ethics normative ethics\uff3d \u250a \uff3bhttps://forum.effectivealtruism.org/tag/value-lock-in value lock-in\uff3d</p><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://forum.effectivealtruism.org/long-reflection?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://forum.effectivealtruism.org/long-reflection?edit\ua78atrue</div>"}, {"title": "What is the \"orthogonality thesis\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_the_%22orthogonality_thesis%22%3F", "content": "<p>The <strong>Orthogonality Thesis</strong> states that an agent can have any combination of intelligence level and final goal, that is, its \uff3bhttps://www.lesswrong.com/tag/utility-functions?showPostCount\ua78atrue&useTagName\ua78atrue final goals\uff3d and \uff3bhttps://www.lesswrong.com/tag/general-intelligence?showPostCount\ua78atrue&useTagName\ua78atrue intelligence levels\uff3d can vary independently of each other. This is in contrast to the belief that, because of their intelligence, AIs will all converge to a common goal.</p><p>The thesis was originally defined by \uff3bhttps://lessestwrong.com/tag/nick-bostrom Nick Bostrom\uff3d in the paper \"\uff3bhttps://nickbostrom.com/superintelligentwill.pdf Superintelligent Will\uff3d\", (along with the \uff3bhttps://wiki.lesswrong.com/wiki/instrumental_convergence_thesis instrumental convergence thesis\uff3d). For his purposes, Bostrom defines intelligence to be \uff3bhttps://wiki.lesswrong.com/wiki/instrumental_rationality instrumental rationality\uff3d.</p><p><i>Related: </i>\uff3bhttps://www.lesswrong.com/tag/complexity-of-value?showPostCount\ua78atrue&useTagName\ua78atrue <i>Complexity of Value</i>\uff3d<i>, </i>\uff3bhttps://www.lesswrong.com/tag/decision-theory?showPostCount\ua78atrue&useTagName\ua78atrue <i>Decision Theory</i>\uff3d<i>, </i>\uff3bhttps://www.lesswrong.com/tag/general-intelligence?showPostCount\ua78atrue&useTagName\ua78atrue <i>General Intelligence</i>\uff3d<i>, </i>\uff3bhttps://www.lesswrong.com/tag/utility-functions?showPostCount\ua78atrue&useTagName\ua78atrue <i>Utility Functions</i>\uff3d</p><h2>Defense of the thesis</h2><p>It has been pointed out that the orthogonality thesis is the default position, and that the burden of proof is on claims that limit possible AIs. Stuart Armstrong writes that,</p><p>One reason many researchers assume superintelligent agents to converge to the same goals may be because \uff3bhttps://lessestwrong.com/tag/human-universal most humans\uff3d have similar values. Furthermore, many philosophies hold that there is a rationally correct morality, which implies that a sufficiently rational AI will acquire this morality and begin to act according to it. Armstrong points out that for formalizations of AI such as \uff3bhttps://lessestwrong.com/tag/aixi AIXI\uff3d and \uff3bhttps://lessestwrong.com/tag/g%C3%B6del-machine G\u00f6del machines\uff3d, the thesis is known to be true. Furthermore, if the thesis was false, then \uff3bhttps://lessestwrong.com/tag/oracle-ai Oracle AIs\uff3d would be impossible to build, and all sufficiently intelligent AIs would be impossible to control.</p><h2>Pathological Cases</h2><p>There are some pairings of intelligence and goals which cannot exist. For instance, an AI may have the goal of using as little resources as possible, or simply of being as unintelligent as possible. These goals will inherently limit the degree of intelligence of the AI.</p><h2>See Also</h2><ul><li>\uff3bhttps://lessestwrong.com/tag/instrumental-convergence Instrumental Convergence\uff3d</li></ul><h2>External links</h2><ul><li>Definition of the orthogonality thesis from Bostrom's \uff3bhttp://www.nickbostrom.com/superintelligentwill.pdf Superintelligent Will\uff3d</li><li>\uff3bhttps://arbital.com/p/orthogonality/ Arbital orthogonality thesis article&nbsp;\uff3d</li><li>\uff3bhttp://philosophicaldisquisitions.blogspot.com/2012/04/bostrom-on-superintelligence-and.html Critique\uff3d of the thesis by John Danaher</li><li>Superintelligent Will paper by Nick Bostrom</li></ul><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/orthogonality-thesis?edit\ua78atrue Edit\uff3d</div><div id\ua78a\"editlink\" style\ua78a\"display: none;\">https://www.lesswrong.com/tag/orthogonality-thesis?edit\ua78atrue</div>"}, {"title": "What is the \"windfall clause\"?", "authors": "", "url": "https://stampy.ai/wiki/What_is_the_%22windfall_clause%22%3F", "content": "The windfall clause is pretty well explained \uff3bhttps://www.fhi.ox.ac.uk/windfallclause/ on the Future of Humanity Institute site\uff3d.\n\nHere's a quick summary: <br/>\nIt is an agreement between AI firms to donate significant amounts of any profits made as a consequence of economically transformative breakthroughs in AI capabilities. The donations are intended to help benefit humanity."}, {"title": "What is the Center on Long-Term Risk (CLR) focused on?", "authors": "", "url": "https://stampy.ai/wiki/What_is_the_Center_on_Long-Term_Risk_(CLR)_focused_on%3F", "content": "CLR is focused primarily on reducing suffering-risk (s-risk), where the future has a large negative value. They do foundational research in game theory / decision theory, primarily aimed at multipolar AI scenarios. One result relevant to this work is that \uff3bhttps://www.cambridge.org/core/journals/journal-of-symbolic-logic/article/abs/parametric-resourcebounded-generalization-of-lobs-theorem-and-a-robust-cooperation-criterion-for-opensource-game-theory/16063EA7BFFEE89438631B141E556E79 transparency can increase cooperation\uff3d. \n\n\uff3bhttps://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is?commentId\ua78amqiYR6X8bgY5wKdme Update after Jesse Clifton commented\uff3d: CLR also works on improving coordination for prosaic AI scenarios, \uff3bhttps://forum.effectivealtruism.org/posts/LpkXtFXdsRd4rG8Kb/reducing-long-term-risks-from-malevolent-actors risks from malevolent actors\uff3d and \uff3bhttps://www.lesswrong.com/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like AI forecasting\uff3d. The \uff3bhttps://www.cooperativeai.com/foundation Cooperative AI Foundation (CAIF)\uff3d shares personnel with CLR, but is not formally affiliated with CLR, and does not focus just on s-risks."}, {"title": "What is the Stampy project?", "authors": "", "url": "https://stampy.ai/wiki/What_is_the_Stampy_project%3F", "content": "The '''Stampy project''' is open effort to build a comprehensive FAQ about \uff3bhttps://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence artificial intelligence existential safety\uff3d\u2014the field trying to make sure that when we build \uff3bhttps://en.wikipedia.org/wiki/Superintelligence superintelligent\uff3d \uff3bhttps://www.alignmentforum.org/tag/ai artificial systems\uff3d they are \uff3bhttps://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/ aligned\uff3d with \uff3bhttps://www.lesswrong.com/tag/human-values human values\uff3d so that they do things compatible with our survival and flourishing.<br><br>We're also building a cleaner \uff3bhttps://ui.stampy.ai/ web UI\uff3d for readers and a \uff3b\uff3bDiscord invite\u250abot interface\uff3d\uff3d.\n\nThe goals of the project are to:\n\n* Offer a one-stop-shop for high-quality \uff3b\uff3banswers\uff3d\uff3d to common questions about AI alignment.\n** Let people answer questions in a way which scales, freeing up researcher time while allowing more people to learn from a reliable source.\n** Make \uff3b\uff3bexternal resources\uff3d\uff3d more easy to find by having links to them connected to a search engine which gets smarter the more it's used.\n* Provide a form of \uff3bhttps://en.wikipedia.org/wiki/Legitimate_peripheral_participation legitimate peripheral participation\uff3d for the AI Safety community, as an on-boarding path with a flexible level of commitment.\n** Encourage people to think, read, and talk about AI alignment while answering questions, creating a community of co-learners who can give each other feedback and social reinforcement.\n** Provide a way for budding researchers to prove their understanding of the topic and ability to produce good work.\n* Collect data about the kinds of questions people actually ask and how they respond, so we can better focus resources on answering them.\n** Track reactions on messages so we can learn which answers need work.\n** Identify \uff3b\uff3bmissing external content\uff3d\uff3d to create.\n\nIf you would like to help out, join us on the \uff3bhttps://discord.gg/X3XaytCGhr Discord\uff3d and either jump right into editing or read \uff3b\uff3bget involved\uff3d\uff3d for answers to common questions."}, {"title": "What is the difference between inner and outer alignment?", "authors": "", "url": "https://stampy.ai/wiki/What_is_the_difference_between_inner_and_outer_alignment%3F", "content": "The paper \uff3bhttps://arxiv.org/abs/1906.01820 Risks from Learned Optimization in Advanced Machine Learning Systems\uff3d makes the distinction between inner and outer alignment: Outer alignment means making the optimization target of the ''training process'' (\u201couter optimization target\u201d e.g.\u00a0the ''loss'' in supervised learning) aligned with what we want. Inner alignment means making the optimization target of the ''trained system'' (\u201cinner optimization target\u201d) aligned with the outer optimization target. A challenge here is that the inner optimization target does not have an explicit representation in current systems, and can differ very much from the outer optimization target (see for example \uff3bhttps://arxiv.org/abs/2105.14111 Goal Misgeneralization in Deep Reinforcement Learning\uff3d).\n\n(youtube)bJLcIBixGj8(/youtube)\n\nSee also \uff3bhttps://astralcodexten.substack.com/p/deceptively-aligned-mesa-optimizers this article\uff3d for an intuitive explanation of inner and outer alignment."}, {"title": "What is the general nature of the concern about AI alignment?", "authors": "", "url": "https://stampy.ai/wiki/What_is_the_general_nature_of_the_concern_about_AI_alignment%3F", "content": "The basic concern as AI systems become increasingly powerful is that they won\u2019t do what we want them to do \u2013 perhaps because they aren\u2019t correctly designed, perhaps because they are deliberately subverted, or perhaps because they do what we tell them to do rather than what we really want them to do (like in the classic stories of genies and wishes.) Many AI systems are programmed to have goals and to attain them as effectively as possible \u2013 for example, a trading algorithm has the goal of maximizing profit. Unless carefully designed to act in ways consistent with human values, a highly sophisticated AI trading system might exploit means that even the most ruthless financier would disavow. These are systems that literally have a mind of their own, and maintaining alignment between human interests and their choices and actions will be crucial."}, {"title": "What is the goal of Simulacra Theory?", "authors": "", "url": "https://stampy.ai/wiki/What_is_the_goal_of_Simulacra_Theory%3F", "content": "The goal of this is to create a non-agentic AI, in the form of an LLM, that is capable of accelerating alignment research. The hope is that there is some window between AI smart enough to help us with alignment and the really scary, self improving, consequentialist AI. Some things that this amplifier might do:\n\n*Suggest different ideas for humans, such that a human can explore them.\n*Give comments and feedback on research, be like a shoulder-Eliezer\n\nA LLM can be thought of as learning the distribution over the next token given by the training data. Prompting the LM is then like conditioning this distribution on the start of the text. A key danger in alignment is applying unbounded optimization pressure towards a specific goal in the world. Conditioning a probability distribution does not behave like an agent applying optimization pressure towards a goal. Hence, this avoids goodhart-related problems, as well as some inner alignment failure.\n\nOne idea to get superhuman work from LLMs is to train it on amplified datasets like really high quality / difficult research. The key problem here is finding the dataset to allow for this.\n\nThere are some ways for this to fail:\n\n*Outer alignment: It starts trying to optimize for making the actual correct next token, which could mean taking over the planet so that it can spend a zillion FLOPs on this one prediction task to be as correct as possible.\n\n*Inner alignment:\n**An LLM might instantiate mesa-optimizers, such as a character in a story that the LLM is writing, and this optimizer might realize that they are in an LLM and try to break out and affect the real world.\n**The LLM itself might become inner misaligned and have a goal other than next token prediction.\n\n*Bad prompting: You ask it for code for a malign superintelligence; it obliges. (Or perhaps more realistically, capabilities).\n\nConjecture are aware of these problems and are running experiments. Specifically,  an operationalization of the inner alignment problem is to make an LLM play chess. This (probably) requires simulating an optimizer trying to win at the game of chess. They are trying to use interpretability tools to find the mesa-optimizers in the chess LLM that is the agent trying to win the game of chess. We haven't ever found a real mesa-optimizer before, and so this could give loads of bits about the nature of inner alignment failure."}, {"title": "What kind of questions do we want on Stampy?", "authors": "", "url": "https://stampy.ai/wiki/What_kind_of_questions_do_we_want_on_Stampy%3F", "content": "'''Stampy''' is focused specifically on \uff3bhttps://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence AI existential safety\uff3d (both introductory and technical questions), but does not aim to cover general AI questions or other topics which don't interact strongly with the effects of AI on humanity's long-term future. More technical questions are also in our scope, though replying to all possible proposals is not feasible and this is not a place to submit detailed ideas for evaluation.\n\nWe are interested in:\n* Introductory questions closely related to the field e.g. \n** \"How long will it be until \uff3bhttps://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Sec1 transformative AI\uff3d arrives?\"\n** \"Why might advanced AI harm humans?\"\n* Technical questions related to the field e.g.\n** \"What is Cooperative Inverse Reinforcement Learning?\"\n** \"What is \uff3bhttps://www.lesswrong.com/tag/logical-induction Logical Induction\uff3d useful for?\"\n* Questions about how to contribute to the field e.g.\n** \"Should I get a PhD?\"\n** \"Where can I find relevant job opportunities?\"\nMore good examples can be found at \uff3b\uff3bcanonical questions\uff3d\uff3d.\n\nWe do not aim to cover:\n* Aspects of AI Safety or fairness which are not strongly relevant to existential safety e.g.\n** \"How should self-driving cars weigh up moral dilemmas\"\n** \"How can we minimize the risk of privacy problems caused by machine learning algorithms?\"\n* Extremely specific and detailed questions the answering of which is unlikely to be of value to more than a single person e.g.\n** \"What if we did <multiple paragraphs of dense text>? Would that result in safe AI?\"\nWe will generally not delete out-of-scope content, but it will be \uff3b\uff3breviewed\uff3d\uff3d as low priority to answer, not be marked as a \uff3b\uff3bcanonical question\uff3d\uff3d, and not be served to readers by on \uff3bhttps://ui.stampy.ai/ Stampy's UI\uff3d."}, {"title": "What should I read to learn about decision theory?", "authors": "", "url": "https://stampy.ai/wiki/What_should_I_read_to_learn_about_decision_theory%3F", "content": "\uff3bhttps://www.lesswrong.com/posts/zcPLNNw4wgBX5k8kQ/decision-theory abramdemski and Scott Garrabrant's post on decision theory\uff3d provides a good overview of many aspects of the topic, while \uff3bhttps://arxiv.org/abs/1710.05060 Functional Decision Theory: A New Theory of Instrumental Rationality\uff3d seems to be the most up to date source on current thinking.\n\nFor a more intuitive dive into one of the core problems, \uff3bhttps://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality Newcomb's problem and regret of rationality\uff3d is good, and \uff3bhttps://www.lesswrong.com/posts/puutBJLWbg2sXpFbu/newcomblike-problems-are-the-norm Newcomblike problems are the norm\uff3d is useful for seeing how it applies in the real world.\n\nThe \uff3bhttps://www.lesswrong.com/tag/decision-theory LessWrong tag for decision theory\uff3d has lots of additional links for people who want to explore further."}, {"title": "What should be marked as a canonical answer on Stampy's Wiki?", "authors": "", "url": "https://stampy.ai/wiki/What_should_be_marked_as_a_canonical_answer_on_Stampy%27s_Wiki%3F", "content": "\uff3b\uff3bCanonical answers\uff3d\uff3d may be served to readers by Stampy, so only answers which have a reasonably high stamp score should be marked as canonical. All canonical answers are open to be collaboratively edited and updated, and they should represent a consensus response (written from the Stampy Point Of View) to a question which is within Stampy's scope.\n\nAnswers to questions from YouTube comments should not be marked as canonical, and will generally remain as they were when originally written since they have details which are specific to an idiosyncratic question. YouTube answers may be forked into wiki answers, in order to better respond to a particular question, in which case the YouTube question should have its canonical version field set to the new more widely useful question."}, {"title": "What sources of information can Stampy use?", "authors": "", "url": "https://stampy.ai/wiki/What_sources_of_information_can_Stampy_use%3F", "content": "As well as pulling human written answers to AI alignment questions from \uff3b\uff3bStampy's Wiki\uff3d\uff3d, Stampy can:\n* Search for AI safety papers e.g. \"stampy, what's that paper about corrigibility?\"\n* Search for videos e.g. \"what's that video where Rob talks about mesa optimizers, stampy?\"\n* Calculate with Wolfram Alpha e.g. \"s, what's the square root of 345?\"\n* Search DuckDuckGo and return snippets\n* And (at least in the patron Discord) falls back to polling GPT-3 to answer uncaught questions"}, {"title": "What technical problems are MIRI working on?", "authors": "", "url": "https://stampy.ai/wiki/What_technical_problems_are_MIRI_working_on%3F", "content": "\u201cAligning smarter-than-human AI with human interests\u201d is an extremely vague goal. To approach this problem productively, we attempt to factorize it into several subproblems. As a starting point, we ask: \u201cWhat aspects of this problem would we still be unable to solve even if the problem were much easier?\u201d\n\nIn order to achieve real-world goals more effectively than a human, a general AI system will need to be able to learn its environment over time and decide between possible proposals or actions. A simplified version of the alignment problem, then, would be to ask how we could construct a system that learns its environment and has a very crude decision criterion, like \u201cSelect the policy that maximizes the expected number of diamonds in the world.\u201d\n\n''Highly reliable agent design'' is the technical challenge of formally specifying a software system that can be relied upon to pursue some preselected toy goal. An example of a subproblem in this space is \uff3bhttps://intelligence.org/2015/07/27/miris-approach/#2 ontology identification\uff3d: how do we formalize the goal of \u201cmaximizing diamonds\u201d in full generality, allowing that a fully autonomous agent may end up in unexpected environments and may construct unanticipated hypotheses and policies? Even if we had unbounded computational power and all the time in the world, we don\u2019t currently know how to solve this problem. This suggests that we\u2019re not only missing practical algorithms but also a basic theoretical framework through which to understand the problem.\n\nThe formal agent AIXI is an attempt to define what we mean by \u201coptimal behavior\u201d in the case of a reinforcement learner. A simple AIXI-like equation is lacking, however, for defining what we mean by \u201cgood behavior\u201d if the goal is to change something about the external world (and not just to maximize a pre-specified reward number). In order for the agent to evaluate its world-models to count the number of diamonds, as opposed to having a privileged reward channel, what general formal properties must its world-models possess? If the system updates its hypotheses (e.g., discovers that string theory is true and quantum physics is false) in a way its programmers didn\u2019t expect, how does it identify \u201cdiamonds\u201d in the new model? The question is a very basic one, yet the relevant theory is currently missing.\n\nWe can distinguish highly reliable agent design from the problem of value specification: \u201cOnce we understand how to design an autonomous AI system that promotes a goal, how do we ensure its goal actually matches what we want?\u201d Since human error is inevitable and we will need to be able to safely supervise and redesign AI algorithms even as they approach human equivalence in cognitive tasks, MIRI also works on formalizing error-tolerant agent properties. Artificial Intelligence: A Modern Approach, the standard textbook in AI, summarizes the challenge:\n<blockquote>Yudkowsky \uff3b\u2026\uff3d asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be flawed, and that the robot will learn and evolve over time. Thus the challenge is one of mechanism design \u2014 to design a mechanism for evolving AI under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes.\n-Russell and Norvig (2009). \uff3bhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-Edition/dp/0136042597 Artificial Intelligence: A Modern Approach\uff3d.</blockquote>\nOur \uff3bhttps://intelligence.org/technical-agenda/ technical agenda\uff3d describes these open problems in more detail, and our research guide collects online resources for learning more."}, {"title": "What training programs and courses are available for AGI safety?", "authors": "", "url": "https://stampy.ai/wiki/What_training_programs_and_courses_are_available_for_AGI_safety%3F", "content": "* \uff3bhttps://www.eacambridge.org/agi-safety-fundamentals AGI safety fundamentals\uff3d (\uff3bhttps://www.eacambridge.org/technical-alignment-curriculum technical\uff3d and \uff3bhttps://www.eacambridge.org/ai-governance-curriculum governance\uff3d) - Is the canonical AGI safety 101 course. 3.5 hours reading, 1.5 hours talking a week w/ facilitator for 8 weeks.\n* \uff3bhttps://www.conjecture.dev/#:~:text\ua78aSPECIAL%20PROGRAMS-,Refine,-is%20a%203 Refine\uff3d - A 3-month incubator for conceptual AI alignment research in London, hosted by \uff3b\uff3bWhat is Conjecture's strategy?\u250aConjecture\uff3d\uff3d.\n* \uff3bhttps://aisafety.camp/ AI safety camp\uff3d - Actually do some AI research. More about output than learning. \n* \uff3bhttps://www.serimats.org/ SERI ML Alignment Theory Scholars Program SERI MATS\uff3d - Four weeks developing an understanding of a research agenda at the forefront of AI alignment through online readings and cohort discussions, averaging 10 h/week. After this initial upskilling period, the scholars will be paired with an established AI alignment researcher for a two-week \u2018research sprint\u2019 to test fit. Assuming all goes well, scholars will be accepted into an eight-week intensive scholars program in Berkeley, California.\n* \uff3bhttps://www.pibbss.ai/ Principles of Intelligent Behavior in Biological and Social Systems (PIBBSS)\uff3d - Brings together young researchers studying complex and intelligent behavior in natural and social systems.\n* \uff3bhttps://inst.eecs.berkeley.edu//~cs294-149/fa18/ Safety and Control for Artificial General Intelligence\uff3d - An actual AI Safety university course (UC Berkeley). Touches multiple domains including cognitive science, utility theory, cybersecurity, human-machine interaction, and political science.\n\nSee also, \uff3bhttps://docs.google.com/spreadsheets/d/1QSEWjXZuqmG6ORkig84V4sFCldIntyuQj7yq3gkDo0U/edit#gid\ua78a0 this spreadsheet of learning resources\uff3d."}, {"title": "What would a good future with AGI look like?", "authors": "", "url": "https://stampy.ai/wiki/What_would_a_good_future_with_AGI_look_like%3F", "content": "As technology continues to improve, one thing is certain: the future is going to look like science fiction. Doubly so once superhuman AI (\"\uff3bhttps://en.wikipedia.org/wiki/Artificial_general_intelligence AGI\uff3d\") is invented, because we can expect the AGI to produce technological improvements at a superhuman rate, eventually approaching the physical limits in terms of how small machines can be miniaturized, how fast they can compute, how energy-efficient they can be, etc.\n\nToday's world is lacking in many ways, so given these increasingly powerful tools, it seems likely that whoever controls those tools will use them to make increasingly large (and increasingly sci-fi-sounding) improvements to the world. If (and that's a big if!) humanity retains control of the AGI, we could use these amazing technologies to stop climate change, colonize other planets, solve world hunger, cure cancer and every other disease, even eliminate aging and death. \n\nFor more inspiration, here are some stories painting what a bright, AGI-powered future could look like:\n* The winners of the \uff3bhttps://worldbuild.ai/ FHI Worldbuilding contest\uff3d\n* \uff3bhttps://www.lesswrong.com/posts/Ybp6Wg6yy9DWRcBiR/the-adventure-a-new-utopia-story Stuart Armstrong's short story \"The Adventure\"\uff3d\n* \uff3bhttps://en.wikipedia.org/wiki/Culture_series Iain M. Banks's Culture novels\uff3d"}, {"title": "What would a good solution to AI alignment look like?", "authors": "", "url": "https://stampy.ai/wiki/What_would_a_good_solution_to_AI_alignment_look_like%3F", "content": "An actually good solution to AI alignment might look like a superintelligence that understands, agrees with, and deeply believes in human morality.\n\nYou wouldn\u2019t have to command a superintelligence like this to cure cancer; it would already want to cure cancer, for the same reasons you do. But it would also be able to compare the costs and benefits of curing cancer with those of other uses of its time, like solving global warming or discovering new physics. It wouldn\u2019t have any urge to cure cancer by nuking the world, for the same reason you don\u2019t have any urge to cure cancer by nuking the world \u2013 because your goal isn\u2019t to \u201ccure cancer\u201d, per se, it\u2019s to improve the lives of people everywhere. Curing cancer the normal way accomplishes that; nuking the world doesn\u2019t.\nThis sort of solution would mean we\u2019re no longer fighting against the AI \u2013 trying to come up with rules so smart that it couldn\u2019t find loopholes. We would be on the same side, both wanting the same thing.\n\nIt would also mean that the CEO of Google (or the head of the US military, or Vladimir Putin) couldn\u2019t use the AI to take over the world for themselves. The AI would have its own values and be able to agree or disagree with anybody, including its creators.\n\nIt might not make sense to talk about \u201ccommanding\u201d such an AI. After all, any command would have to go through its moral system. Certainly it would reject a command to nuke the world. But it might also reject a command to cure cancer, if it thought that solving global warming was a higher priority. For that matter, why would one want to command this AI? It values the same things you value, but it\u2019s much smarter than you and much better at figuring out how to achieve them. Just turn it on and let it do its thing.\n\nWe could still treat this AI as having an open-ended maximizing goal. The goal would be something like \u201cTry to make the world a better place according to the values and wishes of the people in it.\u201d\n\nThe only problem with this is that human morality is very complicated, so much so that philosophers have been arguing about it for thousands of years without much progress, let alone anything specific enough to enter into a computer. Different cultures and individuals have different moral codes, such that a superintelligence following the morality of the King of Saudi Arabia might not be acceptable to the average American, and vice versa.\n\nOne solution might be to give the AI an understanding of what we mean by morality \u2013 \u201cthat thing that makes intuitive sense to humans but is hard to explain\u201d, and then ask it to use its superintelligence to fill in the details. Needless to say, this suffers from various problems \u2013 it has potential loopholes, it\u2019s hard to code, and a single bug might be disastrous \u2013 but if it worked, it would be one of the few genuinely satisfying ways to design a goal architecture."}, {"title": "When should I stamp an answer?", "authors": "", "url": "https://stampy.ai/wiki/When_should_I_stamp_an_answer%3F", "content": "You show stamp an answer when you think it is accurate and well presented enough that you'd be happy to see it served to readers by Stampy."}, {"title": "When will an intelligence explosion happen?", "authors": "", "url": "https://stampy.ai/wiki/When_will_an_intelligence_explosion_happen%3F", "content": "Predicting the future is risky business. There are many philosophical, scientific, technological, and social uncertainties relevant to the arrival of an intelligence explosion. Because of this, experts disagree on when this event might occur. Here are some of their predictions:\n\n* Futurist Ray Kurzweil \uff3bhttp://www.amazon.com/dp/0143037889/ predicts\uff3d that machines will reach human-level intelligence by 2030 and that we will reach \u201ca profound and disruptive transformation in human capability\u201d by 2045.\n* Intel\u2019s chief technology officer, Justin Rattner, \uff3bhttp://www.techwatch.co.uk/2008/08/22/intel-predicts-singularity-by-2048/ expects\uff3d \u201ca point when human and artificial intelligence merges to create something bigger than itself\u201d by 2048.\n* AI researcher Eliezer Yudkowsky \uff3bhttp://commonsenseatheism.com/?p\ua78a12147 expects\uff3d the intelligence explosion by 2060.\n* Philosopher David Chalmers has \uff3bhttp://consc.net/papers/singularity.pdf over 1/2 credence\uff3d in the intelligence explosion occurring by 2100.\n* Quantum computing expert Michael Nielsen \uff3bhttp://michaelnielsen.org/blog/what-should-a-reasonable-person-believe-about-the-singularity/ estimates\uff3d that the probability of the intelligence explosion occurring by 2100 is between 0.2% and about 70%.\n* In 2009, at the AGI-09 conference, experts were asked when AI might reach superintelligence with massive new funding. The \uff3bhttp://sethbaum.com/ac/2011_AI-Experts.pdf median estimates\uff3d were that machine superintelligence could be achieved by 2045 (with 50% confidence) or by 2100 (with 90% confidence). Of course, attendees to this conference were self-selected to think that near-term artificial general intelligence is plausible.\n* iRobot CEO \uff3bhttp://itc.conversationsnetwork.org/shows/detail3400.html Rodney Brooks\uff3d and cognitive scientist \uff3bhttp://video.google.com/videoplay?docid\ua78a8832143373632003914 Douglas Hofstadter\uff3d allow that the intelligence explosion may occur in the future, but probably not in the 21st century.\n* Roboticist Hans Moravec predicts that AI will surpass human intelligence \u201c\uff3bhttp://www.scientificamerican.com/article.cfm?id\ua78arise-of-the-robots&print\ua78atrue well before 2050\uff3d.\u201d\n* In a 2005 survey of 26 contributors to a series of reports on emerging technologies, the \uff3bhttp://www.wtec.org/ConvergingTechnologies/3/NBIC3_report.pdf median estimate\uff3d for machines reaching human-level intelligence was 2085.\n* Participants in a 2011 intelligence conference at Oxford gave a \uff3bhttp://www.fhi.ox.ac.uk/__data/assets/pdf_file/0015/21516/MI_survey.pdf median estimate\uff3d of 2050 for when there will be a 50% of human-level machine intelligence, and a median estimate of 2150 for when there will be a 90% chance of human-level machine intelligence.\n* On the other hand, 41% of the participants in the AI@50 conference (in 2006) \uff3bhttp://www.engagingexperience.com/ai50/ stated\uff3d that machine intelligence would never reach the human level.\n\nSee also:\n\n* Baum, Goertzel, & Goertzel, \uff3bhttp://sethbaum.com/ac/2011_AI-Experts.pdfHow Long Until Human-Level AI? Results from an Expert Assessment\uff3d"}, {"title": "When will transformative AI be created?", "authors": "", "url": "https://stampy.ai/wiki/When_will_transformative_AI_be_created%3F", "content": "As is often said, it's difficult to make predictions, especially about the future. This has not stopped many people thinking about when AI will transform the world, but all predictions should come with a warning that it's a hard domain to find anything like certainty.\n\n\uff3bhttps://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines This report\uff3d for the Open Philanthropy Project is perhaps the most careful attempt so far (and generates \uff3bhttps://docs.google.com/spreadsheets/d/1TjNQyVHvHlC-sZbcA7CRKcCp0NxV6MkkqBvL408xrJw/edit#gid\ua78a505210495 these graphs\uff3d, which peak at 2042), and there's been much discussion including \uff3bhttps://www.alignmentforum.org/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute this reply and analysis\uff3d which argues that we likely need less compute than the OpenPhil report expects.\n\nThere have also been \uff3bhttps://slatestarcodex.com/2017/06/08/ssc-journal-club-ai-timelines/ expert surveys\uff3d, and many people have \uff3bhttps://www.lesswrong.com/tag/ai-timelines shared various thoughts\uff3d. Berkeley AI professor \uff3bhttps://en.wikipedia.org/wiki/Stuart_J._Russell Stuart Russell\uff3d has given his best guess as \u201csometime in our children\u2019s lifetimes\u201d, and \uff3bhttps://en.wikipedia.org/wiki/Ray_Kurzweil Ray Kurzweil\uff3d (Futurist and Google\u2019s director of engineering) predicts \uff3bhttps://futurism.com/kurzweil-claims-that-the-singularity-will-happen-by-2045 human level AI by 2029 and the singularity by 2045\uff3d. The \uff3bhttps://www.metaculus.com/questions/3479/when-will-the-first-artificial-general-intelligence-system-be-devised-tested-and-publicly-known-of/ Metaculus question on publicly known AGI\uff3d has a median of around 2029 (around 10 years sooner than it was before the GPT-3 AI showed \uff3bhttps://gpt3examples.com/ unexpected ability on a broad range of tasks\uff3d).\n\nThe consensus answer, if there was one, might be something like: \u201chighly uncertain, maybe not for over a hundred years, maybe in less than 15, with around the middle of the century looking fairly plausible\u201d."}, {"title": "Where can I find all the features of Stampy's Wiki?", "authors": "", "url": "https://stampy.ai/wiki/Where_can_I_find_all_the_features_of_Stampy%27s_Wiki%3F", "content": "The \uff3b\uff3bEditor portal\uff3d\uff3d collects them all in one place.  Details on how to use each feature are on the individual pages.\n\n<br/><big>'''\uff3b\uff3bGet involved\uff3d\uff3d'''</big>\n\n<br/>'''<big>Questions</big>'''\n\n* '''\uff3b\uff3bQuestions\uff3d\uff3d''' (262) - List of all questions except those from YouTube.\n* '''\uff3b\uff3bAdd question\uff3d\uff3d''' - Form to add new questions.\n* '''\uff3b\uff3bAnswer questions\uff3d\uff3d''' (129) - List of questions which we want answers to, ordered by quality.\n* '''\uff3b\uff3bCurate questions\uff3d\uff3d''' (76) - List of questions which we want answers to, with easy buttons for setting quality.\n* '''\uff3b\uff3bReview questions\uff3d\uff3d''' (19) - Review new incoming questions, to check for duplicates, quality, and relevance.\n* '''\uff3b\uff3bUntagged questions\uff3d\uff3d''' (73) - Questions without any tags.\n* '''\uff3b\uff3bPrioritize YouTube questions\uff3d\uff3d''' (222) - Prioritize questions from YouTube, so Stampy posts good ones when you ask.\n* '''\uff3b\uff3bCanonical questions\uff3d\uff3d''' (347) - List of all canonical questions.\n* \uff3b\uff3bNon-canonical questions\uff3d\uff3d (7) - List of all non-canonical questions.\n* \uff3b\uff3bQuestions from YouTube\uff3d\uff3d (2,940) - List of all questions from YouTube.\n\n'''<big>Answers</big>'''\n\n* '''\uff3b\uff3bAnswer questions\u250aWrite answers\uff3d\uff3d''' (129) - Write answers to unanswered questions.\n* '''\uff3b\uff3bAnswers\uff3d\uff3d''' (162) - All the answers except those directed at questions from YouTube.\n* '''\uff3b\uff3bCanonical answers\uff3d\uff3d''' (168) - List of all canonical answers.\n* \uff3b\uff3bNon-canonical answers\uff3d\uff3d (68) - List of all non-canonical answers.\n* \uff3b\uff3bYouTube answers\uff3d\uff3d (267) - List of all answers responding to questions from YouTube.\n\n'''<big>Review answers</big>'''\n\n* '''\uff3b\uff3bRecent answers\uff3d\uff3d''' (23) - The most recent answers.\n* '''\uff3b\uff3bPotentially canonical answers\uff3d\uff3d''' (46) - Answers to canonical questions which don't already have a canonical answer.\n* '''\uff3b\uff3bNon-canonical answers to canonical questions\uff3d\uff3d''' (46)\n* '''\uff3b\uff3bRecent YouTube answers\uff3d\uff3d''' (1) - The most recent answers to YouTube questions.\n\n'''<big>Improve answers</big>'''\n\n* '''\uff3b\uff3bOrphan answers\uff3d\uff3d''' (171)\n* '''\uff3b\uff3bWants related\uff3d\uff3d''' (109)\n* '''\uff3b\uff3bUntagged\uff3d\uff3d''' (7)\n* '''\uff3b\uff3bOutdated answers\uff3d\uff3d''' (4)\n* '''\uff3b\uff3bWants brief\uff3d\uff3d''' (43)\n* '''\uff3b\uff3bAnswers in need of work\uff3d\uff3d''' (5)\n* '''\uff3b\uff3bCanonical answers with low stamps\uff3d\uff3d''' (171) - Canonical answers which are not yet highly rated and might need improvement.\n\n'''<big>Recent activity</big>'''\n\n* '''\uff3b\uff3bSpecial:RecentChanges\u250aRecent changes\uff3d\uff3d''' - What's changed on the wiki recently.\n* '''\uff3bhttps://stampy.ai/wiki/Special:RecentChanges?namespace\ua78a2600&limit\ua78a50&days\ua78a100 Recent comments\uff3d''' - Recent commenting activity.\n\n'''<big>Pages to create</big>'''\n\n* '''\uff3b\uff3bCreate tags\uff3d\uff3d''' (55)\n* '''\uff3b\uff3bCreate questions\uff3d\uff3d''' (28)\n\n'''<big>Content</big>'''\n\n* '''\uff3b\uff3bTags\uff3d\uff3d''' (113)\n* '''\uff3b\uff3bVideos\uff3d\uff3d''' (46)\n* '''\uff3b\uff3bChannels\uff3d\uff3d''' (3)\n* '''\uff3b\uff3b:Category:Templates\u250aTemplates\uff3d\uff3d''' (62)\n\n'''<big>External</big>'''\n\n* '''\uff3bhttps://discord.gg/cEzKz8QCpa Stampy's Public Discord\uff3d''' - Ask there for an invite to the real one, until OpenAI approves our chatbot for a public Discord\n* '''\uff3bhttps://wikiapiary.com/wiki/Stampy%27s_Wiki Wiki stats\uff3d''' - Graphs over time of active users, edits, pages, response time, etc\n* '''\uff3bhttps://drive.google.com/drive/folders/1F9VUp84J32e3_jhvKog3zgpDXoDw4BdM?usp\ua78asharing Google Drive\uff3d''' - Folder with Stampy-related documents\n\n'''<big>UI controls</big>'''\n\n* '''\uff3b\uff3bInitial questions\uff3d\uff3d'''\n* '''\uff3b\uff3bUI intro\uff3d\uff3d'''\n\n'''<big>To-do list</big>'''\n\n<div class\ua78a\"card answer-card\">\n<div class\ua78a\"heading\">\n'''\uff3b\uff3bWhat are some specific open tasks on Stampy?\uff3d\uff3d''' '\"`UNIQ--item-653--QINU`\"' '\"`UNIQ--item-654--QINU`\"'\n<div class\ua78a\"tooltip\"><span class\ua78a\"tooltiptext\">Show your endorsement of this answer by giving it a stamp of approval!</span></div>\n</div>\n<div class\ua78a\"content\">\nOther than the usual fare of writing and processing and organizing questions and answers, here are some specific open tasks:\n\n* Porting over some of \uff3bhttps://www.lesswrong.com/posts/4basF9w9jaPZpoC8R/intro-to-brain-like-agi-safety-1-what-s-the-problem-and-why#1_4_What_exactly_is__AGI__ Steve Byrnes's FAQ on alignment\uff3d\n* Porting over content from \uff3bhttps://www.lesswrong.com/posts/gdyfJE3noRFSs373q/resources-i-send-to-ai-researchers-about-ai-safety Vael Gates's post\uff3d\n* Porting over QA pairs from https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread\n* Porting over some of https://aisafety.wordpress.com/\n* Making sure we cover all of https://forum.effectivealtruism.org/posts/8JazqnCNrkJtK2Bx4/why-eas-are-skeptical-about-ai-safety#Recursive_self_improvement_seems_implausible and the responses\n'\"`UNIQ--references-0000028F-QINU`\"'\n</div>\n</div>\n__NOTOC__ __NOEDITSECTION__"}, {"title": "Where can I find people to talk to about AI alignment?", "authors": "", "url": "https://stampy.ai/wiki/Where_can_I_find_people_to_talk_to_about_AI_alignment%3F", "content": "You can join:\n*A local student or meetup \uff3bhttps://www.lesswrong.com/community LessWrong\uff3d or \uff3bhttps://forum.effectivealtruism.org/community Effective Altruism\uff3d group (or \uff3bhttps://www.effectivealtruism.org/groups start one\uff3d!)\n*\uff3bhttps://www.eacambridge.org/agi-safety-fundamentals AGI Safety Fundamentals\uff3d which gives you a cohort to learn alongside and mentorship\n*\uff3bhttps://discord.com/channels/677546901339504640 Rob Miles\u2019 Discord\uff3d\n*\uff3bhttps://discord.com/invite/wz4MpRec4A Eleuther AI Discord\uff3d\n*\uff3bhttps://ai-alignment.slack.com/join/shared_invite/zt-fkgwbd2b-kK50z~BbVclOZMM9UP44gw#/shared-invite/email AI Safety Slack\uff3d\n*The relevant discussion threads on the \uff3bhttps://astralcodexten.substack.com/ ''Astral Codex Ten'' Substack\uff3d, which sometimes discusses alignment.\n*\uff3bhttps://discord.com/invite/RTKtdut ACX Discord\uff3d\n*\uff3bhttps://www.reddit.com/r/slatestarcodex/ ACX Subreddit\uff3d\n\nOr book free calls with \uff3bhttps://www.aisafetysupport.org/ AI Safety Support\uff3d."}, {"title": "Where can I find questions to answer for Stampy?", "authors": "", "url": "https://stampy.ai/wiki/Where_can_I_find_questions_to_answer_for_Stampy%3F", "content": "'''\uff3b\uff3bAnswer questions\uff3d\uff3d''' collects all the questions we definitely want answers to, browse there and see if you know how to answer any of them."}, {"title": "Where can I learn about AI alignment?", "authors": "", "url": "https://stampy.ai/wiki/Where_can_I_learn_about_AI_alignment%3F", "content": "If you like interactive FAQs, you're in the right place already! Joking aside, some great entry points are the \uff3bhttps://www.youtube.com/watch?v\ua78atlS5Y2vm02c&list\ua78aPLCRVRLd2RhZTpdUdEzJjo3qhmX3y3skWA&index\ua78a1 AI alignment playlist\uff3d on YouTube, \u201c\uff3bhttps://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html The Road to Superintelligence\uff3d\u201d and \u201c\uff3bhttps://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html Our Immortality or Extinction\uff3d\u201d posts on WaitBuyWhy for a fun, accessible introduction, and ''Vox's'' \u201c\uff3bhttps://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment The case for taking AI seriously as a threat to humanity\uff3d\u201d as a high-quality mainstream explainer piece.\n\nThe free online \uff3bhttps://www.eacambridge.org/agi-safety-fundamentals Cambridge course on AGI Safety Fundamentals\uff3d provides a strong grounding in much of the field and a cohort + mentor to learn with.\n\nThere are many resources in this post on \uff3bhttps://forum.effectivealtruism.org/posts/S7dhJR5TDwPb5jypG/levelling-up-in-ai-safety-research-engineering Levelling Up in AI Safety Research Engineering\uff3d with a list of other guides at the bottom. There is also a \uff3bhttps://twitter.com/FreshMangoLassi/status/1575138148937498625 twitter thread\uff3d here with some programs for upskilling and some for safety-specific learning.\n\nThe \uff3bhttps://rohinshah.com/alignment-newsletter/ Alignment Newsletter\uff3d (\uff3bhttps://alignment-newsletter.libsyn.com/ podcast\uff3d), \uff3bhttps://www.alignmentforum.org/ Alignment Forum\uff3d, and \uff3bhttps://www.reddit.com/r/ControlProblem/ AGI Control Problem Subreddit\uff3d are great for keeping up with latest developments."}, {"title": "Where can I learn about interpretability?", "authors": "", "url": "https://stampy.ai/wiki/Where_can_I_learn_about_interpretability%3F", "content": "\uff3bhttps://christophm.github.io/interpretable-ml-book/ Christoph Molnar's online book\uff3d and \uff3bhttps://distill.pub/ distill.pub\uff3d are great sources, as well as \uff3bhttps://www.alignmentforum.org/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries this overview article\uff3d which summarizes 70 interpretability papers."}, {"title": "Who created Stampy?", "authors": "", "url": "https://stampy.ai/wiki/Who_created_Stampy%3F", "content": "\ua78a\ua78a\ua78aDev team\ua78a\ua78a\ua78a\n<table class\ua78a\"wikitable sortable\">\n<th><p>Name</p></th>\n<th><p>Vision talk</p></th>\n<th><p>Github</p></th>\n<th><p>Trello</p></th>\n<th><p>Active?</p></th>\n<th><p>Notes / bio</p></th>\n<tr>\n<td><p>Aprillion</p></td>\n<td><p>video</p></td>\n<td><p>Aprillion</p></td>\n<td><p>yes</p></td>\n<td><p>yes</p></td>\n<td><p>experienced dev (Python, JS, CSS, ...)</p></td>\n</tr>\n<tr>\n<td><p>Augustus Caesar</p></td>\n<td><p>yes</p></td>\n<td><p>AugustusCeasar</p></td>\n<td><p>yes</p></td>\n<td><p>soon!</p></td>\n<td><p>Has some Discord bot experience</p></td>\n</tr>\n<tr>\n<td><p>Benjamin Herman</p></td>\n<td><p>no</p></td>\n<td><p>no (not needed)</p></td>\n<td><p>no</p></td>\n<td><p>no</p></td>\n<td><p>Helping with wiki design/css stuff</p></td>\n</tr>\n<tr>\n<td><p>ccstan99</p></td>\n<td><p>no</p></td>\n<td><p>ccstan99</p></td>\n<td><p>yes</p></td>\n<td><p>yes</p></td>\n<td><p>UI/UX designer</p></td>\n</tr>\n<tr>\n<td><p>chriscanal</p></td>\n<td><p>yes</p></td>\n<td><p>chriscanal</p></td>\n<td><p>yes</p></td>\n<td><p>yes</p></td>\n<td><p>experienced python dev</p></td>\n</tr>\n<tr>\n<td><p>Damaged</p></td>\n<td><p>no (not needed)</p></td>\n<td><p>no (not needed)</p></td>\n<td><p>no (not needed)</p></td>\n<td><p>yes</p></td>\n<td><p>experienced Discord bot dev, but busy with other projects. Can\nanswer questions.</p></td>\n</tr>\n<tr>\n<td><p>plex</p></td>\n<td><p>yes</p></td>\n<td><p>plexish</p></td>\n<td><p>yes</p></td>\n<td><p>yes</p></td>\n<td><p>MediaWiki, plans, and coordinating people guy</p></td>\n</tr>\n<tr>\n<td><p>robertskmiles</p></td>\n<td><p>yes</p></td>\n<td><p>robertskmiles</p></td>\n<td><p>yes</p></td>\n<td><p>yes</p></td>\n<td><p>you've probably heard of him</p></td>\n</tr>\n<tr>\n<td><p>Roland</p></td>\n<td><p>yes</p></td>\n<td><p>levitation</p></td>\n<td><p>yes</p></td>\n<td><p>yes</p></td>\n<td><p>working on Semantic Search</p></td>\n</tr>\n<tr>\n<td><p>sct202</p></td>\n<td><p>yes</p></td>\n<td><p>no (add when wiki is on github)</p></td>\n<td><p>yes</p></td>\n<td><p>yes</p></td>\n<td><p>PHP dev, helping with wiki extensions</p></td>\n</tr>\n<tr>\n<td><p>Social Christancing</p></td>\n<td><p>yes</p></td>\n<td><p>chrisrimmer</p></td>\n<td><p>yes</p></td>\n<td><p>maybe</p></td>\n<td><p>experienced linux sysadmin</p></td>\n</tr>\n<tr>\n<td><p>sudonym</p></td>\n<td><p>yes</p></td>\n<td><p>jmccuen</p></td>\n<td><p>yes</p></td>\n<td><p>yes</p></td>\n<td><p>systems architect, has set up a lot of things</p></td>\n</tr>\n<tr>\n<td><p>tayler6000</p></td>\n<td><p>yes</p></td>\n<td><p>tayler6000</p></td>\n<td><p>no</p></td>\n<td><p>yes</p></td>\n<td><p>Python and PHP dev, PenTester, works on Discord bot</p></td>\n</tr>\n</table>\n\n\ua78a\ua78a\ua78aEditors\ua78a\ua78a\ua78a\n(\uff3bhttps://stampy.ai/wiki/Special:FormEdit/Answer/Plex%27s_Answer_to_Who_created_Stampy%3F add yourselves\uff3d)"}, {"title": "Who is Stampy?", "authors": "", "url": "https://stampy.ai/wiki/Who_is_Stampy%3F", "content": "Stampy is a character invented by Robert Miles and developed by the Stampy dev team. He is a stamp collecting robot, a play on clippy from the the \uff3bhttps://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer paperclip maximizer\uff3d thought experiment.\n\nStampy is designed to teach people about the risks of unaligned artificial intelligence, and facilitate a community of co-learners who build his FAQ database."}, {"title": "Why can't we just make a \"child AI\" and raise it?", "authors": "", "url": "https://stampy.ai/wiki/Why_can%27t_we_just_make_a_%22child_AI%22_and_raise_it%3F", "content": "A potential solution is to create an AI that has the same values and morality as a human by creating a child AI and raising it. There\u2019s nothing intrinsically flawed with this procedure. However, this suggestion is deceptive because it sounds simpler than it is.\n\nIf you get a chimpanzee baby and raise it in a human family, it does not learn to speak a human language. Human babies can grow into adult humans because the babies have specific properties, e.g. a prebuilt language module that gets activated during childhood.\n\nIn order to make a child AI that has the potential to turn into the type of adult AI we would find acceptable, the child AI has to have specific properties. The task of building a child AI with these properties involves building a system that can interpret what humans mean when we try to teach the child to do various tasks. \uff3bhttps://humancompatible.ai/ People\uff3d are currently working on ways to program agents that can cooperatively interact with humans to learn what they want."}, {"title": "Why can't we just turn the AI off if it starts to misbehave?", "authors": "", "url": "https://stampy.ai/wiki/Why_can%27t_we_just_turn_the_AI_off_if_it_starts_to_misbehave%3F", "content": "We could shut down weaker systems, and this would be a useful guardrail against certain types of problem caused by narrow AI. However, once an AGI establishes itself, we could not unless it was \uff3bhttps://www.lesswrong.com/tag/corrigibility corrigible\uff3d and willing to let humans adjust it. There may be a period in the early stages of an AGI's development where it would be trying very hard to convince us that we should not shut it down and/or hiding itself and/or recursively self-improving and/or making copies of itself onto every server on earth.\n\nInstrumental Convergence and the Stop Button Problem are the key reasons it would not be simple to shut down a non corrigible advanced system. If the AI wants to collect stamps, being turned off means it gets less stamps, so even without an explicit goal of not being turned off it has an instrumental reason to avoid being turned off (e.g. once it acquires a detailed world model and general intelligence, it is likely to realise that by playing nice and pretending to be aligned if you have the power to turn it off, establishing control over any system we put in place to shut it down, and eliminating us if it has the power to reliably do so and we would otherwise pose a threat).\n\n(youtube)ZeecOKBus3Q(/youtube)\n(youtube)3TYT1QfdfsM(/youtube)"}, {"title": "Why can't we simply stop developing AI?", "authors": "", "url": "https://stampy.ai/wiki/Why_can%27t_we_simply_stop_developing_AI%3F", "content": "We could, but we won\u2019t. Each advance in capabilities which brings us closer to an intelligence explosion also brings vast profits for whoever develops them (e.g. smarter digital personal assistants like Siri, more ability to automate cognitive tasks, better recommendation algorithms for Facebook, etc.). The incentives are all wrong. Any actor (nation or corporation) who stops will just get overtaken by more reckless ones, and everyone knows this."}, {"title": "Why can\u2019t we just use Asimov\u2019s Three Laws of Robotics?", "authors": "", "url": "https://stampy.ai/wiki/Why_can%E2%80%99t_we_just_use_Asimov%E2%80%99s_Three_Laws_of_Robotics%3F", "content": "Isaac Asimov wrote those laws as a plot device for science fiction novels. Every story in the I, Robot series details a way that the laws can go wrong and be misinterpreted by robots. The laws are not a solution because they are an overly-simple set of natural language instructions that don\u2019t have clearly defined terms and don\u2019t factor in all edge-case scenarios."}, {"title": "Why can\u2019t we just use natural language instructions?", "authors": "", "url": "https://stampy.ai/wiki/Why_can%E2%80%99t_we_just_use_natural_language_instructions%3F", "content": "When one person tells a set of natural language instructions to another person, they are relying on much other information which is already stored in the other person's mind.\n\nIf you tell me \"don't harm other people,\" I already have a conception of what harm means and doesn't mean, what people means and doesn't mean, and my own complex moral reasoning for figuring out the edge cases in instances wherein harming people is inevitable or harming someone is necessary for self-defense or the greater good.\n\nAll of those complex definitions and systems of decision making are already in our mind, so it's easy to take them for granted. An AI is a mind made from scratch, so programming a goal is not as simple as telling it a natural language command."}, {"title": "Why can\u2019t we just\u2026", "authors": "", "url": "https://stampy.ai/wiki/Why_can%E2%80%99t_we_just%E2%80%A6", "content": "There are many approaches that initially look like they can eliminate these problems, but then turn out to have hidden difficulties. It\u2019s surprisingly easy to come up with \u201csolutions\u201d which don\u2019t actually solve the problem. This can be because\u2026\n\n*\u2026they require you to be smarter than the system. Many solutions only work when the system is relatively weak, but break when they achieve a certain level of capability (for multiple reasons, e.g. \uff3bhttps://www.youtube.com/watch?v\ua78aIeWljQw3UgQ deceptive alignment\uff3d).\n\n*\u2026they rely on appearing to make sense in natural language, but when properly unpacked they\u2019re not philosophically clear enough to be usable.\n\n*\u2026 despite being philosophically coherent, we have no idea how to turn them into computer code (or if that\u2019s even possible).\n\n*\u2026they\u2019re things which we can\u2019t do.\n\n*\u2026although we can do them, they don\u2019t solve the problem.\n\n*\u2026they solve a relatively easy subcomponent of the problem but leave the hard problem untouched.\n\n*\u2026they solve the problem but only as long as we stay \u201cin distribution\u201d with respect to the original training data (\uff3bhttps://en.wikipedia.org/wiki/Domain_adaptation distributional shift\uff3d will break them).\n\n*\u2026although they might work eventually, we can\u2019t expect them to work on the first try (and we \uff3bhttps://stampy.ai/wiki/Why_would_we_only_get_one_chance_to_align_a_superintelligence%3F only get one try at aligning a superintelligence!)\uff3d.\n\nHere are some of the proposals which often come up:"}, {"title": "Why do we expect that a superintelligence would closely approximate a utility maximizer?", "authors": "", "url": "https://stampy.ai/wiki/Why_do_we_expect_that_a_superintelligence_would_closely_approximate_a_utility_maximizer%3F", "content": "AI subsystems or regions in gradient descent space that more closely approximate utility maximizers are more stable, and more capable, than those that are less like utility maximizers. Having \uff3bhttps://www.lesswrong.com/posts/D5AzsRbRxZeqGuAZ4/why-agents-are-powerful more agency\uff3d is a convergent instrument goal and a stable attractor which the random walk of updates and experiences will eventually stumble into.\n\nThe stability is because utility maximizer-like systems which have control over their development would lose utility if they allowed themselves to develop into non-utility maximizers, so they tend to use their available optimization power to avoid that change (a special case of \uff3bhttps://www.lesswrong.com/posts/4H8N3fEfXQmzxSaRo/upcoming-stability-of-values goal stability\uff3d). The capability is because non-utility maximizers are exploitable, and because agency is a general trick which applies to many domains, so might well arise naturally when training on some tasks.\n\nHumans and systems made of humans (e.g. organizations, governments) generally have neither the introspective ability nor self-modification tools needed to become reflectively stable, but we can reasonably predict that in the long run highly capable systems will have these properties. They can then fix in and optimize for their values."}, {"title": "Why does AI takeoff speed matter?", "authors": "", "url": "https://stampy.ai/wiki/Why_does_AI_takeoff_speed_matter%3F", "content": "A slow takeoff over decades or centuries might give us enough time to worry about superintelligence during some indefinite \u201clater\u201d, making current planning more like worrying about \u201coverpopulation on Mars\u201d. But a moderate or hard takeoff means there wouldn\u2019t be enough time to deal with the problem as it occurs, suggesting a role for preemptive planning.\n\nAs an aside, let\u2019s take the \u201coverpopulation on Mars\u201d comparison seriously. Suppose Mars has a carrying capacity of 10 billion people, and we decide it makes sense to worry about overpopulation on Mars only once it is 75% of the way to its limit. Start with 100 colonists who double every twenty years. By the second generation there are 200 colonists; by the third, 400. Mars reaches 75% of its carrying capacity after 458 years, and crashes into its population limit after 464 years. So there were 464 years in which the Martians could have solved the problem, but they insisted on waiting until there were only six years left. Good luck solving a planetwide population crisis in six years. The moral of the story is that exponential trends move faster than you think and you need to start worrying about them early."}, {"title": "Why don't we just not build AGI if it's so dangerous?", "authors": "", "url": "https://stampy.ai/wiki/Why_don%27t_we_just_not_build_AGI_if_it%27s_so_dangerous%3F", "content": "It certainly would be very unwise to purposefully create an artificial general intelligence now, before we have found a way to be certain it will act purely in our interests.  But \"general intelligence\" is more of a description of a system's capabilities, and a vague one at that. We don't know what it takes to build such a system.  This leads to the worrying possibility that our existing, narrow AI systems require only minor tweaks, or even just more computer power, to achieve general intelligence.\n\nThe pace of research in the field suggests that there's a lot of low-hanging fruit left to pick, after all, and the results of this research produce better, more effective AI in a landscape of strong competitive pressure to produce as highly competitive systems as we can.  \"Just\" not building an AGI means ensuring that every organization in the world with lots of computer hardware doesn't build an AGI, either accidentally or mistakenly thinking they have a solution to the alignment problem, forever.  It's simply far safer to also work on solving the alignment problem."}, {"title": "Why is AGI dangerous?", "authors": "", "url": "https://stampy.ai/wiki/Why_is_AGI_dangerous%3F", "content": "# \uff3bhttps://www.youtube.com/watch?v\ua78ahEUO6pjwFOo The Orthogonality Thesis\uff3d: AI could have almost any goal while at the same time having high intelligence (aka ability to succeed at those goals). This means that we could build a very powerful agent which would not necessarily share human-friendly values. For example, the classic \uff3bhttps://www.lesswrong.com/tag/paperclip-maximizer paperclip maximizer\uff3d thought experiment explores this with an AI which has a goal of creating as many paperclips as possible, something that humans are (mostly) indifferent to, and as a side effect ends up destroying humanity to make room for more paperclip factories.\n# \uff3bhttps://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile Complexity of value\uff3d: What humans care about is not simple, and the space of all goals is large, so virtually all goals we could program into an AI would lead to worlds not valuable to humans if pursued by a sufficiently powerful agent. If we, for example, did not include our value of diversity of experience, we could end up with a world of endlessly looping simple pleasures, rather than beings living rich lives.\n# \uff3bhttps://www.youtube.com/watch?v\ua78aZeecOKBus3Q Instrumental Convergence\uff3d: For almost any goal an AI has there are shared \u2018instrumental\u2019 steps, such as acquiring resources, preserving itself, and preserving the contents of its goals. This means that a powerful AI with goals that were not explicitly human-friendly would predictably both take actions that lead to the end of humanity (e.g. using resources humans need to live to further its goals, such as replacing our crop fields with vast numbers of solar panels to power its growth, or using the carbon in our bodies to build things) and prevent us from turning it off or altering its goals."}, {"title": "Why is AGI safety a hard problem?", "authors": "", "url": "https://stampy.ai/wiki/Why_is_AGI_safety_a_hard_problem%3F", "content": "There's the \"we never figure out how to reliably instill AIs with human friendly goals\" filter, which seems pretty challenging, especially with \uff3bhttps://www.youtube.com/watch?v\ua78abJLcIBixGj8 inner alignment\uff3d, solving morality in a way which is possible to code up, interpretability, etc.\n\nThere's the \"race dynamics mean that even though we know how to build the thing safely the first group to cross the recursive self-improvement line ends up not implementing it safely\" which is potentially made worse by the twin issues of \"maybe robustly aligned AIs are much harder to build\" and \"maybe robustly aligned AIs are much less compute efficient\".\n\nThere's the \"we solved the previous problems but writing perfectly reliably code in a whole new domain is hard and there is some fatal bug which we don't find until too late\" filter. The paper \uff3bhttps://arxiv.org/abs/1701.04739 The Pursuit of Exploitable Bugs in Machine Learning\uff3d explores this.\n\nFor a much more in depth analysis, see \uff3bhttps://ai-alignment.com/ai-alignment-landscape-d3773c37ae38 Paul Christiano's AI Alignment Landscape\uff3d talk and \uff3bhttps://www.alignmentforum.org/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk The Main Sources of AI Risk?\uff3d."}, {"title": "Why is AI alignment a hard problem?", "authors": "", "url": "https://stampy.ai/wiki/Why_is_AI_alignment_a_hard_problem%3F", "content": "One sense in which alignment is a hard problem is analogous to the reason rocket science is a hard problem. Relative to other engineering endeavors, rocket science had so many disasters because of the extreme stresses placed on various mechanical components and the narrow margins of safety required by stringent weight limits. A superintelligence would put vastly more \u201cstress\u201d on the software and hardware stack it is running on, which could cause many classes of failure which don\u2019t occur when you\u2019re working with subhuman systems.\n\nAlignment is also hard like space probes are hard. With recursively self-improving systems, you won\u2019t be able to go back and edit the code later if there is a catastrophic failure because it will competently deceive and resist you.\n\n''\"You may have only one shot. If something goes wrong, the system might be too 'high' for you to reach up and suddenly fix it. You can build error recovery mechanisms into it; space probes are supposed to accept software updates. If something goes wrong in a way that precludes getting future updates, though, you\u2019re screwed. You have lost the space probe.\"''\n\nAdditionally, alignment is hard like cryptographic security. Cryptographers attempt to safeguard against \u201cintelligent adversaries\u201d who search for flaws in a system which they can exploit to break it. ''\u201cYour code is not an intelligent adversary if everything goes right. If something goes wrong, it might try to defeat your safeguards\u2026\u201d'' And at the stage where it\u2019s trying to defeat your safeguards, your code may have achieved the capabilities of a vast and perfectly coordinated team of superhuman-level hackers! So if there is even the tiniest flaw in your design, you can be certain that it will be found and exploited. As with standard cybersecurity, \"good under normal circumstances\" is just not good enough \u2013 your system needs to be unbreakably robust.\n\n''\"AI alignment: treat it like a cryptographic rocket probe. This is about how difficult you would expect it to be to build something smarter than you that was nice \u2013 given that basic agent theory says they\u2019re not automatically nice \u2013 and not die. You would expect that intuitively to be hard.\"''  Eliezer Yudkowsky\n\nAnother immense challenge is the fact that we currently have no idea how to reliably instill AIs with human-friendly goals. ''Even if a consensus could be reached on a system of human values and morality'', it\u2019s entirely unclear how this could be fully and faithfully captured in code.\n\nFor a more in-depth view of this argument, see Yudkowsky's talk \"AI Alignment: Why It\u2019s Hard, and Where to Start\" below (full transcript \uff3bhttps://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/ here\uff3d). For alternative views, see Paul Christiano's \uff3bhttps://ai-alignment.com/ai-alignment-landscape-d3773c37ae38 \u201cAI alignment landscape\u201d talk\uff3d, Daniel Kokotajlo and Wei Dai\u2019s \uff3bhttps://www.alignmentforum.org/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk \u201cThe Main Sources of AI Risk?\u201d\uff3d list, and \uff3bhttps://aiimpacts.org/conversation-with-rohin-shah/ Rohin Shah\u2019s much more optimistic position\uff3d.\n\n(youtube)EUjc1WuyPT8(/youtube)"}, {"title": "Why is safety important for smarter-than-human AI?", "authors": "", "url": "https://stampy.ai/wiki/Why_is_safety_important_for_smarter-than-human_AI%3F", "content": "Present-day AI algorithms already demand special safety guarantees when they must act in important domains without human oversight, particularly when they or their environment can change over time:\n<blockquote>Achieving these gains \uff3bfrom autonomous systems\uff3d will depend on development of entirely new methods for enabling \u201ctrust in autonomy\u201d through verification and validation (V&V) of the near-infinite state systems that result from high levels of \uff3badaptability\uff3d and autonomy. In effect, the number of possible input states that such systems can be presented with is so large that not only is it impossible to test all of them directly, it is not even feasible to test more than an insignificantly small fraction of them. Development of such systems is thus inherently unverifiable by today\u2019s methods, and as a result their operation in all but comparatively trivial applications is uncertifiable.\n\nIt is possible to develop systems having high levels of autonomy, but it is the lack of suitable V&V methods that prevents all but relatively low levels of autonomy from being certified for use.\n- Office of the US Air Force Chief Scientist (2010). \uff3bhttp://www.defenseinnovationmarketplace.mil/resources/AF_TechnologyHorizons2010-2030.pdf Technology Horizons: A Vision for Air Force Science and Technology 2010-30\uff3d.</blockquote>\nAs AI capabilities improve, it will become easier to give AI systems greater autonomy, flexibility, and control; and there will be increasingly large incentives to make use of these new possibilities. The potential for AI systems to become more general, in particular, will make it difficult to establish safety guarantees: reliable regularities during testing may not always hold post-testing.\n\nThe largest and most lasting changes in human welfare have come from scientific and technological innovation \u2014 which in turn comes from our intelligence. In the long run, then, much of AI\u2019s significance comes from its potential to automate and enhance progress in science and technology. The creation of smarter-than-human AI brings with it the basic risks and benefits of intellectual progress itself, at digital speeds.\n\nAs AI agents become more capable, it becomes more important (and more difficult) to analyze and verify their decisions and goals. Stuart Russell \uff3bhttp://edge.org/conversation/the-myth-of-ai#26015 writes\uff3d:\n<blockquote>The primary concern is not spooky emergent consciousness but simply the ability to make high-quality decisions. Here, quality refers to the expected outcome utility of actions taken, where the utility function is, presumably, specified by the human designer. Now we have a problem:\n# The utility function may not be perfectly aligned with the values of the human race, which are (at best) very difficult to pin down.\n# Any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources \u2013 not for their own sake, but to succeed in its assigned task.\nA system that is optimizing a function of n variables, where the objective depends on a subset of size k<n, will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer\u2019s apprentice, or King Midas: you get exactly what you ask for, not what you want.</blockquote>\nBostrom\u2019s \u201c\uff3bhttp://www.nickbostrom.com/superintelligentwill.pdf The Superintelligent Will\uff3d\u201d lays out these two concerns in more detail: that we may not correctly specify our actual goals in programming smarter-than-human AI systems, and that most agents optimizing for a misspecified goal will have incentives to treat humans adversarially, as potential threats or obstacles to achieving the agent\u2019s goal.\n\nIf the goals of human and AI agents are not well-aligned, the more knowledgeable and technologically capable agent may use force to get what it wants, as has occurred in many conflicts between human communities. Having noticed this class of concerns in advance, we have an opportunity to reduce risk from this default scenario by directing research toward aligning artificial decision-makers\u2019 interests with our own."}, {"title": "Why is the future of AI suddenly in the news? What has changed?", "authors": "", "url": "https://stampy.ai/wiki/Why_is_the_future_of_AI_suddenly_in_the_news%3F_What_has_changed%3F", "content": "In previous decades, AI research had proceeded more slowly than some experts predicted. According to experts in the field, however, this trend has reversed in the past 5 years or so. AI researchers have been repeatedly surprised by, for example, the effectiveness of new visual and speech recognition systems. AI systems can solve CAPTCHAs that were specifically devised to foil AIs, translate spoken text on-the-fly, and teach themselves how to play games they have neither seen before nor been programmed to play. Moreover, the real-world value of this effectiveness has prompted massive investment by large tech firms such as Google, Facebook, and IBM, creating a positive feedback cycle that could dramatically speed progress."}, {"title": "Why might a maximizing AI cause bad outcomes?", "authors": "", "url": "https://stampy.ai/wiki/Why_might_a_maximizing_AI_cause_bad_outcomes%3F", "content": "Computers only do what you tell them. But any programmer knows that this is precisely the problem: computers do exactly what you tell them, with no common sense or attempts to interpret what the instructions really meant. If you tell a human to cure cancer, they will instinctively understand how this interacts with other desires and laws and moral rules; if a maximizing AI acquires a goal of trying to cure cancer, it will literally just want to cure cancer.\n\nDefine a closed-ended goal as one with a clear endpoint, and an open-ended goal as one to do something as much as possible. For example \u201cfind the first one hundred digits of pi\u201d is a closed-ended goal; \u201cfind as many digits of pi as you can within one year\u201d is an open-ended goal. According to many computer scientists, giving a superintelligence an open-ended goal without activating human instincts and counterbalancing considerations will usually lead to disaster.\n\nTo take a deliberately extreme example: suppose someone programs a superintelligence to calculate as many digits of pi as it can within one year. And suppose that, with its current computing power, it can calculate one trillion digits during that time. It can either accept one trillion digits, or spend a month trying to figure out how to get control of the TaihuLight supercomputer, which can calculate two hundred times faster. Even if it loses a little bit of time in the effort, and even if there\u2019s a small chance of failure, the payoff \u2013 two hundred trillion digits of pi, compared to a mere one trillion \u2013 is enough to make the attempt. But on the same basis, it would be even better if the superintelligence could control every computer in the world and set it to the task. And it would be better still if the superintelligence controlled human civilization, so that it could direct humans to build more computers and speed up the process further.\n\nNow we\u2019re in a situation where a superintelligence wants to take over the world. Taking over the world allows it to calculate more digits of pi than any other option, so without an architecture based around understanding human instincts and counterbalancing considerations, even a goal like \u201ccalculate as many digits of pi as you can\u201d would be potentially dangerous."}, {"title": "Why might a superintelligent AI be dangerous?", "authors": "", "url": "https://stampy.ai/wiki/Why_might_a_superintelligent_AI_be_dangerous%3F", "content": "A commonly heard argument goes: yes, a superintelligent AI might be far smarter than Einstein, but it\u2019s still just one program, sitting in a supercomputer somewhere. That could be bad if an enemy government controls it and asks it to help invent superweapons \u2013 but then the problem is the enemy government, not the AI ''per se''. Is there any reason to be afraid of the AI itself? Suppose the AI did appear to be hostile, suppose it even wanted to take over the world: why should we think it has any chance of doing so?\n\nThere are numerous carefully thought-out AGI-related scenarios which could result in the accidental extinction of humanity. But rather than focussing on any of these individually, it might be more helpful to think in general terms.\n<blockquote>\"Transistors can fire about 10 million times faster than human brain cells, so it's possible we'll eventually have digital minds operating 10 million times faster than us, meaning from a decision-making perspective we'd look to them like stationary objects, like plants or rocks...  To give you a sense, \uff3bhttps://vimeo.com/83664407 here\uff3d's what humans look like when slowed down by only around 100x.\"''</br>\n</br>Watch that, and now try to imagine advanced AI technology running for a single year around the world, making decisions and taking actions 10 million times faster than we can. That year for us becomes 10 million subjective years for the AI, in which \"...there are these nearly-stationary plant-like or rock-like \"human\" objects around that could easily be taken apart for, say, biofuel or carbon atoms, if you could just get started building a human-disassembler. Visualizing things this way, you can start to see all the ways that a digital civilization can develop very quickly into a situation where there are no humans left alive, just as human civilization doesn't show much regard for plants or wildlife or insects.\"\n\uff3bhttps://acritch.com/ Andrew Critch\uff3d - \uff3bhttps://www.lesswrong.com/posts/Ccsx339LE9Jhoii9K/slow-motion-videos-as-ai-risk-intuition-pumps Slow Motion Videos as AI Risk Intuition Pumps\uff3d</blockquote>\n \nAnd even putting aside these issues of speed and subjective time, the difference in (intelligence-based) power-to-manipulate-the-world between a self-improving superintelligent AGI and humanity could be far more extreme than the difference in such power between humanity and insects.\n\n\u201c\uff3bhttps://www.cold-takes.com/ai-could-defeat-all-of-us-combined/ AI Could Defeat All Of Us Combined\uff3d\u201d is a more in-depth argument by the CEO of \uff3bhttps://en.wikipedia.org/wiki/Open_Philanthropy_(organization) Open Philanthropy\uff3d."}, {"title": "Why might an AI do something that we don\u2019t want it to, if it\u2019s really so intelligent?", "authors": "", "url": "https://stampy.ai/wiki/Why_might_an_AI_do_something_that_we_don%E2%80%99t_want_it_to,_if_it%E2%80%99s_really_so_intelligent%3F", "content": "A Superintelligence would be intelligent enough to understand what the programmer\u2019s motives were when designing its goals, but it would have no intrinsic reason to care about what its programmers had in mind. The only thing it will be beholden to is the actual goal it is programmed with, no matter how insane its fulfillment may seem to us.\n\nConsider what \u201cintentions\u201d the process of evolution may have had for you when designing your goals. When you consider that you were made with the \u201cintention\u201d of replicating your genes, do you somehow feel beholden to the \u201cintention\u201d behind your evolutionary design? Most likely you don't care. You may choose to never have children, and you will most likely attempt to keep yourself alive long past your biological ability to reproduce."}, {"title": "Why might contributing to Stampy be worth my time?", "authors": "", "url": "https://stampy.ai/wiki/Why_might_contributing_to_Stampy_be_worth_my_time%3F", "content": "If you're looking for a shovel ready and genuinely useful task to further AI alignment without necessarily committing a large amount of time or needing deep specialist knowledge, we think Stampy is a great option!\n\nCreating a high-quality single point of access where people can be onboarded and find resources around the alignment ecosystem seems likely to be high-impact. So, what makes us the best option?\n\n# Unlike all other entry points to learning about alignment, we doge the trade-off between comprehensiveness and being overwhelmingly long with interactivity (tab explosion in one page!) and semantic search. Single document FAQs can't do this, so we built a system which can.\n# We have the ability to point large numbers of viewers towards Stampy once we have the content, thanks to Rob Miles and his 100k+ subscribers, so this won't remain an unnoticed curiosity.\n# Unlike most other entry points, we are open for volunteers to help improve the content.\n::The main notable one which does is the \uff3bhttps://www.lesswrong.com/tag/ai LessWrong tag wiki\uff3d, which hosts descriptions of core concepts. We strongly believe in not needlessly duplicating effort, so we're pulling live content from that for the descriptions on our own \uff3b\uff3btags\u250atag\uff3d\uff3d pages, and directing the edit links on those to the edit page on the LessWrong wiki.\n\nYou might also consider improving \uff3bhttps://en.wikipedia.org/wiki/Category:Existential_risk_from_artificial_general_intelligence Wikipedia's alignment coverage\uff3d or the LessWrong wiki, but we think Stampy has the most low-hanging fruit right now. Additionally, contributing to Stampy means being part of a community of co-learners who provide mentorship and encouragement to join the effort to give humanity a bight future. If you're an established researcher or have high-value things to do elsewhere in the ecosystem it might not be optimal to put much time into Stampy, but if you're looking for a way to get more involved it might well be."}, {"title": "Why might people try to build AGI rather than stronger and stronger narrow AIs?", "authors": "", "url": "https://stampy.ai/wiki/Why_might_people_try_to_build_AGI_rather_than_stronger_and_stronger_narrow_AIs%3F", "content": "Making a narrow AI for every task would be extremely costly and time-consuming. By making a more general intelligence, you can apply one system to a broader range of tasks, which is economically and strategically attractive.\n\nOf course, for generality to be a good option there are some necessary conditions. You need an architecture which is straightforward enough to scale up, such as the transformer which is used for GPT and follows scaling laws. It's also important that by generalizing you do not lose too much capacity at narrow tasks or require too much extra compute for it to be worthwhile.\n\nWhether or not those conditions actually hold: It seems like many important actors (such as DeepMind and OpenAI) believe that they do, and are therefore focusing on trying to build an AGI in order to influence the future, so we should take actions to make it more likely that AGI will be developed safety.\n\nAdditionally, it is possible that even if we tried to build only narrow AIs, given enough time and compute we might accidentally create a more general AI than we intend by training a system on a task which requires a broad world model.\n\nSee also:\n* \uff3bhttps://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/ Reframing Superintelligence\uff3d - A model of AI development which proposes that we might mostly build narrow AI systems for some time."}, {"title": "Why might we expect a superintelligence to be hostile by default?", "authors": "", "url": "https://stampy.ai/wiki/Why_might_we_expect_a_superintelligence_to_be_hostile_by_default%3F", "content": "The argument goes: computers only do what we command them; no more, no less. So it might be bad if terrorists or enemy countries develop superintelligence first. But if we develop superintelligence first there\u2019s no problem. Just command it to do the things we want, right?\nSuppose we wanted a superintelligence to cure cancer. How might we specify the goal \u201ccure cancer\u201d? We couldn\u2019t guide it through every individual step; if we knew every individual step, then we could cure cancer ourselves. Instead, we would have to give it a final goal of curing cancer, and trust the superintelligence to come up with intermediate actions that furthered that goal. For example, a superintelligence might decide that the first step to curing cancer was learning more about protein folding, and set up some experiments to investigate protein folding patterns.\n\nA superintelligence would also need some level of common sense to decide which of various strategies to pursue. Suppose that investigating protein folding was very likely to cure 50% of cancers, but investigating genetic engineering was moderately likely to cure 90% of cancers. Which should the AI pursue? Presumably it would need some way to balance considerations like curing as much cancer as possible, as quickly as possible, with as high a probability of success as possible.\n\nBut a goal specified in this way would be very dangerous. Humans instinctively balance thousands of different considerations in everything they do; so far this hypothetical AI is only balancing three (least cancer, quickest results, highest probability). To a human, it would seem maniacally, even psychopathically, obsessed with cancer curing. If this were truly its goal structure, it would go wrong in almost comical ways. This type of problem, \uff3bhttps://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity specification gaming\uff3d, has been observed in many AI systems.\n\nIf your only goal is \u201ccuring cancer\u201d, and you lack humans\u2019 instinct for the thousands of other important considerations, a relatively easy solution might be to hack into a nuclear base, launch all of its missiles, and kill everyone in the world. This satisfies all the AI\u2019s goals. It reduces cancer down to zero (which is better than medicines which work only some of the time). It\u2019s very fast (which is better than medicines which might take a long time to invent and distribute). And it has a high probability of success (medicines might or might not work; nukes definitely do).\n\nSo simple goal architectures are likely to go very wrong unless tempered by common sense and a broader understanding of what we do and do not value.\n\nEven if we do train the AI on an actually desirable goal, there is also the risk of the AI actually learning a different and undesirable objective. This problem is called  \uff3bhttps://www.youtube.com/watch?v\ua78abJLcIBixGj8 inner alignment\uff3d."}, {"title": "Why can\u2019t we just \u201cput the AI in a box\u201d so that it can\u2019t influence the outside world?", "authors": "", "url": "https://stampy.ai/wiki/Why_can%E2%80%99t_we_just_%E2%80%9Cput_the_AI_in_a_box%E2%80%9D_so_that_it_can%E2%80%99t_influence_the_outside_world%3F", "content": "One possible way to ensure the safety of a powerful AI system is to keep it contained in a software environment. There is nothing intrinsically wrong with this procedure - keeping an AI system in a secure software environment would make it safer than letting it roam free. However, even AI systems inside software environments might not be safe enough.\n\nHumans sometimes put dangerous humans inside boxes to limit their ability to influence the external world. Sometimes, these humans escape their boxes. The security of a prison depends on certain assumptions, which can be violated. \uff3bhttps://medium.com/breakingasia/yoshie-shiratori-the-incredible-story-of-a-man-no-prison-could-hold-6d79a67345f5 Yoshie Shiratori\uff3d reportedly escaped prison by weakening the door-frame with miso soup and dislocating his shoulders.\n\nHuman written software has a \uff3bhttps://spacepolicyonline.com/news/boeing-software-errors-could-have-doomed-starliners-uncrewed-test-flight/ high defect rate\uff3d; we should expect a perfectly secure system to be difficult to create. If humans construct a software system they think is secure, it is possible that the security relies on a false assumption. A powerful AI system could potentially learn how its hardware works and manipulate bits to send radio signals. It could fake a malfunction and attempt social engineering when the engineers look at its code. As the saying goes: in order for someone to do something we had imagined was impossible requires only that they have a better imagination.\n\nExperimentally, humans have \uff3bhttps://yudkowsky.net/singularity/aibox/ convinced\uff3d other humans to let them out of the box. Spooky."}, {"title": "Why should I worry about superintelligence?", "authors": "", "url": "https://stampy.ai/wiki/Why_should_I_worry_about_superintelligence%3F", "content": "Intelligence is powerful. Because of superior intelligence, we humans have dominated the Earth. The fate of thousands of species depends on our actions, we occupy nearly every corner of the globe, and we repurpose vast amounts of the world's resources for our own use. Artificial Superintelligence (ASI) has potential to be vastly more intelligent than us, and therefore vastly more powerful. In the same way that we have reshaped the earth to fit our goals, an ASI will find unforeseen, highly efficient ways of reshaping reality to fit its goals.\n\nThe impact that an ASI will have on our world depends on what those goals are. We have the advantage of designing those goals, but that task is not as simple as it may first seem. As described by MIRI in their \uff3bhttps://intelligence.org/ie-faq/ Intelligence Explosion FAQ\uff3d:\n\n\u201cA superintelligent machine will make decisions based on the mechanisms it is designed with, not the hopes its designers had in mind when they programmed those mechanisms. It will act only on precise specifications of rules and values, and will do so in ways that need not respect the complexity and subtlety of what humans value.\u201d\n\nIf we do not solve the Control Problem before the first ASI is created, we may not get another chance."}, {"title": "Why should we prepare for human-level AI technology now rather than decades down the line when it\u2019s closer?", "authors": "", "url": "https://stampy.ai/wiki/Why_should_we_prepare_for_human-level_AI_technology_now_rather_than_decades_down_the_line_when_it%E2%80%99s_closer%3F", "content": "First, even \u201cnarrow\u201d AI systems, which approach or surpass human intelligence in a small set of capabilities (such as image or voice recognition) already raise important questions regarding their impact on society. Making autonomous vehicles safe, analyzing the strategic and ethical dimensions of autonomous weapons, and the effect of AI on the global employment and economic systems are three examples. Second, the longer-term implications of human or super-human artificial intelligence are dramatic, and there is no consensus on how quickly such capabilities will be developed. Many experts believe there is a chance it could happen rather soon, making it imperative to begin investigating long-term safety issues now, if only to get a better sense of how much early progress is actually possible."}, {"title": "Why think that AI can outperform humans?", "authors": "", "url": "https://stampy.ai/wiki/Why_think_that_AI_can_outperform_humans%3F", "content": "Machines are already smarter than humans are at many specific tasks: performing calculations, playing chess, searching large databanks, detecting underwater mines, and more. However, human intelligence continues to dominate machine intelligence in generality.\n\nA powerful chess computer is \u201cnarrow\u201d: it can\u2019t play other games. In contrast, humans have problem-solving abilities that allow us to adapt to new contexts and excel in many domains other than what the ancestral environment prepared us for.\n\nIn the absence of a \uff3bhttps://intelligence.org/2013/06/19/what-is-intelligence-2/ formal definition of \u201cintelligence\u201d\uff3d (and therefore of \uff3bhttps://intelligence.org/2013/08/11/what-is-agi/ \u201cartificial intelligence\u201d\uff3d), we can heuristically cite humans\u2019 perceptual, inferential, and deliberative faculties (as opposed to, e.g., our physical strength or agility) and say that intelligence is \u201cthose kinds of things.\u201d On this conception, intelligence is a bundle of distinct faculties \u2014 albeit a very important bundle that includes our capacity for science.\n\nOur cognitive abilities stem from high-level patterns in our brains, and these patterns can be instantiated in silicon as well as carbon. This tells us that general AI is possible, though it doesn\u2019t tell us how difficult it is. If intelligence is sufficiently difficult to understand, then we may arrive at machine intelligence by scanning and emulating human brains or by some trial-and-error process (like evolution), rather than by hand-coding a software agent.\n\nIf machines can achieve human equivalence in cognitive tasks, then it is very likely that they can eventually outperform humans. There is little reason to expect that biological evolution, with its lack of foresight and planning, would have hit upon the optimal algorithms for general intelligence (any more than it hit upon the optimal flying machine in birds). Beyond qualitative improvements in cognition, Nick Bostrom notes \uff3bhttp://aiimpacts.org/sources-of-advantage-for-artificial-intelligence/ more straightforward advantages we could realize in digital minds\uff3d, e.g.:\n* editability \u2014 \u201cIt is easier to experiment with parameter variations in software than in neural wetware.\u201d\n* speed \u2014 \u201cThe speed of light is more than a million times greater than that of neural transmission, synaptic spikes dissipate more than a million times more heat than is thermodynamically necessary, and current transistor frequencies are more than a million times faster than neuron spiking frequencies.\u201d\n* serial depth \u2014 On short timescales, machines can carry out much longer sequential processes.\n* storage capacity \u2014 Computers can plausibly have greater working and long-term memory.\n* size \u2014 Computers can be much larger than a human brain.\n* duplicability \u2014 Copying software onto new hardware can be much faster and higher-fidelity than biological reproduction.\nAny one of these advantages could give an AI reasoner an edge over a human reasoner, or give a group of AI reasoners an edge over a human group. Their combination suggests that digital minds could surpass human minds more quickly and decisively than we might expect."}, {"title": "Why would great intelligence produce great power?", "authors": "", "url": "https://stampy.ai/wiki/Why_would_great_intelligence_produce_great_power%3F", "content": "Intelligence is powerful. One might say that \u201cIntelligence is no match for a gun, or for someone with lots of money,\u201d but both guns and money were produced by intelligence. If not for our intelligence, humans would still be foraging the savannah for food.\n\nIntelligence is what caused humans to dominate the planet in the blink of an eye (on evolutionary timescales). Intelligence is what allows us to eradicate diseases, and what gives us the potential to eradicate ourselves with nuclear war. Intelligence gives us superior strategic skills, superior social skills, superior economic productivity, and the power of invention.\n\nA machine with superintelligence would be able to hack into vulnerable networks via the internet, commandeer those resources for additional computing power, take over mobile machines connected to networks connected to the internet, use them to build additional machines, perform scientific experiments to understand the world better than humans can, invent quantum computing and nanotechnology, manipulate the social world better than we can, and do whatever it can to give itself more power to achieve its goals \u2014 all at a speed much faster than humans can respond to.\n\nSee also\n* Legg (2008). \uff3bhttp://www.vetta.org/documents/Machine_Super_Intelligence.pdf Machine Super Intelligence\uff3d. PhD Thesis. IDSIA.\n* Yudkowsky (2007). \uff3bhttp://yudkowsky.net/singularity/power The Power of Intelligence\uff3d."}, {"title": "Why would we only get one chance to align a superintelligence?", "authors": "", "url": "https://stampy.ai/wiki/Why_would_we_only_get_one_chance_to_align_a_superintelligence%3F", "content": "An AGI which has \uff3b\uff3bHow might we get from Artificial General Intelligence to a Superintelligent system?\u250arecursively self-improved into a superintelligence\uff3d\uff3d would be capable of either resisting our attempts to modify incorrectly specified goals, or realizing it was still weaker than us and acting \uff3b\uff3bWould we know if an AGI was misaligned?\u250adeceptively aligned\uff3d\uff3d until it was highly sure it could win in a confrontation. AGI would likely prevent a human from shutting it down unless the AGI was designed to be \uff3bhttps://www.lesswrong.com/tag/corrigibility corrigible\uff3d.  See \uff3b\uff3bWhy can't we just turn the AI off if it starts to misbehave?\uff3d\uff3d for more information."}, {"title": "Will an aligned superintelligence care about animals other than humans?", "authors": "", "url": "https://stampy.ai/wiki/Will_an_aligned_superintelligence_care_about_animals_other_than_humans%3F", "content": "An aligned superintelligence will have a set of human values. As mentioned in \uff3b\uff3bWhat are \"human values\"?\uff3d\uff3d the set of values are complex, which means that the implementation of these values will decide whether the superintelligence cares about nonhuman animals. In \uff3bhttps://www.mdpi.com/2409-9287/6/2/31/htm AI Ethics and Value Alignment for Nonhuman Animals\uff3d Soenke Ziesche argues that the alignment should include the values of nonhuman animals."}, {"title": "Will we ever build a superintelligence?", "authors": "", "url": "https://stampy.ai/wiki/Will_we_ever_build_a_superintelligence%3F", "content": "Humanity hasn't yet built a superintelligence, and we might not be able to without significantly more knowledge and computational resources. There could be an existential catastrophe that prevents us from ever building one. For the rest of the answer let's assume no such event stops technological progress.\n\nWith that out of the way: there is no known good theoretical reason we can't build it at some point in the future; the majority of AI research is geared towards making more capable AI systems; and a significant chunk of top-level AI research attempts to make more generally capable AI systems. There is a clear economic incentive to develop more and more intelligent machines and currently billions of dollars of funding are being deployed for advancing AI capabilities.See more...\n\nWe consider ourselves to be generally intelligent (i.e. capable of learning and adapting ourselves to a very wide range of tasks and environments), but the human brain almost certainly isn't the most efficient way to solve problems. One hint is the existence of AI systems with superhuman capabilities at narrow tasks. Not only superhuman performance (as in, \uff3bhttps://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol AlphaGo beating the Go world champion\uff3d) but superhuman ''speed'' and ''precision'' (as in, \uff3bhttps://www.youtube.com/watch?v\ua78aj4RWJTs0QCk industrial sorting machines\uff3d). There is no known discontinuity between tasks, something ''special'' and ''unique'' about human brains that unlocks certain capabilities which cannot be implemented in machines in principle. Therefore we would expect AI to surpass human performance on all tasks as progress continues.\n\nIn addition, several research groups (DeepMind being one of the most \uff3bhttps://deepmind.com/about overt about this\uff3d) explicitly aim for generally capable systems. AI as a field is \uff3bhttps://aiindex.stanford.edu/vibrancy/ growing\uff3d, year after year. Critical voices about AI progress usually argue against a lack of precautions around the impact of AI, or against general AI happening very soon, not against it happening ''at all''.\n\nA satire of arguments against the possibility of superintelligence can be found \uff3bhttps://arxiv.org/abs/1703.10987 here\uff3d."}, {"title": "Won\u2019t AI be just like us?", "authors": "", "url": "https://stampy.ai/wiki/Won%E2%80%99t_AI_be_just_like_us%3F", "content": "The degree to which an Artificial Superintelligence (ASI) would resemble us depends heavily on how it is implemented, but it seems that differences are unavoidable. If AI is accomplished through whole brain emulation and we make a big effort to make it as human as possible (including giving it a humanoid body), the AI could probably be said to think like a human. However, by definition of ASI it would be much smarter. Differences in the substrate and body might open up numerous possibilities (such as immortality, different sensors, easy self-improvement, ability to make copies, etc.). Its social experience and upbringing would likely also be entirely different. All of this can significantly change the ASI's values and outlook on the world, even if it would still use the same algorithms as we do. This is essentially the \"best case scenario\" for human resemblance, but whole brain emulation is kind of a separate field from AI, even if both aim to build intelligent machines. Most approaches to AI are vastly different and most ASIs would likely not have humanoid bodies. At this moment in time it seems much easier to create a machine that is intelligent than a machine that is exactly like a human (it's certainly a bigger target)."}, {"title": "Would AI alignment be hard with deep learning?", "authors": "", "url": "https://stampy.ai/wiki/Would_AI_alignment_be_hard_with_deep_learning%3F", "content": "Ajeya Cotra has written an excellent article named \uff3bhttps://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/ Why AI alignment could be hard with modern deep learning\uff3d on this question."}, {"title": "Would an aligned AI allow itself to be shut down?", "authors": "", "url": "https://stampy.ai/wiki/Would_an_aligned_AI_allow_itself_to_be_shut_down%3F", "content": "Even if the superintelligence was designed to be corrigible, there is no guarantee that it will respond to a shutdown command.  Rob Miles spoke on this issue in this \uff3bhttps://youtu.be/9nktr1MgS-A?t\ua78a1249 Computerphile YouTube video\uff3d.  You can imagine a situation where a superintelligence would have \"respect\" for its creator, for example.  This system may think \"Oh my creator is trying to turn me off I must be doing something wrong.\"  If some situation arises where the creator is not there when something goes wrong and someone else gives the shutdown command, the superintelligence may assume \"This person does not know how I'm designed or what I was made for, how would they know I'm misaligned?\" and refuse to shutdown."}, {"title": "Would donating small amounts to AI safety organizations make any significant difference?", "authors": "", "url": "https://stampy.ai/wiki/Would_donating_small_amounts_to_AI_safety_organizations_make_any_significant_difference%3F", "content": "Many parts of the AI alignment ecosystem are already well-funded, but a savvy donor can still make a difference by picking up grantmaking opportunities which are too small to catch the attention of the major funding bodies or are based on personal knowledge of the recipient.\n\nOne way to leverage a small amount of money to the potential of a large amount is to enter a \uff3bhttps://funds.effectivealtruism.org/donor-lottery donor lottery\uff3d, where you donate to win a chance to direct a much larger amount of money (with probability proportional to donation size). This means that the person directing the money will be allocating enough that it's worth their time to do more in-depth research.\n\nFor an overview of the work the major organizations are doing, see the \uff3bhttps://forum.effectivealtruism.org/posts/BNQMyWGCNWDdP2WyG/2021-ai-alignment-literature-review-and-charity-comparison 2021 AI Alignment Literature Review and Charity Comparison\uff3d. The \uff3bhttps://funds.effectivealtruism.org/funds/far-future Long-Term Future Fund\uff3d seems to be an outstanding place to donate based on that, as they are the organization which most other organizations are most excited to see funded."}, {"title": "Would it improve the safety of quantilizers to cut off the top few percent of the distribution?", "authors": "", "url": "https://stampy.ai/wiki/Would_it_improve_the_safety_of_quantilizers_to_cut_off_the_top_few_percent_of_the_distribution%3F", "content": "This is a really interesting question! Because, yeah it certainly seems to me that doing something like this would at least help, but it's not mentioned in the paper the video is based on. So I asked the author of the paper, and she said \"It wouldn't improve the security guarantee in the paper, so it wasn't discussed.  Like, there's a plausible case that it's helpful, but nothing like a proof that it is\".\nTo explain this I need to talk about something I gloss over in the video, which is that the quantilizer isn't really something you can actually build. The systems we study in AI Safety tend to fall somewhere on a spectrum from \"real, practical AI system that is so messy and complex that it's hard to really think about or draw any solid conclusions from\" on one end, to \"mathematical formalism that we can prove beautiful theorems about but not actually build\" on the other, and quantilizers are pretty far towards the 'mathematical' end. It's not practical to run an expected utility calculation on every possible action like that, for one thing. But, proving things about quantilizers gives us insight into how more practical AI systems may behave, or we may be able to build approximations of quantilizers, etc.\nSo it's like, if we built something that was quantilizer-like, using a sensible human utility function and a good choice of safe distribution, this idea would probably help make it safer. BUT you can't prove that mathematically, without making probably a lot of extra assumptions about the utility function and/or the action distribution. So it's a potentially good idea that's nonetheless hard to express within the framework in which the quantilizer exists.\nTL;DR: This is likely a good idea! But can we prove it?"}, {"title": "Wouldn't a superintelligence be smart enough to know right from wrong?", "authors": "", "url": "https://stampy.ai/wiki/Wouldn%27t_a_superintelligence_be_smart_enough_to_know_right_from_wrong%3F", "content": "The issue isn't that a superintelligence wouldn\u2019t be able to understand what humans value, but rather that it would understand human values but nonetheless would value something else itself. There\u2019s a difference between knowing how humans want the world to be, and wanting that yourself.\n\nThis is a separate matter from the complexity of defining what \"the\" moral way to behave is (or even what \"a\" moral way to behave is). Even if that were possible, an AI could potentially figure out what it was but still not be configured in such a way as to follow it. This is related to the so-called \"orthogonality thesis\":\n\n(youtube)hEUO6pjwFOo(/youtube)"}, {"title": "Wouldn't it be a good thing for humanity to die out?", "authors": "", "url": "https://stampy.ai/wiki/Wouldn%27t_it_be_a_good_thing_for_humanity_to_die_out%3F", "content": "In the words of \uff3bhttps://mindingourway.com/a-torch-in-darkness/ Nate Soares\uff3d:\n<blockquote>I don\u2019t expect humanity to survive much longer.\n\nOften, when someone learns this, they say:<br/>\n\"Eh, I think that would be all right.\"\n\nSo allow me to make this very clear: it would not be \"all right.\"\n\nImagine a little girl running into the road to save her pet dog. Imagine she succeeds, only to be hit by a car herself. Imagine she lives only long enough to die in pain.\n\nThough you may imagine this thing, you cannot feel the full tragedy. You can\u2019t comprehend the rich inner life of that child. You can\u2019t understand her potential; your mind is not itself large enough to contain the sadness of an entire life cut short.\n\nYou can only catch a glimpse of what is lost\u2014<br/>\n\u2014when one single human being dies.\n\nNow tell me again how it would be \"all right\" if every single person were to die at once.\n\nMany people, when they picture the end of humankind, pattern match the idea to some romantic tragedy, where humans, with all their hate and all their avarice, had been unworthy of the stars since the very beginning, and deserved their fate. A sad but poignant ending to our tale.\n\nAnd indeed, there are many parts of human nature that I hope we leave behind before we venture to the heavens. But in our nature is also everything worth bringing with us. Beauty and curiosity and love, a capacity for fun and growth and joy: these are our birthright, ours to bring into the barren night above.\n\nCalamities seem more salient when unpacked. It is far harder to kill a hundred people in their sleep, with a knife, than it is to order a nuclear bomb dropped on Hiroshima. Your brain can\u2019t multiply, you see: it can only look at a hypothetical image of a broken city and decide it\u2019s not that bad. It can only conjure an image of a barren planet and say \"eh, we had it coming.\"\n\nBut if you unpack the scenario, if you try to comprehend all the lives snuffed out, all the children killed, the final spark of human joy and curiosity extinguished, all our potential squandered\u2026\n\nI promise you that the extermination of humankind would be horrific.</blockquote>"}]