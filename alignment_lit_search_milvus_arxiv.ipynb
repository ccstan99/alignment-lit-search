{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Search Engine\n",
    "In this example we will be going over the code used to build a Text Search Engine. This example uses a modified BERT model to convert text to vectors stored in Milvus, which can then be combined with Milvus to search for similar text to the user input text.\n",
    "\n",
    "### Sourced from Milvus Bootcamp\n",
    "- https://github.com/milvus-io/bootcamp/tree/master/solutions/text_search_engine\n",
    "- https://github.com/milvus-io/bootcamp/tree/master/solutions/question_answering_system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "This example uses the English News dataset. In this example, we use a small subset of the dataset containing 180 mutually corresponding title-texts, which can be found in the **Data** directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "\n",
    "|  Packages   |  Servers    |\n",
    "|-                  | -                 |   \n",
    "| pymilvus==2.0.0rc5      | milvus 2.0.0-rc5    |\n",
    "| sentence_transformers      | postgres          |\n",
    "| psycopg2          |\n",
    "| pandas           |\n",
    "| numpy   |\n",
    "\n",
    "We have included a `requirements.txt` file in order to easily satisfy the required packages. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up and Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Packages\n",
    "Install the required python packages with `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Milvus Server\n",
    "\n",
    "This demo uses Milvus 2.0, please refer to the [Install Milvus](https://milvus.io/cn/docs/install_standalone-docker.md) guide to learn how to use this docker container. For this example we wont be mapping any local volumes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/milvus-io/milvus/master/deployments/docker/standalone/docker-compose.yml -O docker-compose.yml\n",
    "!sudo docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Postgres Server\n",
    "For now, Milvus doesn't support storing multiple attributes for the data. Because of this we have to use another service to store these attributes and search through them, in this case PostgreSQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db3d2bcec8f2bd24a04c4e5aa87b792c3cdd50f4957cdeb88290029fb77795fd\r\n"
     ]
    }
   ],
   "source": [
    "! docker run --name postgres0 -d  -p 5438:5432 -e POSTGRES_HOST_AUTH_METHOD=trust postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-16 02:02:31.819 UTC [1] LOG:  starting PostgreSQL 15.0 (Debian 15.0-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\r\n",
      "2022-10-16 02:02:31.819 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\r\n",
      "2022-10-16 02:02:31.819 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\r\n",
      "2022-10-16 02:02:31.820 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\r\n",
      "2022-10-16 02:02:31.823 UTC [62] LOG:  database system was shut down at 2022-10-16 02:02:31 UTC\r\n",
      "2022-10-16 02:02:31.826 UTC [1] LOG:  database system is ready to accept connections\r\n"
     ]
    }
   ],
   "source": [
    "! docker logs postgres0 --tail 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Servers\n",
    "We first start off by connecting to the servers. In this case the docker containers are running on localhost and the ports are the default ports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Milvus, BERT and Postgresql\n",
    "from pymilvus import connections\n",
    "import psycopg2\n",
    "connections.connect(host='localhost', port='19530')\n",
    "conn = psycopg2.connect(host='localhost', port='5438', user='postgres', password='postgres')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Collection and Setting Index\n",
    "#### 1. Creating the Collection    \n",
    "The next step is to create a collection, which requires declaring the name of the collection and the dimension of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import Collection, CollectionSchema, FieldSchema, DataType, utility\n",
    "\n",
    "table_name= \"alignment_lit\"\n",
    "field_name = \"vector_embedding\"\n",
    "\n",
    "if utility.has_collection(table_name):\n",
    "    utility.drop_collection(table_name)\n",
    "\n",
    "pk = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True)\n",
    "field = FieldSchema(name=field_name, dtype=DataType.FLOAT_VECTOR, dim=768)\n",
    "schema = CollectionSchema(fields=[pk,field], description=\"AI Alignment Literature Dataset\")\n",
    "collection = Collection(name=table_name, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Setting an Index\n",
    "After creating the collection we want to assign it an index type. This can be done before or after inserting the data. When done before, indexes will be made as data comes in and fills the data segments. In this example we are using IVF_SQ8 which requires the 'nlist' parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(code=0, message='')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_param = {\n",
    "        \"metric_type\":\"L2\",\n",
    "        \"index_type\":\"IVF_SQ8\",\n",
    "        \"params\":{\"nlist\":1024}\n",
    "    }\n",
    "collection.create_index(field_name=field_name, index_params=index_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Table in Postgres  \n",
    "PostgresSQL will be used to store Milvus ID and its corresponding title and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create postgres table successfully!\n"
     ]
    }
   ],
   "source": [
    "#Deleting previouslny stored table for clean run\n",
    "drop_table = \"DROP TABLE IF EXISTS \" + table_name\n",
    "cursor.execute(drop_table)\n",
    "conn.commit()\n",
    "\n",
    "try:\n",
    "    sql = \"CREATE TABLE if not exists \" + table_name + \" (ids bigint, title text, authors text, url text, text text);\"\n",
    "    cursor.execute(sql)\n",
    "    conn.commit()\n",
    "    print(\"create postgres table successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"can't create a postgres table: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and Storing the News Data\n",
    "#### 1. Generating Embeddings\n",
    "In this example we are using the sentence_transformer library  to encode the sentence into vectors. This library uses a modified BERT model to generate the embeddings, and in this example we are using a model pretrained using Microsoft's `mpnet`. More info can be found [here](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "model = SentenceTransformer('allenai-specter')\n",
    "# Get questions and answers.\n",
    "data = pd.read_json('arxiv_pos_list.json')\n",
    "title_data = data['title'].tolist()\n",
    "text_data = data['abstract'].tolist()\n",
    "\n",
    "title_text_data = data['title'].map(str) + '[SEP]' + data['abstract'].map(str)\n",
    "\n",
    "sentence_embeddings = model.encode(title_text_data)\n",
    "sentence_embeddings = normalize(sentence_embeddings)\n",
    "print(type(sentence_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Inserting Vectors into Milvus\n",
    "Since this example dataset contains only 100 vectors, we are inserting all of them as one batch insert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "em =list(sentence_embeddings)\n",
    "mr = collection.insert([em])\n",
    "ids = mr.primary_keys\n",
    "dicts ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "959"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inserting IDs and Title-text into PostgreSQL\n",
    "In order to transfer the data into Postgres, we are creating a new file that combines all the data into a readable format. Once created, we pass this file into the Postgress server through STDIN due to the Postgres container not having access to the file locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted into Postgress Sucessfully!\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "\n",
    "# conn.rollback()\n",
    "# extra delimiters | and quotes in title/text causes parsing issues, must strip\n",
    "def clean_string(old_string):\n",
    "    return old_string.replace(\"|\",\"\").replace(\"\\\"\",\"\").replace(\"'\",\"\")\n",
    "\n",
    "\n",
    "def record_temp_csv(fname, ids, title, authors, urls, text):\n",
    "    with open(fname,'w') as f:\n",
    "        for i in range(len(ids)):\n",
    "            line = str(ids[i]) + \"|\" + clean_string(title[i]) + \"|\" + \", \".join(authors[i]) + \\\n",
    "            \"|\" + clean_string(urls[i]) + \"|\" + clean_string(text[i]) + \"\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "def copy_data_to_pg(table_name, fname, conn, cur):\n",
    "    fname = os.path.join(os.getcwd(),fname)\n",
    "    try:\n",
    "        sql = \"COPY \" + table_name + \" FROM STDIN DELIMITER '|' CSV HEADER\"\n",
    "        cursor.copy_expert(sql, open(fname, \"r\"))\n",
    "        conn.commit()\n",
    "        print(\"Inserted into Postgress Sucessfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Copy Data into Postgress failed: \", e)\n",
    "        \n",
    "DATA_WITH_IDS = 'arxiv_pos_list.csv'   \n",
    "\n",
    "record_temp_csv(DATA_WITH_IDS, ids, title_data, data['authors'].tolist(), data['url'].tolist(), text_data)\n",
    "copy_data_to_pg(table_name, DATA_WITH_IDS, conn, cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search\n",
    "#### 1. Processing Query\n",
    "When searching for a question, we first put the question through the same model to generate an embedding. Then with that embedding vector we  can search for similar embeddings in Milvus.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "\n",
    "title = \"Could regulating the creators deliver trustworthy AI?\"\n",
    "# title = \"Fooling the primate brain\"\n",
    "\n",
    "query_embeddings = []\n",
    "embed = model.encode(title)\n",
    "embed = embed.reshape(1,-1)\n",
    "embed = normalize(embed)\n",
    "query_embeddings = embed.tolist()\n",
    "\n",
    "collection.load()\n",
    "results = collection.search(query_embeddings, field_name, param=search_params, limit=15, expr=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Getting the Similar Titles\n",
    "There may not have titles that are similar to the given one. So we can set a threshold value, here we use 0.5, and when the most similar distance retrieved is less than this value, a hint that the system doesn't include the relevant question is returned. We then use the result ID's to pull out the similar titles from the Postgres server and print them with their corresponding similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 436701982162682773 (0.233) http://arxiv.org/abs/2006.14750v1 Could regulating the creators deliver trustworthy AI?\n",
      " 436701982162682720 (0.312) http://arxiv.org/abs/2107.06641v3 Trustworthy AI: A Computational Perspective\n",
      " 436701982162682627 (0.321) http://arxiv.org/abs/2101.02032v5 Socially Responsible AI Algorithms: Issues, Purposes, and Challenges\n",
      " 436701982162682712 (0.336) http://arxiv.org/abs/2002.06276v1 Trustworthy AI\n",
      " 436701982162682777 (0.337) http://arxiv.org/abs/2110.01167v2 Trustworthy AI: From Principles to Practices\n",
      " 436701982162682568 (0.343) http://arxiv.org/abs/2112.07773v1 Filling gaps in trustworthy development of AI\n",
      " 436701982162682700 (0.352) http://arxiv.org/abs/1905.04994v2 Governance by Glass-Box: Implementing Transparent Moral Bounds for AI Behaviour\n",
      " 436701982162682624 (0.369) http://arxiv.org/abs/2204.13828v1 Designing for Responsible Trust in AI Systems: A Communication Perspective\n",
      " 436701982162682038 (0.371) http://arxiv.org/abs/2110.06674v1 Truthful AI: Developing and governing AI that does not lie\n",
      " 436701982162682714 (0.389) http://arxiv.org/abs/2102.07536v1 The corruptive force of AI-generated advice\n",
      " 436701982162682727 (0.404) http://arxiv.org/abs/1605.02817v2 Unethical Research: How to Create a Malevolent Artificial Intelligence\n",
      " 436701982162682462 (0.406) http://arxiv.org/abs/2104.03741v1 Voluntary safety commitments provide an escape from over-regulation in AI development\n",
      " 436701982162682640 (0.406) http://arxiv.org/abs/2106.11036v1 Know Your Model (KYM): Increasing Trust in AI and Machine Learning\n",
      " 436701982162682756 (0.410) http://arxiv.org/abs/2105.00002v1 Ethics-Based Auditing to Develop Trustworthy AI\n",
      " 436701982162682785 (0.413) http://arxiv.org/abs/2003.11157v1 AI loyalty: A New Paradigm for Aligning Stakeholder Interests"
     ]
    }
   ],
   "source": [
    "similar_titles = []\n",
    "\n",
    "for result in results[0]:\n",
    "    sql = \"select title, url from \" + table_name + \" where ids = \" + str(result.id) + \";\"\n",
    "    print(\"\\n\", result.id, end='')\n",
    "    cursor.execute(sql)\n",
    "    rows = cursor.fetchall()\n",
    "    if len(rows):\n",
    "        title, url = rows[0]\n",
    "        similar_titles.append((title, url, result.distance))\n",
    "        print(f' ({result.distance:.3f}) {url} {title}', end='')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Get the text\n",
    "After getting a list of similar titles, choose the one that you feel is closest to yours. Then you can use that title to find the corresponding text in Postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Could regulating the creators deliver trustworthy AI? ]( http://arxiv.org/abs/2006.14750v1 )\n",
      "\n",
      "Labhaoise Ni Fhaolain, Andrew Hines\n",
      "\n",
      "Is a new regulated profession, such as Artificial Intelligence (AI) Architect who is responsible and accountable for AI outputs necessary to ensure trustworthy AI? AI is becoming all pervasive and is often deployed in everyday technologies, devices and services without our knowledge. There is heightened awareness of AI in recent years which has brought with it fear. This fear is compounded by the inability to point to a trustworthy source of AI, however even the term trustworthy AI itself is troublesome. Some consider trustworthy AI to be that which complies with relevant laws, while others point to the requirement to comply with ethics and standards (whether in addition to or in isolation of the law). This immediately raises questions of whose ethics and which standards should be applied and whether these are sufficient to produce trustworthy AI in any event.\n"
     ]
    }
   ],
   "source": [
    "title, _, _ = similar_titles[0]\n",
    "sql = \"select text, authors, url from \" + table_name + \" where title = '\" + title + \"';\"\n",
    "cursor.execute(sql)\n",
    "text, authors, url = cursor.fetchone()\n",
    "print(f'[ {title} ]( {url} )\\n\\n{authors}\\n\\n{text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pymilvus import utility\n",
    "# utility.drop_collection(\"question_answer\")\n",
    "# utility.drop_collection(\"text_collection\")\n",
    "# utility.drop_collection(\"alignment_lit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name = alignment_lit\n",
      "description = AI Alignment Literature Dataset\n",
      "schema = {\n",
      "  auto_id: True\n",
      "  description: AI Alignment Literature Dataset\n",
      "  fields: [{\n",
      "    name: id\n",
      "    description: \n",
      "    type: 5\n",
      "    is_primary: True\n",
      "    auto_id: True\n",
      "  }, {\n",
      "    name: vector_embedding\n",
      "    description: \n",
      "    type: 101\n",
      "    params: {'dim': 768}\n",
      "  }]\n",
      "}\n",
      "num_entities = 959\n",
      "primary_field = {\n",
      "    name: id\n",
      "    description: \n",
      "    type: 5\n",
      "    is_primary: True\n",
      "    auto_id: True\n",
      "  }\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import utility, Collection\n",
    "\n",
    "colls = utility.list_collections()\n",
    "for coll_name in colls:\n",
    "    print(\"\\nname =\", coll_name)\n",
    "    coll = Collection(coll_name)\n",
    "#     info = coll.get_collection_info()\n",
    "#     print(\"INFO\\n\", info)\n",
    "#     stats = coll.get_collection_stats()\n",
    "#     print(\"STATS\\n\", stats)    \n",
    "    print(\"description =\", coll.description)\n",
    "    print(\"schema =\", coll.schema)\n",
    "    print(\"num_entities =\", coll.num_entities)\n",
    "    print(\"primary_field =\", coll.primary_field)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from pymilvus import Collection, connections\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "def lit_search(query):\n",
    "    model = SentenceTransformer('allenai-specter')\n",
    "    \n",
    "    query_embeddings = []\n",
    "    embed = model.encode(query)\n",
    "    embed = embed.reshape(1,-1)\n",
    "    embed = normalize(embed)\n",
    "    query_embeddings = embed.tolist()\n",
    "\n",
    "    connections.connect(host='localhost', port='19530')\n",
    "    conn = psycopg2.connect(host='localhost', port='5438', user='postgres', password='postgres')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    table_name = \"alignment_lit\"\n",
    "    field_name = \"vector_embedding\"\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "\n",
    "    collection = Collection(table_name)\n",
    "    collection.load()\n",
    "    results = collection.search(query_embeddings, field_name, param=search_params, limit=5, expr=None)\n",
    "\n",
    "    similar_titles = []\n",
    "    response = \"\"\n",
    "\n",
    "    for result in results[0]:\n",
    "        sql = \"select title from \" + table_name + \" where ids = \" + str(result.id) + \";\"\n",
    "        cursor.execute(sql)\n",
    "        rows = cursor.fetchall()\n",
    "        if len(rows):\n",
    "            title = rows[0][0]\n",
    "            similar_titles.append((result.distance, title))\n",
    "            \n",
    "    results = pd.DataFrame(similar_titles)\n",
    "    results.columns = [\"Score\", \"Title\"]\n",
    "    results[\"Score\"] = results[\"Score\"].round(decimals = 3)\n",
    "    \n",
    "    score, title = similar_titles[0]\n",
    "    sql = \"select text, authors, url from \" + table_name + \" where title = '\" + title + \"';\"\n",
    "    cursor.execute(sql)\n",
    "    text, authors, url = cursor.fetchone()\n",
    "    response += f'[{title}]({url})\\n\\n{authors}\\n\\n{text}'\n",
    "\n",
    "    return results, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score                                              Title\n",
      "0  0.328                                         OpenAI Gym\n",
      "1  0.388       A Berkeley View of Systems Challenges for AI\n",
      "2  0.392                    The AI Index 2021 Annual Report\n",
      "3  0.416  Weak AI is Likely to Never Become Strong AI, S...\n",
      "4  0.416                 The 30-Year Cycle In The AI Debate\n",
      "[OpenAI Gym](http://arxiv.org/abs/1606.01540v1)\n",
      "\n",
      "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba\n",
      "\n",
      "OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.\n"
     ]
    }
   ],
   "source": [
    "# out1, out2 = lit_search(\"open ai gym\")\n",
    "# print(out1)\n",
    "# print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7fc1a2ae8be0>, 'http://127.0.0.1:7869/', None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    title = gr.Textbox(label=\"Question\")\n",
    "    submit_btn = gr.Button(\"Search\")\n",
    "    output = [gr.DataFrame(label=\"Results\", headers=[\"Score\", \"Title\"]), gr.Markdown(label=\"Output\")]\n",
    "    submit_btn.click(fn=lit_search, inputs=title, outputs=output)\n",
    "\n",
    "demo.launch(inline=True)\n",
    "# demo.launch(inline=True, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "There are similar questions in the database, here are the closest matches: \n",
    "('Trustworthy AI: A Computational Perspective', 0.3309140205383301)\n",
    "('Could regulating the creators deliver trustworthy AI?', 0.3363613784313202)\n",
    "('Designing for Responsible Trust in AI Systems: A Communication Perspective', 0.36429765820503235)\n",
    "('Socially Responsible AI Algorithms: Issues, Purposes, and Challenges', 0.3735373020172119)\n",
    "('Trustworthy AI', 0.373695433139801)\n",
    "('Trustworthy AI: From Principles to Practices', 0.37520506978034973)\n",
    "('Know Your Model (KYM): Increasing Trust in AI and Machine Learning', 0.37871670722961426)\n",
    "('The corruptive force of AI-generated advice', 0.38148611783981323)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = gr.Interface.load(\"huggingface/gpt2\")\n",
    "# translator = gr.Interface.load(\"huggingface/t5-small\")\n",
    "\n",
    "# gr.Series(generator, translator).launch()  # this demo generates text, then translates it to German, and outputs the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "958\n"
     ]
    }
   ],
   "source": [
    "# cursor.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\")\n",
    "# for item in cursor.fetchall():\n",
    "#      print(item)\n",
    "cursor.execute(\"SELECT count(*) FROM alignment_lit\")\n",
    "for item in cursor.fetchone():\n",
    "   print(item)\n",
    "\n",
    "# conn.rollback()\n",
    "# cursor.execute(\"SELECT * FROM alignment_lit LIMIT 5\")\n",
    "\n",
    "# import psycopg2\n",
    "\n",
    "# conn = None\n",
    "# try:\n",
    "#     conn = psycopg2.connect(host='localhost', port='5438', user='postgres', password='postgres')\n",
    "#     cursor = conn.cursor()\n",
    "\n",
    "#     cursor.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\")\n",
    "\n",
    "#     for table in cursor.fetchall():\n",
    "#         print(table)\n",
    "# except (Exception, psycopg2.DatabaseError) as error:\n",
    "#         print(error)\n",
    "# finally:\n",
    "#     if cursor is not None:\n",
    "#         cursor.close()\n",
    "#         print('Cursor connection closed.')\n",
    "#     if conn is not None:\n",
    "#         conn.close()\n",
    "#         print('Database connection closed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
