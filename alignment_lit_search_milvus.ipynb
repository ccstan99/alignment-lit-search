{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Search Engine\n",
    "In this example we will be going over the code used to build a Text Search Engine. This example uses a modified BERT model to convert text to vectors stored in Milvus, which can then be combined with Milvus to search for similar text to the user input text.\n",
    "\n",
    "### Sourced from Milvus Bootcamp\n",
    "- https://github.com/milvus-io/bootcamp/tree/master/solutions/text_search_engine\n",
    "- https://github.com/milvus-io/bootcamp/tree/master/solutions/question_answering_system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "This example uses the English News dataset. In this example, we use a small subset of the dataset containing 180 mutually corresponding title-texts, which can be found in the **Data** directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "\n",
    "|  Packages   |  Servers    |\n",
    "|-                  | -                 |   \n",
    "| pymilvus==2.0.0rc5      | milvus 2.0.0-rc5    |\n",
    "| sentence_transformers      | postgres          |\n",
    "| psycopg2          |\n",
    "| pandas           |\n",
    "| numpy   |\n",
    "\n",
    "We have included a `requirements.txt` file in order to easily satisfy the required packages. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up and Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Packages\n",
    "Install the required python packages with `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Milvus Server\n",
    "\n",
    "This demo uses Milvus 2.0, please refer to the [Install Milvus](https://milvus.io/cn/docs/install_standalone-docker.md) guide to learn how to use this docker container. For this example we wont be mapping any local volumes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/milvus-io/milvus/master/deployments/docker/standalone/docker-compose.yml -O docker-compose.yml\n",
    "!sudo docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Postgres Server\n",
    "For now, Milvus doesn't support storing multiple attributes for the data. Because of this we have to use another service to store these attributes and search through them, in this case PostgreSQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: Conflict. The container name \"/postgres0\" is already in use by container \"db3d2bcec8f2bd24a04c4e5aa87b792c3cdd50f4957cdeb88290029fb77795fd\". You have to remove (or rename) that container to be able to reuse that name.\r\n",
      "See 'docker run --help'.\r\n"
     ]
    }
   ],
   "source": [
    "! docker run --name postgres0 -d  -p 5438:5432 -e POSTGRES_HOST_AUTH_METHOD=trust postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-17 08:05:13.733 UTC [1] LOG:  starting PostgreSQL 15.0 (Debian 15.0-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\r\n",
      "2022-10-17 08:05:13.734 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\r\n",
      "2022-10-17 08:05:13.734 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\r\n",
      "2022-10-17 08:05:13.735 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\r\n",
      "2022-10-17 08:05:13.738 UTC [28] LOG:  database system was shut down at 2022-10-16 04:56:00 UTC\r\n",
      "2022-10-17 08:05:13.747 UTC [1] LOG:  database system is ready to accept connections\r\n"
     ]
    }
   ],
   "source": [
    "! docker logs postgres0 --tail 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Servers\n",
    "We first start off by connecting to the servers. In this case the docker containers are running on localhost and the ports are the default ports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Milvus, BERT and Postgresql\n",
    "from pymilvus import connections\n",
    "import psycopg2\n",
    "connections.connect(host='localhost', port='19530')\n",
    "conn = psycopg2.connect(host='localhost', port='5438', user='postgres', password='postgres')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Collection and Setting Index\n",
    "#### 1. Creating the Collection    \n",
    "The next step is to create a collection, which requires declaring the name of the collection and the dimension of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import Collection, CollectionSchema, FieldSchema, DataType, utility\n",
    "\n",
    "table_name= \"alignment_lit\"\n",
    "field_name = \"vector_embedding\"\n",
    "\n",
    "if utility.has_collection(table_name):\n",
    "    utility.drop_collection(table_name)\n",
    "\n",
    "pk = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True)\n",
    "field = FieldSchema(name=field_name, dtype=DataType.FLOAT_VECTOR, dim=768)\n",
    "schema = CollectionSchema(fields=[pk,field], description=\"AI Alignment Literature Dataset\")\n",
    "collection = Collection(name=table_name, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Setting an Index\n",
    "After creating the collection we want to assign it an index type. This can be done before or after inserting the data. When done before, indexes will be made as data comes in and fills the data segments. In this example we are using IVF_SQ8 which requires the 'nlist' parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(code=0, message='')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_param = {\n",
    "        \"metric_type\":\"L2\",\n",
    "        \"index_type\":\"IVF_SQ8\",\n",
    "        \"params\":{\"nlist\":1024}\n",
    "    }\n",
    "collection.create_index(field_name=field_name, index_params=index_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Table in Postgres  \n",
    "PostgresSQL will be used to store Milvus ID and its corresponding title and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create postgres table successfully!\n"
     ]
    }
   ],
   "source": [
    "#Deleting previouslny stored table for clean run\n",
    "drop_table = \"DROP TABLE IF EXISTS \" + table_name\n",
    "cursor.execute(drop_table)\n",
    "conn.commit()\n",
    "\n",
    "try:\n",
    "#     sql = \"CREATE TABLE if not exists \" + table_name + \" (ids bigint, title text, authors text, url text, text text);\"\n",
    "    sql = \"CREATE TABLE if not exists \" + table_name + \" (ids bigint, title text, authors text, url text, content text);\"\n",
    "    cursor.execute(sql)\n",
    "    conn.commit()\n",
    "    print(\"create postgres table successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"can't create a postgres table: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and Storing the News Data\n",
    "#### 1. Generating Embeddings\n",
    "In this example we are using the sentence_transformer library  to encode the sentence into vectors. This library uses a modified BERT model to generate the embeddings, and in this example we are using a model pretrained using Microsoft's `mpnet`. More info can be found [here](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "model = SentenceTransformer('allenai-specter')\n",
    "# Get questions and answers.\n",
    "data = pd.read_json('stampy-arxiv.json')\n",
    "# data = pd.read_json('arxiv_pos_list.json')\n",
    "# title_data = data['title'].tolist()\n",
    "# text_data = data['content'].tolist()\n",
    "\n",
    "title_text_data = data['title'].map(str) + '[SEP]' + data['content'].map(str)\n",
    "\n",
    "sentence_embeddings = model.encode(title_text_data)\n",
    "sentence_embeddings = normalize(sentence_embeddings)\n",
    "print(type(sentence_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DARTS: Differentiable Architecture Search</td>\n",
       "      <td>Hanxiao Liu, Karen Simonyan, Yiming Yang</td>\n",
       "      <td>http://arxiv.org/abs/1806.09055v2</td>\n",
       "      <td>This paper addresses the scalability challenge...</td>\n",
       "      <td>[-0.009074252098798752, 0.06410601735115051, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can You Trust Your Model's Uncertainty? Evalua...</td>\n",
       "      <td>Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary N...</td>\n",
       "      <td>http://arxiv.org/abs/1906.02530v2</td>\n",
       "      <td>Modern machine learning methods including deep...</td>\n",
       "      <td>[-0.021988436579704285, 0.010300278663635254, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quantifying Perceptual Distortion of Adversari...</td>\n",
       "      <td>Matt Jordan, Naren Manoj, Surbhi Goel, Alexand...</td>\n",
       "      <td>http://arxiv.org/abs/1902.08265v1</td>\n",
       "      <td>Recent work has shown that additive threat mod...</td>\n",
       "      <td>[0.038873378187417984, 0.03144840523600578, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adversarial Soft Advantage Fitting: Imitation ...</td>\n",
       "      <td>Paul Barde, Julien Roy, Wonseok Jeon, Joelle P...</td>\n",
       "      <td>http://arxiv.org/abs/2006.13258v6</td>\n",
       "      <td>Adversarial Imitation Learning alternates betw...</td>\n",
       "      <td>[-0.016344817355275154, -0.021231725811958313,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fooling the primate brain with minimal, target...</td>\n",
       "      <td>Li Yuan, Will Xiao, Giorgia Dellaferrera, Gabr...</td>\n",
       "      <td>http://arxiv.org/abs/2011.05623v3</td>\n",
       "      <td>Artificial neural networks (ANNs) are consider...</td>\n",
       "      <td>[-0.014621314592659473, 0.04372721165418625, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>Would an aligned AI allow itself to be shut down?</td>\n",
       "      <td></td>\n",
       "      <td>https://stampy.ai/wiki/Would_an_aligned_AI_all...</td>\n",
       "      <td>Even if the superintelligence was designed to ...</td>\n",
       "      <td>[-0.007493434939533472, 0.01429771352559328, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>Would donating small amounts to AI safety orga...</td>\n",
       "      <td></td>\n",
       "      <td>https://stampy.ai/wiki/Would_donating_small_am...</td>\n",
       "      <td>Many parts of the AI alignment ecosystem are a...</td>\n",
       "      <td>[-0.028546422719955444, 0.03731328994035721, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>Would it improve the safety of quantilizers to...</td>\n",
       "      <td></td>\n",
       "      <td>https://stampy.ai/wiki/Would_it_improve_the_sa...</td>\n",
       "      <td>This is a really interesting question! Because...</td>\n",
       "      <td>[0.008645042777061462, -0.0015172301791608334,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>Wouldn't a superintelligence be smart enough t...</td>\n",
       "      <td></td>\n",
       "      <td>https://stampy.ai/wiki/Wouldn%27t_a_superintel...</td>\n",
       "      <td>The issue isn't that a superintelligence would...</td>\n",
       "      <td>[-0.008411599323153496, 0.03759940713644028, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>Wouldn't it be a good thing for humanity to di...</td>\n",
       "      <td></td>\n",
       "      <td>https://stampy.ai/wiki/Wouldn%27t_it_be_a_good...</td>\n",
       "      <td>In the words of ［https://mindingourway.com/a-t...</td>\n",
       "      <td>[-0.017185041680932045, 0.029293004423379898, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1153 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0             DARTS: Differentiable Architecture Search   \n",
       "1     Can You Trust Your Model's Uncertainty? Evalua...   \n",
       "2     Quantifying Perceptual Distortion of Adversari...   \n",
       "3     Adversarial Soft Advantage Fitting: Imitation ...   \n",
       "4     Fooling the primate brain with minimal, target...   \n",
       "...                                                 ...   \n",
       "1148  Would an aligned AI allow itself to be shut down?   \n",
       "1149  Would donating small amounts to AI safety orga...   \n",
       "1150  Would it improve the safety of quantilizers to...   \n",
       "1151  Wouldn't a superintelligence be smart enough t...   \n",
       "1152  Wouldn't it be a good thing for humanity to di...   \n",
       "\n",
       "                                                authors  \\\n",
       "0              Hanxiao Liu, Karen Simonyan, Yiming Yang   \n",
       "1     Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary N...   \n",
       "2     Matt Jordan, Naren Manoj, Surbhi Goel, Alexand...   \n",
       "3     Paul Barde, Julien Roy, Wonseok Jeon, Joelle P...   \n",
       "4     Li Yuan, Will Xiao, Giorgia Dellaferrera, Gabr...   \n",
       "...                                                 ...   \n",
       "1148                                                      \n",
       "1149                                                      \n",
       "1150                                                      \n",
       "1151                                                      \n",
       "1152                                                      \n",
       "\n",
       "                                                    url  \\\n",
       "0                     http://arxiv.org/abs/1806.09055v2   \n",
       "1                     http://arxiv.org/abs/1906.02530v2   \n",
       "2                     http://arxiv.org/abs/1902.08265v1   \n",
       "3                     http://arxiv.org/abs/2006.13258v6   \n",
       "4                     http://arxiv.org/abs/2011.05623v3   \n",
       "...                                                 ...   \n",
       "1148  https://stampy.ai/wiki/Would_an_aligned_AI_all...   \n",
       "1149  https://stampy.ai/wiki/Would_donating_small_am...   \n",
       "1150  https://stampy.ai/wiki/Would_it_improve_the_sa...   \n",
       "1151  https://stampy.ai/wiki/Wouldn%27t_a_superintel...   \n",
       "1152  https://stampy.ai/wiki/Wouldn%27t_it_be_a_good...   \n",
       "\n",
       "                                                content  \\\n",
       "0     This paper addresses the scalability challenge...   \n",
       "1     Modern machine learning methods including deep...   \n",
       "2     Recent work has shown that additive threat mod...   \n",
       "3     Adversarial Imitation Learning alternates betw...   \n",
       "4     Artificial neural networks (ANNs) are consider...   \n",
       "...                                                 ...   \n",
       "1148  Even if the superintelligence was designed to ...   \n",
       "1149  Many parts of the AI alignment ecosystem are a...   \n",
       "1150  This is a really interesting question! Because...   \n",
       "1151  The issue isn't that a superintelligence would...   \n",
       "1152  In the words of ［https://mindingourway.com/a-t...   \n",
       "\n",
       "                                             embeddings  \n",
       "0     [-0.009074252098798752, 0.06410601735115051, 0...  \n",
       "1     [-0.021988436579704285, 0.010300278663635254, ...  \n",
       "2     [0.038873378187417984, 0.03144840523600578, -0...  \n",
       "3     [-0.016344817355275154, -0.021231725811958313,...  \n",
       "4     [-0.014621314592659473, 0.04372721165418625, -...  \n",
       "...                                                 ...  \n",
       "1148  [-0.007493434939533472, 0.01429771352559328, -...  \n",
       "1149  [-0.028546422719955444, 0.03731328994035721, -...  \n",
       "1150  [0.008645042777061462, -0.0015172301791608334,...  \n",
       "1151  [-0.008411599323153496, 0.03759940713644028, -...  \n",
       "1152  [-0.017185041680932045, 0.029293004423379898, ...  \n",
       "\n",
       "[1153 rows x 5 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(data.iloc[1]['embeddings']) = 768\n",
    "data['embeddings'] = sentence_embeddings.tolist()\n",
    "data.to_json('stampy_arxiv_embeddings.json', orient='records')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Inserting Vectors into Milvus\n",
    "Since this example dataset contains only 100 vectors, we are inserting all of them as one batch insert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "em =list(sentence_embeddings)\n",
    "mr = collection.insert([em])\n",
    "ids = mr.primary_keys\n",
    "dicts ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1153"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('arxiv_pos_list.json')\n",
    "\n",
    "arxiv_df = data[['paper_version', 'title', 'authors', 'url', 'abstract']]\n",
    "# user paper_version as id or index\n",
    "arxiv_df = arxiv_df.rename(columns = {'paper_version':'id', 'abstract':'content'})\n",
    "# join list of author strings into concat string separated by commas\n",
    "arxiv_df['authors'] = arxiv_df['authors'].str.join(', ')\n",
    "# export json version, should add embeddings\n",
    "arxiv_df.to_json('new_data_arxiv_data.json', orient='records')\n",
    "# csv use paper_id as index to avoid extra column\n",
    "arxiv_df = arxiv_df.set_index(\"id\")\n",
    "# export to csv, alt to sql import\n",
    "arxiv_df.to_csv('new_data_arxiv_data.csv')\n",
    "\n",
    "# new_data[new_data['id'] == '1808.03644v1'] # for regular column name not index\n",
    "# print(new_data.loc['1808.03644v1']) # by index\n",
    "# print(len(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A lot of concern appears to focus on human-lev...</td>\n",
       "      <td></td>\n",
       "      <td>https://stampy.ai/wiki/A_lot_of_concern_appear...</td>\n",
       "      <td>AI is already superhuman at some tasks, for ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Any AI will be a computer program. Why wouldn'...</td>\n",
       "      <td></td>\n",
       "      <td>https://stampy.ai/wiki/Any_AI_will_be_a_comput...</td>\n",
       "      <td>While it is true that a computer program alway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are Google, OpenAI, etc. aware of the risk?</td>\n",
       "      <td></td>\n",
       "      <td>https://stampy.ai/wiki/Are_Google,_OpenAI,_etc...</td>\n",
       "      <td>The major AI companies are thinking about this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are there types of advanced AI that would be s...</td>\n",
       "      <td></td>\n",
       "      <td>https://stampy.ai/wiki/Are_there_types_of_adva...</td>\n",
       "      <td>We don’t yet know which AI architectures are s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aren't robots the real problem? How can AI cau...</td>\n",
       "      <td></td>\n",
       "      <td>https://stampy.ai/wiki/Aren%27t_robots_the_rea...</td>\n",
       "      <td>What’s new and potentially risky is not the ab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title authors  \\\n",
       "0  A lot of concern appears to focus on human-lev...           \n",
       "1  Any AI will be a computer program. Why wouldn'...           \n",
       "2        Are Google, OpenAI, etc. aware of the risk?           \n",
       "3  Are there types of advanced AI that would be s...           \n",
       "4  Aren't robots the real problem? How can AI cau...           \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://stampy.ai/wiki/A_lot_of_concern_appear...   \n",
       "1  https://stampy.ai/wiki/Any_AI_will_be_a_comput...   \n",
       "2  https://stampy.ai/wiki/Are_Google,_OpenAI,_etc...   \n",
       "3  https://stampy.ai/wiki/Are_there_types_of_adva...   \n",
       "4  https://stampy.ai/wiki/Aren%27t_robots_the_rea...   \n",
       "\n",
       "                                             content  \n",
       "0  AI is already superhuman at some tasks, for ex...  \n",
       "1  While it is true that a computer program alway...  \n",
       "2  The major AI companies are thinking about this...  \n",
       "3  We don’t yet know which AI architectures are s...  \n",
       "4  What’s new and potentially risky is not the ab...  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, json\n",
    "import pandas as pd\n",
    "\n",
    "stampyURL = \"https://stampy.ai/w/api.php?action=ask&query=[[Category:Answers]][[Canonical::true]][[OutOfScope::false]]%7Climit%3D1000%7Cformat%3Dplainlist%7C%3FAnswer%7C%3FAnswerTo&format=json\"\n",
    "stampyJSON = requests.get(stampyURL).json()[\"query\"][\"results\"]\n",
    "\n",
    "# len(stampyJSON.keys()) = 194\n",
    "stampyQA = []\n",
    "for key in wikiJSON.keys():\n",
    "    answer = wikiJSON[key]['printouts']\n",
    "#     print(answer)\n",
    "#     print(answer['AnswerTo'][0]['fulltext'])\n",
    "#     print(answer['Answer'][0])\n",
    "    item = {'title': answer['AnswerTo'][0]['fulltext'],\n",
    "            'authors': '',\n",
    "            'url': answer['AnswerTo'][0]['fullurl'],\n",
    "            'content': answer['Answer'][0]}\n",
    "    stampyQA.append(item)\n",
    "stampy_df = pd.DataFrame(stampyQA)\n",
    "json.dump(stampyQA, open(\"stampyQA.json\", \"w\"))\n",
    "\n",
    "stampy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "new_data = arxiv_df.append(stampy_df)\n",
    "new_data.to_json('stampy_arxiv_plain.json', orient='records')\n",
    "\n",
    "# model = SentenceTransformer('allenai-specter')\n",
    "# new_data['embeddings'] = model.encode(new_data['title'].map(str) + '[SEP]' + new_data['content'].map(str))\n",
    "# new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://stampy.ai/w/api.php?action=query&format=json&formatversion=2&prop=revisions&rvprop=content&rvslots=*&titles=Are%20there%20types%20of%20advanced%20AI%20that%20would%20be%20safer%20than%20others%3F\n",
      "title Are there types of advanced AI that would be safer than others?\n",
      "pageid 6199\n",
      "content {{Question\n",
      "|canonical=Yes\n",
      "|uniqueid=cd83810f-d574-4b88-a745-0c6e0bd191ea\n",
      "|alternatephrasings=Are some AI designs less dangerous?` Are there any safe kinds of AI?\n",
      "|reviewed=4\n",
      "|canonicalanswer=Answer to Are there types of advanced AI that would be safer than others?\n",
      "|asked=No\n",
      "|asker=FLI\n",
      "|date=2021/07/14\n",
      "|origin=FLI's FAQ\n",
      "|commeturl=https://futureoflife.org/ai-faqs/\n",
      "}}\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "query = 'Are there types of advanced AI that would be safer than others?'\n",
    "# this format will give pageid and content.alternatephrasings\n",
    "stampyURL = 'https://stampy.ai/w/api.php?action=query&format=json&formatversion=2&prop=revisions&rvprop=content&rvslots=*&titles='+urllib.parse.quote(query)\n",
    "print(stampyURL)\n",
    "stampyJSON = requests.get(stampyURL).json()[\"query\"][\"pages\"][0]\n",
    "print(\"title\", stampyJSON['title'])\n",
    "print(\"pageid\", stampyJSON['pageid'])\n",
    "print(\"content\", stampyJSON['revisions'][0][\"slots\"][\"main\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inserting IDs and Title-text into PostgreSQL\n",
    "In order to transfer the data into Postgres, we are creating a new file that combines all the data into a readable format. Once created, we pass this file into the Postgress server through STDIN due to the Postgres container not having access to the file locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sql COPY alignment_lit FROM STDIN DELIMITER '|' CSV HEADER\n",
      "cursor.copy_expert\n",
      "Inserted into Postgress Sucessfully!\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "\n",
    "# conn.rollback()\n",
    "# extra delimiters | and quotes in title/text causes parsing issues, must strip\n",
    "def clean_string(old_string):\n",
    "    return old_string.replace(\"|\",\"\").replace(\"\\\"\",\"\").replace(\"'\",\"\").replace(\"\\n\",\"\")\n",
    "\n",
    "\n",
    "def record_temp_csv(fname, ids, title, authors, urls, text):\n",
    "    with open(fname,'w') as f:\n",
    "        for i in range(len(ids)):\n",
    "            line = str(ids[i]) + \"|\" + clean_string(title[i]) + \"|\" + authors[i] + \\\n",
    "            \"|\" + clean_string(urls[i]) + \"|\" + clean_string(text[i]) + \"\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "def copy_data_to_pg(table_name, fname, conn, cur):\n",
    "    fname = os.path.join(os.getcwd(),fname)\n",
    "    try:\n",
    "        sql = \"COPY \" + table_name + \" FROM STDIN DELIMITER '|' CSV HEADER\"\n",
    "        cursor.copy_expert(sql, open(fname, \"r\"))\n",
    "        conn.commit()\n",
    "        print(\"Inserted into Postgress Sucessfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Copy Data into Postgress failed: \", e)\n",
    "        \n",
    "DATA_WITH_IDS = 'stampy_arxiv.csv'   \n",
    "# DATA_WITH_IDS = 'arxiv_pos_list.csv'   \n",
    "\n",
    "# record_temp_csv(DATA_WITH_IDS, ids, title_data, data['authors'].tolist(), data['url'].tolist(), text_data)\n",
    "record_temp_csv(DATA_WITH_IDS, ids, data['title'].tolist(), data['authors'].tolist(), data['url'].tolist(), data['content'].tolist())\n",
    "copy_data_to_pg(table_name, DATA_WITH_IDS, conn, cursor)\n",
    "# conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search\n",
    "#### 1. Processing Query\n",
    "When searching for a question, we first put the question through the same model to generate an embedding. Then with that embedding vector we  can search for similar embeddings in Milvus.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "\n",
    "title = \"Could regulating the creators deliver trustworthy AI?\"\n",
    "# title = \"Fooling the primate brain\"\n",
    "\n",
    "query_embeddings = []\n",
    "embed = model.encode(title)\n",
    "embed = embed.reshape(1,-1)\n",
    "embed = normalize(embed)\n",
    "query_embeddings = embed.tolist()\n",
    "\n",
    "collection.load()\n",
    "results = collection.search(query_embeddings, field_name, param=search_params, limit=15, expr=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Getting the Similar Titles\n",
    "There may not have titles that are similar to the given one. So we can set a threshold value, here we use 0.5, and when the most similar distance retrieved is less than this value, a hint that the system doesn't include the relevant question is returned. We then use the result ID's to pull out the similar titles from the Postgres server and print them with their corresponding similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 436951485157214108 (0.233) http://arxiv.org/abs/2006.14750v1 Could regulating the creators deliver trustworthy AI?\n",
      " 436951485157214055 (0.312) http://arxiv.org/abs/2107.06641v3 Trustworthy AI: A Computational Perspective\n",
      " 436951485157213962 (0.321) http://arxiv.org/abs/2101.02032v5 Socially Responsible AI Algorithms: Issues, Purposes, and Challenges\n",
      " 436951485157214047 (0.336) http://arxiv.org/abs/2002.06276v1 Trustworthy AI\n",
      " 436951485157214112 (0.337) http://arxiv.org/abs/2110.01167v2 Trustworthy AI: From Principles to Practices\n",
      " 436951485157213903 (0.343) http://arxiv.org/abs/2112.07773v1 Filling gaps in trustworthy development of AI\n",
      " 436951485157214035 (0.352) http://arxiv.org/abs/1905.04994v2 Governance by Glass-Box: Implementing Transparent Moral Bounds for AI Behaviour\n",
      " 436951485157213959 (0.369) http://arxiv.org/abs/2204.13828v1 Designing for Responsible Trust in AI Systems: A Communication Perspective\n",
      " 436951485157213373 (0.371) http://arxiv.org/abs/2110.06674v1 Truthful AI: Developing and governing AI that does not lie\n",
      " 436951485157214049 (0.389) http://arxiv.org/abs/2102.07536v1 The corruptive force of AI-generated advice\n",
      " 436951485157214062 (0.404) http://arxiv.org/abs/1605.02817v2 Unethical Research: How to Create a Malevolent Artificial Intelligence\n",
      " 436951485157213797 (0.406) http://arxiv.org/abs/2104.03741v1 Voluntary safety commitments provide an escape from over-regulation in AI development\n",
      " 436951485157213975 (0.406) http://arxiv.org/abs/2106.11036v1 Know Your Model (KYM): Increasing Trust in AI and Machine Learning\n",
      " 436951485157214091 (0.410) http://arxiv.org/abs/2105.00002v1 Ethics-Based Auditing to Develop Trustworthy AI\n",
      " 436951485157214120 (0.413) http://arxiv.org/abs/2003.11157v1 AI loyalty: A New Paradigm for Aligning Stakeholder Interests"
     ]
    }
   ],
   "source": [
    "similar_titles = []\n",
    "\n",
    "for result in results[0]:\n",
    "    sql = \"select title, url from \" + table_name + \" where ids = \" + str(result.id) + \";\"\n",
    "    print(\"\\n\", result.id, end='')\n",
    "    cursor.execute(sql)\n",
    "    rows = cursor.fetchall()\n",
    "    if len(rows):\n",
    "        title, url = rows[0]\n",
    "        similar_titles.append((title, url, result.distance))\n",
    "        print(f' ({result.distance:.3f}) {url} {title}', end='')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Get the text\n",
    "After getting a list of similar titles, choose the one that you feel is closest to yours. Then you can use that title to find the corresponding text in Postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Could regulating the creators deliver trustworthy AI? ]( http://arxiv.org/abs/2006.14750v1 )\n",
      "\n",
      "Labhaoise Ni Fhaolain, Andrew Hines\n",
      "\n",
      "Is a new regulated profession, such as Artificial Intelligence (AI) Architect who is responsible and accountable for AI outputs necessary to ensure trustworthy AI? AI is becoming all pervasive and is often deployed in everyday technologies, devices and services without our knowledge. There is heightened awareness of AI in recent years which has brought with it fear. This fear is compounded by the inability to point to a trustworthy source of AI, however even the term trustworthy AI itself is troublesome. Some consider trustworthy AI to be that which complies with relevant laws, while others point to the requirement to comply with ethics and standards (whether in addition to or in isolation of the law). This immediately raises questions of whose ethics and which standards should be applied and whether these are sufficient to produce trustworthy AI in any event.\n"
     ]
    }
   ],
   "source": [
    "title, _, _ = similar_titles[0]\n",
    "sql = \"select text, authors, url from \" + table_name + \" where title = '\" + title + \"';\"\n",
    "cursor.execute(sql)\n",
    "text, authors, url = cursor.fetchone()\n",
    "print(f'[ {title} ]( {url} )\\n\\n{authors}\\n\\n{text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pymilvus import utility\n",
    "# utility.drop_collection(\"question_answer\")\n",
    "# utility.drop_collection(\"text_collection\")\n",
    "# utility.drop_collection(\"alignment_lit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name = alignment_lit\n",
      "description = AI Alignment Literature Dataset\n",
      "schema = {\n",
      "  auto_id: True\n",
      "  description: AI Alignment Literature Dataset\n",
      "  fields: [{\n",
      "    name: id\n",
      "    description: \n",
      "    type: 5\n",
      "    is_primary: True\n",
      "    auto_id: True\n",
      "  }, {\n",
      "    name: vector_embedding\n",
      "    description: \n",
      "    type: 101\n",
      "    params: {'dim': 768}\n",
      "  }]\n",
      "}\n",
      "num_entities = 1153\n",
      "primary_field = {\n",
      "    name: id\n",
      "    description: \n",
      "    type: 5\n",
      "    is_primary: True\n",
      "    auto_id: True\n",
      "  }\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import utility, Collection\n",
    "\n",
    "colls = utility.list_collections()\n",
    "for coll_name in colls:\n",
    "    print(\"\\nname =\", coll_name)\n",
    "    coll = Collection(coll_name)\n",
    "#     info = coll.get_collection_info()\n",
    "#     print(\"INFO\\n\", info)\n",
    "#     stats = coll.get_collection_stats()\n",
    "#     print(\"STATS\\n\", stats)    \n",
    "    print(\"description =\", coll.description)\n",
    "    print(\"schema =\", coll.schema)\n",
    "    print(\"num_entities =\", coll.num_entities)\n",
    "    print(\"primary_field =\", coll.primary_field)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from pymilvus import Collection, connections\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "def lit_search(query):\n",
    "    model = SentenceTransformer('allenai-specter')\n",
    "    \n",
    "    query_embeddings = []\n",
    "    embed = model.encode(query)\n",
    "    embed = embed.reshape(1,-1)\n",
    "    embed = normalize(embed)\n",
    "    query_embeddings = embed.tolist()\n",
    "\n",
    "    connections.connect(host='localhost', port='19530')\n",
    "    conn = psycopg2.connect(host='localhost', port='5438', user='postgres', password='postgres')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    table_name = \"alignment_lit\"\n",
    "    field_name = \"vector_embedding\"\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "\n",
    "    collection = Collection(table_name)\n",
    "    collection.load()\n",
    "    results = collection.search(query_embeddings, field_name, param=search_params, limit=5, expr=None)\n",
    "\n",
    "    similar_titles = []\n",
    "    response = \"\"\n",
    "\n",
    "    for result in results[0]:\n",
    "        sql = \"select title from \" + table_name + \" where ids = \" + str(result.id) + \";\"\n",
    "        cursor.execute(sql)\n",
    "        rows = cursor.fetchall()\n",
    "        if len(rows):\n",
    "            title = rows[0][0]\n",
    "            similar_titles.append((result.distance, title))\n",
    "            \n",
    "    results = pd.DataFrame(similar_titles)\n",
    "    results.columns = [\"Score\", \"Title\"]\n",
    "    results[\"Score\"] = results[\"Score\"].round(decimals = 3)\n",
    "    \n",
    "    score, title = similar_titles[0]\n",
    "    sql = \"select content, authors, url from \" + table_name + \" where title = '\" + title + \"';\"\n",
    "    cursor.execute(sql)\n",
    "    content, authors, url = cursor.fetchone()\n",
    "    response += f'[{title}]({url})\\n\\n{authors}\\n\\n{content}'\n",
    "\n",
    "    return results, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score                                              Title\n",
      "0  0.328                                         OpenAI Gym\n",
      "1  0.388       A Berkeley View of Systems Challenges for AI\n",
      "2  0.392                    The AI Index 2021 Annual Report\n",
      "3  0.416  Weak AI is Likely to Never Become Strong AI, S...\n",
      "4  0.416                 The 30-Year Cycle In The AI Debate\n",
      "[OpenAI Gym](http://arxiv.org/abs/1606.01540v1)\n",
      "\n",
      "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba\n",
      "\n",
      "OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.\n"
     ]
    }
   ],
   "source": [
    "# out1, out2 = lit_search(\"open ai gym\")\n",
    "# print(out1)\n",
    "# print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "Running on public URL: https://90cea5e56c0fba10.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces: https://www.huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://90cea5e56c0fba10.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7f87d42bffd0>,\n",
       " 'http://127.0.0.1:7864/',\n",
       " 'https://90cea5e56c0fba10.gradio.app')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    title = gr.Textbox(label=\"Question\")\n",
    "    submit_btn = gr.Button(\"Search\")\n",
    "    output = [gr.DataFrame(label=\"Results\", headers=[\"Score\", \"Title\"]), gr.Markdown(label=\"Output\")]\n",
    "    submit_btn.click(fn=lit_search, inputs=title, outputs=output)\n",
    "\n",
    "# demo.launch(inline=True)\n",
    "demo.launch(inline=True, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "There are similar questions in the database, here are the closest matches: \n",
    "('Trustworthy AI: A Computational Perspective', 0.3309140205383301)\n",
    "('Could regulating the creators deliver trustworthy AI?', 0.3363613784313202)\n",
    "('Designing for Responsible Trust in AI Systems: A Communication Perspective', 0.36429765820503235)\n",
    "('Socially Responsible AI Algorithms: Issues, Purposes, and Challenges', 0.3735373020172119)\n",
    "('Trustworthy AI', 0.373695433139801)\n",
    "('Trustworthy AI: From Principles to Practices', 0.37520506978034973)\n",
    "('Know Your Model (KYM): Increasing Trust in AI and Machine Learning', 0.37871670722961426)\n",
    "('The corruptive force of AI-generated advice', 0.38148611783981323)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = gr.Interface.load(\"huggingface/gpt2\")\n",
    "# translator = gr.Interface.load(\"huggingface/t5-small\")\n",
    "\n",
    "# gr.Series(generator, translator).launch()  # this demo generates text, then translates it to German, and outputs the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "958\n"
     ]
    }
   ],
   "source": [
    "# cursor.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\")\n",
    "# for item in cursor.fetchall():\n",
    "#      print(item)\n",
    "cursor.execute(\"SELECT count(*) FROM alignment_lit\")\n",
    "for item in cursor.fetchone():\n",
    "   print(item)\n",
    "\n",
    "# conn.rollback()\n",
    "# cursor.execute(\"SELECT * FROM alignment_lit LIMIT 5\")\n",
    "\n",
    "# import psycopg2\n",
    "\n",
    "# conn = None\n",
    "# try:\n",
    "#     conn = psycopg2.connect(host='localhost', port='5438', user='postgres', password='postgres')\n",
    "#     cursor = conn.cursor()\n",
    "\n",
    "#     cursor.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\")\n",
    "\n",
    "#     for table in cursor.fetchall():\n",
    "#         print(table)\n",
    "# except (Exception, psycopg2.DatabaseError) as error:\n",
    "#         print(error)\n",
    "# finally:\n",
    "#     if cursor is not None:\n",
    "#         cursor.close()\n",
    "#         print('Cursor connection closed.')\n",
    "#     if conn is not None:\n",
    "#         conn.close()\n",
    "#         print('Database connection closed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
